---
title: "Lower bounds for gradient descent. Accelerated gradient descent. Momentum. Nesterov's acceleration"
author: Daniil Merkulov
institute: Optimization methods. MIPT
format: 
    beamer:
        pdf-engine: pdflatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/header.tex  # Custom LaTeX commands and preamble
header-includes:
  - \newcommand{\bgimage}{../files/back14.jpeg}
---

## Recap of Gradient Descent convergence

$$
\text{Gradient Descent:} \qquad \qquad \min_{x \in \mathbb{R}^n} f(x) \qquad \qquad x^{k+1} = x^k - \alpha^k \nabla f(x^k)
$$

|convex (non-smooth) | smooth (non-convex) | smooth & convex | smooth & strongly convex (or PL) |
|:-----:|:-----:|:-----:|:--------:|
| $f(x^k) - f^* \sim  \mathcal{O} \left( \dfrac{1}{\sqrt{k}} \right)$ | $\|\nabla f(x^k)\|^2 \sim \mathcal{O} \left( \dfrac{1}{k} \right)$ | $f(x^k) - f^* \sim  \mathcal{O} \left( \dfrac{1}{k} \right)$ | $\|x^k - x^*\|^2 \sim \mathcal{O} \left( \left(1 - \dfrac{\mu}{L}\right)^k \right)$ |
| $k_\varepsilon \sim  \mathcal{O} \left( \dfrac{1}{\varepsilon^2} \right)$ | $k_\varepsilon \sim \mathcal{O} \left( \dfrac{1}{\varepsilon} \right)$ | $k_\varepsilon \sim  \mathcal{O}  \left( \dfrac{1}{\varepsilon} \right)$ | $k_\varepsilon  \sim \mathcal{O} \left( \kappa \log \dfrac{1}{\varepsilon}\right)$ |

. . .

:::: {.columns}

::: {.column width="50%"}
For smooth strongly convex we have:
$$
f(x^{k})-f^* \leq \left(1- \dfrac{\mu}{L}\right)^k (f(x^0)-f^*).
$$
Note also, that for any $x$
$$
1 - x \leq e^{-x}
$$
:::

. . .

::: {.column width="50%"}
Finally we have 
$$
\begin{aligned}
\varepsilon &= f(x^{k_\varepsilon})-f^* \leq  \left(1- \dfrac{\mu}{L}\right)^{k_\varepsilon} (f(x^0)-f^*) \\
&\leq \exp\left(- k_\varepsilon\dfrac{\mu}{L}\right) (f(x^0)-f^*) \\
k_\varepsilon &\geq \kappa \log \dfrac{f(x^0)-f^*}{\varepsilon} = \mathcal{O} \left( \kappa \log \dfrac{1}{\varepsilon}\right)
\end{aligned}
$$
:::

::::

. . .

\uncover<+->{{\bf Question:} Can we do faster, than this using the first-order information? }\uncover<+->{{\bf Yes, we can.}}

# Lower bounds

## Lower bounds

| convex (non-smooth) | smooth (non-convex)^[[Carmon, Duchi, Hinder, Sidford, 2017](https://arxiv.org/pdf/1710.11606.pdf)] | smooth & convex^[[Nemirovski, Yudin, 1979](https://fmin.xyz/assets/files/nemyud1979.pdf)] | smooth & strongly convex (or PL) |
|:-----:|:-----:|:-----:|:--------:|
| $\mathcal{O} \left( \dfrac{1}{\sqrt{k}} \right)$ | $\mathcal{O} \left( \dfrac{1}{k^2} \right)$ |  $\mathcal{O} \left( \dfrac{1}{k^2} \right)$ | $\mathcal{O} \left( \left(1 - \sqrt{\dfrac{\mu}{L}}\right)^k \right)$ |
| $k_\varepsilon \sim  \mathcal{O} \left( \dfrac{1}{\varepsilon^2} \right)$  | $k_\varepsilon \sim  \mathcal{O}  \left( \dfrac{1}{\sqrt{\varepsilon}} \right)$ | $k_\varepsilon \sim  \mathcal{O}  \left( \dfrac{1}{\sqrt{\varepsilon}} \right)$ | $k_\varepsilon  \sim \mathcal{O} \left( \sqrt{\kappa} \log \dfrac{1}{{\varepsilon}}\right)$ |

## How optimal is $\mathcal{O}\left(\frac1k\right)$?

* Is it somehow possible to understand, that the obtained convergence is the fastest possible with this class of problem and this class of algorithms?
* The iteration of gradient descent:
    $$
    \begin{aligned}
    x^{k+1} &= x^k - \alpha^k \nabla f(x^k)\\
    &= x^{k-1} - \alpha^{k-1} \nabla f(x^{k-1}) - \alpha^k \nabla f(x^k) \\
    & \;\;\vdots \\
    &= x^0 - \sum\limits_{i=0}^k \alpha^{k-i} \nabla f(x^{k-i})
    \end{aligned}
    $$
* Consider a family of first-order methods, where
    $$
    \begin{aligned}
    x^{k+1} &\in x^0 + \text{span} \left\{\nabla f(x^{0}), \nabla f(x^{1}), \ldots, \nabla f(x^{k})\right\} \; & f \text{ - smooth} \\
    x^{k+1} &\in x^0 + \text{span} \left\{g_{0}, g_{1}, \ldots, g_{k}\right\} \text{, where }
    g_{i} \in \partial f(x^{i}) \; & f \text{ - non-smooth}
    \end{aligned}
    $$ {#eq-fom}

## Non-smooth convex case

:::{.callout-theorem}
There exists a function $f$ that is $G$-Lipschitz and convex such that any [method @eq-fom] satisfies
$$
\min_{i \in [1, k]} f(x^i) - \min_{x \in \mathbb{B}(R)} f(x) \geq \frac{GR}{2(1 + \sqrt{k})}
$$
for $R > 0$ and $k \leq n$, where $n$ is the dimension of the problem.
:::

. . .

**Proof idea:** build such a function $f$ that, for any [method @eq-fom], we have
$$
\text{span} \left\{g_{0}, g_{1}, \ldots, g_{k}\right\} \subset \text{span} \left\{e_{1}, e_{2}, \ldots, e_{i}\right\}
$$
where $e_i$ is the $i$-th standard basis vector. At iteration $k\leq n$, there are at least $n-k$ coordinate of $x$ are $0$. This helps us to derive a bound on the error.

## Non-smooth case (proof)

Consider the function:
$$
f(x) = \beta \max_{i \in [1,k]} x[i] + \frac{\alpha}{2} \|x\|_2^2,
$$
where $\alpha, \beta \in \mathbb{R}$ are parameters, and $x[1:k]$ denotes the first $k$ components of $x$.

. . .

**Key Properties:**

* The function $f(x)$ is $\alpha$-strongly convex due to the quadratic term $\frac{\alpha}{2} \|x\|_2^2$.
* The function is non-smooth because the first term introduces a non-differentiable point at the maximum coordinate of $x$.

. . .

Consider the subdifferential of $f(x)$ at $x$:

:::: {.columns}

::: {.column width="50%"}
$$
\begin{aligned}
\partial f(x) &=   \partial \left( \beta\max_{i \in [1,k]} x[i] \right) + \partial \left( \frac{\alpha}{2} \|x\|_2^2 \right) \\
&=\beta \partial \left(\max_{i \in [1,k]} x[i] \right) + \alpha x.\\
&= \beta \text{conv}\left\{e_i \mid i: x[i] =  \max_{j} x[j] \right\} + \alpha x.
\end{aligned}
$$

:::

. . .

::: {.column width="50%"}
It is easy to see, that if $g \in \partial f(x)$ and $\|x\|\leq R$, then
$$
\|g\| \leq \alpha R + \beta
$$

Thus, $f$ is $\alpha R + \beta$-Lipschitz on $B(R)$.
:::
::::

## Non-smooth case (proof)

Next, we describe the first-order oracle for this function. When queried for a subgradient at a point $x$, the oracle returns
$$
\alpha x + \gamma e_{i},
$$
where $i$ is the *first* coordinate for with $x[i] = \max_{1 \leq j \leq k} x[j]$. 

* We ensure, that $\|x^0\| \leq R$ by starting from $x^0 = 0$. 
* When the oracle is queried at $x^0=0$, it returns $e_1$. Consequently, $x^1$ must lie on the line generated by $e_1$. 
* By an induction argument, one shows that for all $i$, the iterate $x^i$ lies in the linear span of $\{e_1,\dots, e_{i}\}$. In particular, for $i \leq k$, the $k+1$-th coordinate of $x_i$ is zero and due to the structure of $f(x)$:
    $$
    f(x^i) \geq 0.
    $$

## Non-smooth case (proof)

* It remains to compute the minimal value of $f$. Define the point $y\in\mathbb{R}^n$ as
    $$
    y[i] = - \frac{\beta}{\alpha k} \quad \text{for } 1 \leq i \leq k,\qquad y[i] = 0 \quad \text{for } k+1 \leq i \leq n.
    $$
* Note, that $0 \in \partial f(y)$:
    $$
    \begin{aligned}
    \partial f(y) &= \alpha y + \beta \text{conv}\left\{e_i \mid i: y[i] =  \max_{j} y[j] \right\} \\
    &= \alpha y + \beta \text{conv}\left\{e_i \mid i: y[i] =  0 \right\} \\
    0 &\in \partial f(y).
    \end{aligned}
    $$
* It follows that the minimum value of $f = f(y) = f(x^*)$ is
    $$
    f(y) = - \frac{\beta^2}{\alpha k} + \frac{\alpha}{2} \cdot \frac{\beta^2}{\alpha^2 k} = - \frac{\beta^2}{2 \alpha k}.
    $$ 
* Now we have:
    $$
    f(x^i) - f(x^*) \geq 0 - \left( - \frac{\beta^2}{2 \alpha k} \right) \geq \frac{\beta^2}{2 \alpha k}.
    $$

## Non-smooth case (proof)

We have: $f(x^i) - f(x^*) \geq \frac{\beta^2}{2 \alpha k}$, while we need to prove that $\min\limits_{i \in [1, k]} f(x^i) - f(x^*) \geq \frac{GR}{2(1 + \sqrt{k})}$.  

. . .

:::: {.columns}

::: {.column width="50%"}
### Convex case
$$
\alpha = \frac{G}{R}\frac{1}{1 + \sqrt{k}} \quad \beta = \frac{\sqrt{k}}{1 + \sqrt{k}}
$$

$$
\frac{\beta^2}{2\alpha} = \frac{GRk}{2(1 + \sqrt{k})}
$$
Note, in particular, that $\|y\|^2_2 = \frac{\beta^2}{\alpha^2 k} = R^2$ with these parameters

$$
\min\limits_{i \in [1, k]} f(x^i) - f(x^*) \geq \frac{\beta^2}{2 \alpha k} =\frac{GR}{2(1 + \sqrt{k})}
$$
:::

. . .

::: {.column width="50%"}
### Strongly convex case
$$
\alpha = \frac{G}{2R} \quad \beta = \frac{G}{2}
$$
Note, in particular, that $\|y\|_2^2 = \frac{\beta^2}{\alpha^2 k} = \frac{G^2}{4\alpha^2 k} = R^2$ with these parameters

$$
\min\limits_{i \in [1, k]} f(x^i) - f(x^*) \geq \frac{G^2}{8\alpha k}
$$
:::

::::

## Smooth case

:::{.callout-theorem}
There exists a function $f$ that is $L$-smooth and convex such that any [method @eq-fom] satisfies
$$
\min_{i \in [1, k]} f(x^i) - f^* \geq \frac{3L \|x^0 - x^*\|_2^2}{32(1+k)^2}
$$
:::

. . .

* No matter what gradient method you provide, there is always a function $f$ that, when you apply your gradient method on minimizing such $f$, the convergence rate is lower bounded as $\mathcal{O}\left(\frac{1}{k^2}\right)$.
* The key to the proof is to explicitly build a special function $f$.

## Nesterovâ€™s worst function

:::: {.columns}

::: {.column width="50%"}
* Let $n=2k+1$ and $A \in \mathbb{R}^{n \times n}$.
    $$
    \begin{bmatrix}
        2 & -1 & 0 & 0 & \cdots & 0 \\
        -1 & 2 & -1 & 0 & \cdots & 0 \\
        0 & -1 & 2 & -1  & \cdots & 0 \\
        0 & 0 & -1 & 2  & \cdots & 0 \\
        \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & 0 & \cdots & 2  \\
    \end{bmatrix}
    $$
* Notice, that
    $$
    x^T A x = x[1]^2 + x[n]^2 + \sum_{i=1}^{n-1} (x[i] - x[i+1])^2,
    $$
    and, from this expression, it's simple to check $0 \preceq A \preceq 4I$.
* Define the following $L$-smooth convex function
    $$
    f(x) = \frac{L}{8} x^T A x - \frac{L}{4}\langle x, e_1 \rangle.
    $$
:::
::: {.column width="50%"}
* The optimal solution $x^*$ satisfies $Ax^* = e_1$, and solving this system of equations gives
    $$
    x^*[i] = 1 - \frac{i}{n+1},
    $$
* And the objective value is
    $$
    \begin{split}
    f(x^*) &=  \frac{L}{8} {x^*}^T A x^* - \frac{L}{4}\langle x^*, e_1 \rangle \\
    &= -\frac{L}{8} \langle x^*, e_1 \rangle = -\frac{L}{8} \left(1 - \frac{1}{n+1}\right).
    \end{split}
    $$
:::
::::

## Smooth case (proof)

TBD

## Smooth case (proof)

TBD

# Acceleration for quadratics



## Condition number

![](conditions.pdf)

## Condition number and convergence speed

Even with the optimal parameter choice, the error at the next step satisfies

$$\|x_{k+1} - x^*\|_2 \leq q \|x_k - x^*\|_2 , \quad\rightarrow \quad \|x_k - x^*\|_2 \leq q^{k} \|x_0 - x^*\|_2,$$

where 

$$
q = \frac{\lambda_{\max} - \lambda_{\min}}{\lambda_{\max} + \lambda_{\min}} = \frac{\kappa - 1}{\kappa+1},
$$

$$\kappa = \frac{\lambda_{\max}}{\lambda_{\min}} \quad \text{for} \quad A \in \mathbb{S}^n_{++}$$

is the condition number of $A$.

Let us do some demo...

## Demo

:::: {.columns}
::: {.column width="50%"}
![](rich_slow.pdf)
:::

::: {.column width="50%"}

* Thus, for **ill-conditioned** matrices the error of the gradient descent method decays very slowly
* This is another reason why **condition number** is so important:
* Besides the bound on the error in the solution, it also gives an estimate of the number of iterations for the iterative methods.

### Consider non-hermitian matrix $A$

Possible cases of gradient descent behaviour:

- convergence
- divergence
- almost stable trajectory

. . .

**Q:** how can we identify our case **before** running iterative method?


:::
::::

## Spectrum directly affects the convergence

![](rich_non_hermitian.pdf){width=60% fig-align="center"}

## One can still formulate a Lyapunov function ^[[Another approach to build Lyapunov functions for the first order methods in the quadratic case. D. M. Merkulov, I. V. Oseledets](https://arxiv.org/pdf/2310.15880)]

![](rich_non_hermitian_Lyapunov.pdf){width=60% fig-align="center"}

## Relation of the method matrix spectrum for the quadratic problem and convergence of methods^[[Another Approach to Build Lyapunov Functions for the First Order Methods in the Quadratic Case](https://link.springer.com/article/10.1134/S0965542524700131)]

![](lyap_eigen.pdf){width=75% fig-align="center"}

![](lyap_converge.pdf){width=75% fig-align="center"}

## Attempt 1: Exact line search aka steepest descent

:::: {.columns}
::: {.column width="80%"}
$$
\alpha_k = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_{k+1}) = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_k - \alpha \nabla f(x_k))
$$
More theoretical than practical approach. It also allows you to analyze the convergence, but often exact line search can be difficult if the function calculation takes too long or costs a lot.

An interesting theoretical property of this method is that each following iteration is orthogonal to the previous one:
$$
\alpha_k = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_k - \alpha \nabla f(x_k))
$$

. . .

Optimality conditions:

. . .

$$
\nabla f(x_{k+1})^\top \nabla f(x_k) = 0
$$

The convergence rate is the same as for the gradient descent!
:::
::: {.column width="20%"}

![Steepest Descent](GD_vs_Steepest.pdf)

[Open In Colab $\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Steepest_descent.ipynb)
:::
::::

## Attempt 2: Chebyshev acceleration

Another way to find $\alpha_k$ is to consider
$$
\|x_{k+1} - x^*\| = (I - \alpha_k A) \|x_k - x^*\| = (I - \alpha_k A) (I - \alpha_{k-1} A)  \|x_{k-1} - x^*\| = \ldots = p(A) \|x_0 - x^*\|,
$$

where $p(A)$ is a **matrix polynomial** (simplest matrix function)  
$$
p(A) = (I - \alpha_k A) \ldots (I - \alpha_0 A),
$$

and $p(0) = I$.

## Optimal choice of time steps
The error is written as 
$$
e_{k+1} = p(A) e_0,
$$

and hence
$$
\|e_{k+1}\| \leq \|p(A)\| \|e_0\|,
$$

where $p(0) = 1$ and $p(A)$ is a **matrix polynomial**. 

To get better **error reduction**, we need to minimize

$$
\Vert p(A) \Vert
$$ 

over all possible polynomials $p(x)$ of degree $k+1$ such that $p(0)=1$. We will use $\|\cdot\|_2$.

##  Polynomials least deviating from zeros

**Important special case:** $A = A^* > 0$.

Then, $A = U \Lambda U^*$,

and 

$$
\| p(A) \|_2 = \| U p(\Lambda) U^* \|_2 = \| p(\Lambda) \|_2 = \max_i |p(\lambda_i)| \overset{!}{\leq} \max_{\lambda_{\min} \leq \lambda \leq \lambda_{\max}} |p(\lambda)|.
$$

The latter inequality is the only approximation. Here we make a **crucial assumption** that we do not want to benefit from the distribution of the spectrum between $\lambda_{\min}$ and $\lambda_{\max}$.

Thus, we need to find a polynomial $p(\lambda)$ such that $p(0) = 1$, and which has the least possible deviation from $0$ on $[\lambda_{\min}, \lambda_{\max}]$.

##  Polynomials least deviating from zeros (2)

We can do the affine transformation of the interval $[\lambda_{\min}, \lambda_{\max}]$ to the interval $[-1, 1]$:

$$
\xi = \frac{\lambda_{\max} + \lambda_{\min} - (\lambda_{\min}-\lambda_{\max})x}{2}, \quad x\in [-1, 1].
$$

The problem is then reduced to the problem of finding the **polynomial least deviating from zero** on an interval $[-1, 1]$.

## Exact solution: Chebyshev polynomials

The exact solution to this problem is given by the famous **Chebyshev polynomials** of the form

$$T_n(x) =  \cos (n \arccos x)$$

## What do you need to know about Chebyshev polynomials

1. This is a polynomial! 

2. We can express $T_n$ from $T_{n-1}$ and $T_{n-2}$: 
  $$
  T_n(x) = 2x T_{n-1}(x) - T_{n-2}(x), \quad T_0(x)=1, \quad T_1(x)=x
  $$

3. $|T_n(x)| \leq 1$ on $x \in [-1, 1]$.

4. It has $(n+1)$ **alternation points**, where the maximal absolute value is achieved (this is the sufficient and necessary condition for the **optimality**) (Chebyshev alternance theorem, no proof here).

5. The **roots** are just 
    $$
    n \arccos x_k = \frac{\pi}{2} + \pi k, \quad \rightarrow\quad x_k = \cos \frac{\pi(2k + 1)}{2n}, \; k = 0, \ldots,n-1
    $$
  
    We can plot them...

## Convergence of the Chebyshev-accelerated gradient descent

Note that $p(x) = (1-\tau_n x)\dots (1-\tau_0 x)$, hence roots of $p(x)$ are $1/\tau_i$ and that we additionally need to map back from $[-1,1]$ to $[\lambda_{\min}, \lambda_{\max}]$.
This results into 

$$\tau_i = \frac{2}{\lambda_{\max} + \lambda_{\min} - (\lambda_{\max} - \lambda_{\min})x_i}, \quad x_i = \cos \frac{\pi(2i + 1)}{2n}\quad i=0,\dots,n-1$$

The convergence (we only give the result without the proof) is now given by

$$
e_{k+1} \leq C q^k e_0, \quad q = \frac{\sqrt{\mathrm{cond}(A)}-1}{\sqrt{\mathrm{cond}(A)}+1},
$$

which is better than in the gradient descent.

## Convergence of the Chebyshev-accelerated gradient descent

Note that $p(x) = (1-\tau_n x)\dots (1-\tau_0 x)$, hence roots of $p(x)$ are $1/\tau_i$ and that we additionally need to map back from $[-1,1]$ to $[\lambda_{\min}, \lambda_{\max}]$.
This results into 

$$\tau_i = \frac{2}{\lambda_{\max} + \lambda_{\min} - (\lambda_{\max} - \lambda_{\min})x_i}, \quad x_i = \cos \frac{\pi(2i + 1)}{2n}\quad i=0,\dots,n-1$$

The convergence (we only give the result without the proof) is now given by

$$
e_{k+1} \leq C q^k e_0, \quad q = \frac{\sqrt{\mathrm{cond}(A)}-1}{\sqrt{\mathrm{cond}(A)}+1},
$$

which is better than in the gradient descent.




# Heavy ball

## Oscillations and acceleration

[![](GD_vs_HB_hor.pdf)](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/GD.ipynb)


## Polyak Heavy ball method

:::: {.columns}

::: {.column width="25%"}
![](GD_HB.pdf)
:::

::: {.column width="75%"}
Let's introduce the idea of momentum, proposed by Polyak in 1964. Recall that the momentum update is

$$
x^{k+1} = x^k - \alpha \nabla f(x^k) + \beta (x^k - x^{k-1}).
$$

. . .

Which is in our (quadratics) case is
$$
\hat{x}_{k+1} = \hat{x}_k - \alpha \Lambda \hat{x}_k + \beta (\hat{x}_k - \hat{x}_{k-1}) = (I - \alpha \Lambda + \beta I) \hat{x}_k - \beta \hat{x}_{k-1}
$$

. . .

This can be rewritten as follows

$$
\begin{split}
&\hat{x}_{k+1} = (I - \alpha \Lambda + \beta I) \hat{x}_k - \beta \hat{x}_{k-1}, \\
&\hat{x}_{k} = \hat{x}_k.
\end{split}
$$

. . .

Letâ€™s use the following notation $\hat{z}_k = \begin{bmatrix} 
\hat{x}_{k+1} \\
\hat{x}_{k}
\end{bmatrix}$. Therefore $\hat{z}_{k+1} = M \hat{z}_k$, where the iteration matrix $M$ is:

. . .

$$
M = \begin{bmatrix} 
I - \alpha \Lambda + \beta I & - \beta I \\
I & 0_{d}
\end{bmatrix}.
$$

:::
::::

## Reduction to a scalar case

Note, that $M$ is $2d \times 2d$ matrix with 4 block-diagonal matrices of size $d \times d$ inside. It means, that we can rearrange the order of coordinates to make $M$ block-diagonal in the following form. Note that in the equation below, the matrix $M$ denotes the same as in the notation above, except for the described permutation of rows and columns. We use this slight abuse of notation for the sake of clarity. 

. . .

:::: {.columns}

::: {.column width="40%"}

![Illustration of matrix $M$ rearrangement](Rearranging_squares.pdf)

:::
:::{.column width="60%"}
$$
\begin{aligned}
\begin{bmatrix} 
\hat{x}_{k}^{(1)} \\
\vdots \\
\hat{x}_{k}^{(d)} \\
\addlinespace 
\hat{x}_{k-1}^{(1)} \\
\vdots \\
\hat{x}_{k-1}^{(d)}
\end{bmatrix} \to 
\begin{bmatrix} 
\hat{x}_{k}^{(1)} \\
\addlinespace 
\hat{x}_{k-1}^{(1)} \\
\vdots \\
\hat{x}_{k}^{(d)} \\
\addlinespace 
\hat{x}_{k-1}^{(d)}
\end{bmatrix} \quad M = \begin{bmatrix}
M_1\\
&M_2\\
&&\ldots\\
&&&M_d
\end{bmatrix}
\end{aligned}
$$
:::
::::

where $\hat{x}_{k}^{(i)}$ is $i$-th coordinate of vector $\hat{x}_{k} \in \mathbb{R}^d$ and $M_i$ stands for $2 \times 2$ matrix. This rearrangement allows us to study the dynamics of the method independently for each dimension. One may observe, that the asymptotic convergence rate of the $2d$-dimensional vector sequence of $\hat{z}_k$ is defined by the worst convergence rate among its block of coordinates. Thus, it is enough to study the optimization in a one-dimensional case.

## Reduction to a scalar case

For $i$-th coordinate with $\lambda_i$ as an $i$-th eigenvalue of matrix $W$ we have: 

$$
M_i = \begin{bmatrix} 
1 - \alpha \lambda_i + \beta & -\beta \\
1 & 0
\end{bmatrix}.
$$

. . .

The method will be convergent if $\rho(M) < 1$, and the optimal parameters can be computed by optimizing the spectral radius
$$
\alpha^*, \beta^* = \arg \min_{\alpha, \beta} \max_{i} \rho(M_i) \quad \alpha^* = \dfrac{4}{(\sqrt{L} + \sqrt{\mu})^2}; \quad \beta^* = \left(\dfrac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}\right)^2.
$$

. . .

It can be shown, that for such parameters the matrix $M$ has complex eigenvalues, which forms a conjugate pair, so the distance to the optimum (in this case, $\Vert z_k \Vert$), generally, will not go to zero monotonically. 

## Heavy ball quadratic convergence

We can explicitly calculate the eigenvalues of $M_i$:

$$
\lambda^M_1, \lambda^M_2 = \lambda \left( \begin{bmatrix} 
1 - \alpha \lambda_i + \beta & -\beta \\
1 & 0
\end{bmatrix}\right) = \dfrac{1+\beta - \alpha \lambda_i \pm \sqrt{(1+\beta - \alpha\lambda_i)^2 - 4\beta}}{2}.
$$

. . .

When $\alpha$ and $\beta$ are optimal ($\alpha^*, \beta^*$), the eigenvalues are complex-conjugated pair $(1+\beta - \alpha\lambda_i)^2 - 4\beta \leq 0$, i.e. $\beta \geq (1 - \sqrt{\alpha \lambda_i})^2$.

. . .

$$
\text{Re}(\lambda^M_1) = \dfrac{L + \mu - 2\lambda_i}{(\sqrt{L} + \sqrt{\mu})^2}; \quad \text{Im}(\lambda^M_1) = \dfrac{\pm 2\sqrt{(L - \lambda_i)(\lambda_i - \mu)}}{(\sqrt{L} + \sqrt{\mu})^2}; \quad \vert \lambda^M_1 \vert = \dfrac{L - \mu}{(\sqrt{L} + \sqrt{\mu})^2}.
$$

. . .

And the convergence rate does not depend on the stepsize and equals to $\sqrt{\beta^*}$.

## Heavy Ball quadratics convergence

:::{.callout-theorem}
Assume that $f$ is quadratic $\mu$-strongly convex $L$-smooth quadratics, then Heavy Ball method with parameters
$$
\alpha = \dfrac{4}{(\sqrt{L} + \sqrt{\mu})^2}, \beta = \dfrac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}
$$

converges linearly:

$$
\|x_k - x^*\|_2 \leq \left( \dfrac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1} \right) \|x_0 - x^*\|
$$

:::

## Heavy Ball Global Convergence ^[[Global convergence of the Heavy-ball method for convex optimization, Euhanna Ghadimi et.al.](https://arxiv.org/abs/1412.7457)]

:::{.callout-theorem}
Assume that $f$ is smooth and convex and that

$$
\beta\in[0,1),\quad \alpha\in\biggl(0,\dfrac{2(1-\beta)}{L}\biggr).
$$

Then, the sequence $\{x_k\}$ generated by Heavy-ball iteration satisfies

$$
f(\overline{x}_T)-f^{\star} \leq  \left\{
\begin{array}[l]{ll}
\frac{\Vert x_{0}-x^\star\Vert^2}{2(T+1)}\biggl(\frac{L\beta}{1-\beta}+\frac{1-\beta}{\alpha}\biggr),\;\;\textup{if}\;\;
\alpha\in\bigl(0,\dfrac{1-\beta}{L}\bigr],\\
\frac{\Vert x_{0}-x^\star\Vert^2}{2(T+1)(2(1-\beta)-\alpha L)}\biggl({L\beta}+\frac{(1-\beta)^2}{\alpha}\biggr),\;\;\textup{if}\;\;
\alpha\in\bigl[\dfrac{1-\beta}{L},\dfrac{2(1-\beta)}{L}\bigr),
\end{array}
\right.
$$

where $\overline{x}_T$ is the Cesaro average of the iterates, i.e., 

$$
\overline{x}_T = \frac{1}{T+1}\sum_{k=0}^T x_k.
$$
:::


## Heavy Ball Global Convergence ^[[Global convergence of the Heavy-ball method for convex optimization, Euhanna Ghadimi et.al.](https://arxiv.org/abs/1412.7457)]

:::{.callout-theorem}
Assume that $f$ is smooth and strongly convex and that

$$
\alpha\in(0,\dfrac{2}{L}),\quad 0\leq  \beta<\dfrac{1}{2}\biggl( \dfrac{\mu \alpha}{2}+\sqrt{\dfrac{\mu^2\alpha^2}{4}+4(1-\frac{\alpha L}{2})} \biggr) .
$$

where $\alpha_0\in(0,1/L]$. Then, the sequence $\{x_k\}$ generated by Heavy-ball iteration converges linearly to a unique optimizer $x^\star$. In particular,

$$
f(x_{k})-f^\star \leq q^k (f(x_0)-f^\star),
$$

where $q\in[0,1)$.
:::

## Heavy ball method summary

* Ensures accelerated convergence for strongly convex quadratic problems
* Local accelerated convergence was proved in the original paper.
* Recently was proved, that there is no global accelerated convergence for the method.
* Method was not extremely popular until the ML boom
* Nowadays, it is de-facto standard for practical acceleration of gradient methods, even for the non-convex problems (neural network training)

# Nesterov accelerated gradient

## The concept of Nesterov Accelerated Gradient method

:::: {.columns}

::: {.column width="27%"}
$$
x_{k+1} = x_k - \alpha \nabla f(x_k)
$$
:::
::: {.column width="34%"}
$$
x_{k+1} = x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1})
$$
:::
::: {.column width="39%"}
$$
\begin{cases}y_{k+1} = x_k + \beta (x_k - x_{k-1}) \\ x_{k+1} = y_{k+1} - \alpha \nabla f(y_{k+1}) \end{cases}
$$
:::

::::

. . .

Let's define the following notation

$$
\begin{aligned}
x^+ &= x - \alpha \nabla f(x) \qquad &\text{Gradient step} \\
d_k &= \beta_k (x_k - x_{k-1}) \qquad &\text{Momentum term}
\end{aligned}
$$

Then we can write down:


$$
\begin{aligned}
x_{k+1} &= x_k^+ \qquad &\text{Gradient Descent} \\
x_{k+1} &= x_k^+ + d_k \qquad &\text{Heavy Ball} \\
x_{k+1} &= (x_k + d_k)^+ \qquad &\text{Nesterov accelerated gradient}
\end{aligned}
$$


## NAG convergence for quadratics

## General case convergence

:::{.callout-theorem}
Let $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is convex and $L$-smooth. The Nesterov Accelerated Gradient Descent (NAG) algorithm is designed to solve the minimization problem starting with an initial point $x_0 = y_0 \in \mathbb{R}^n$ and $\lambda_0 = 0$. The algorithm iterates the following steps:
$$
\begin{aligned}
&\textbf{Gradient update: } &y_{k+1} &= x_k - \frac{1}{L} \nabla f(x_k) \\
&\textbf{Extrapolation: } &x_{k+1} &= (1 - \gamma_k)y_{k+1} + \gamma_k y_k \\
&\textbf{Extrapolation weight: } &\lambda_{k+1} &= \frac{1 + \sqrt{1 + 4\lambda_k^2}}{2} \\
&\textbf{Extrapolation weight: } &\gamma_k &= \frac{1 - \lambda_k}{\lambda_{k+1}}
\end{aligned}
$$
The sequences $\{f(y_k)\}_{k\in\mathbb{N}}$ produced by the algorithm will converge to the optimal value $f^*$ at the rate of $\mathcal{O}\left(\frac{1}{k^2}\right)$, specifically:
$$
f(y_k) - f^* \leq \frac{2L \|x_0 - x^*\|^2}{k^2}
$$
:::

## General case convergence

:::{.callout-theorem}
Let $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is $\mu$-strongly convex and $L$-smooth. The Nesterov Accelerated Gradient Descent (NAG) algorithm is designed to solve the minimization problem starting with an initial point $x_0 = y_0 \in \mathbb{R}^n$ and $\lambda_0 = 0$. The algorithm iterates the following steps:
$$
\begin{aligned}
&\textbf{Gradient update: } &y_{k+1} &= x_k - \frac{1}{L} \nabla f(x_k) \\
&\textbf{Extrapolation: } &x_{k+1} &= (1 - \gamma_k)y_{k+1} + \gamma_k y_k \\
&\textbf{Extrapolation weight: } &\gamma_k &= \frac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}
\end{aligned}
$$
The sequences $\{f(y_k)\}_{k\in\mathbb{N}}$ produced by the algorithm will converge to the optimal value $f^*$ linearly:
$$
f(y_k) - f^* \leq \frac{\mu + L}{2}\|x_0 - x^*\|^2_2 \exp \left(-\frac{k}{\sqrt{\kappa}}\right)
$$
:::