[
  {
    "objectID": "files/mds.html",
    "href": "files/mds.html",
    "title": "",
    "section": "",
    "text": "!pip install -q geopy\n\n\n[notice] A new release of pip is available: 23.2.1 -&gt; 23.3.2\n[notice] To update, run: python3.9 -m pip install --upgrade pip\n\n\n\nimport numpy as np\nfrom geopy.distance import geodesic\n\n# Estimated GPS coordinates for the specified Moscow Metro stations\n# Format: \"Station Name\": (Latitude, Longitude)\nstations_coordinates = {\n    \"Fizteh\": (55.5518, 37.3247),           # Estimated coordinates for Fizteh\n    \"Chkalovskaya\": (55.4523, 37.3926),\n    \"Slavyanski Boulevard\": (55.4347, 37.2814),\n    \"Park Kultury\": (55.4409, 37.3529),\n    \"Komsomolskaya\": (55.4629, 37.3918),\n    \"Yugo-Zapadnaya\": (55.3949, 37.2900),\n    \"Kievskaya\": (55.4440, 37.3356),\n    \"Strogino\": (55.4814, 37.2411),\n    \"Konkovo\": (55.3800, 37.3108),\n    \"VDNKh\": (55.4916, 37.3828),\n    \"Tekstilshiki\": (55.4232, 37.4354)\n}\n\n# Calculate the pairwise distance matrix\ndef calculate_distance_matrix(coords_dict):\n    station_names = list(coords_dict.keys())\n    n_stations = len(station_names)\n    distance_matrix = np.zeros((n_stations, n_stations))\n\n    for i in range(n_stations):\n        for j in range(n_stations):\n            if i != j:\n                distance_matrix[i][j] = geodesic(coords_dict[station_names[i]], coords_dict[station_names[j]]).kilometers\n            else:\n                distance_matrix[i][j] = 0\n\n    return station_names, distance_matrix\n\nstation_names, distance_matrix = calculate_distance_matrix(stations_coordinates)\ndistance_matrix, station_names\n\n\n(array([[ 0.        , 11.8795683 , 13.32121955, 12.47476534, 10.76730899,\n         17.60531703, 12.02141556,  9.45093857, 19.14704757,  7.64111262,\n         15.93607716],\n        [11.8795683 ,  0.        ,  7.30513134,  2.81466242,  1.18120326,\n          9.11269391,  3.72338261, 10.11508331,  9.57219188,  4.41903113,\n          4.22314047],\n        [13.32121955,  7.30513134,  0.        ,  4.57796148,  7.6589319 ,\n          4.46432896,  3.58332329,  5.79065187,  6.36819994,  9.01496207,\n          9.83338314],\n        [12.47476534,  2.81466242,  4.57796148,  0.        ,  3.47233987,\n          6.48795948,  1.14799012,  8.3873321 ,  7.28559471,  5.95292621,\n          5.58202776],\n        [10.76730899,  1.18120326,  7.6589319 ,  3.47233987,  0.        ,\n          9.94233619,  4.13173454,  9.75035755, 10.55879981,  3.24552038,\n          5.21047455],\n        [17.60531703,  9.11269391,  4.46432896,  6.48795948,  9.94233619,\n          0.        ,  6.18218626, 10.11534229,  2.11882318, 12.26352411,\n          9.73385562],\n        [12.02141556,  3.72338261,  3.58332329,  1.14799012,  4.13173454,\n          6.18218626,  0.        ,  7.28489644,  7.29627254,  6.08240265,\n          6.72859259],\n        [ 9.45093857, 10.11508331,  5.79065187,  8.3873321 ,  9.75035755,\n         10.11534229,  7.28489644,  0.        , 12.12071314,  9.02961058,\n         13.89681105],\n        [19.14704757,  9.57219188,  6.36819994,  7.28559471, 10.55879981,\n          2.11882318,  7.29627254, 12.12071314,  0.        , 13.23410172,\n          9.24357874],\n        [ 7.64111262,  4.41903113,  9.01496207,  5.95292621,  3.24552038,\n         12.26352411,  6.08240265,  9.02961058, 13.23410172,  0.        ,\n          8.31043471],\n        [15.93607716,  4.22314047,  9.83338314,  5.58202776,  5.21047455,\n          9.73385562,  6.72859259, 13.89681105,  9.24357874,  8.31043471,\n          0.        ]]),\n ['Fizteh',\n  'Chkalovskaya',\n  'Slavyanski Boulevard',\n  'Park Kultury',\n  'Komsomolskaya',\n  'Yugo-Zapadnaya',\n  'Kievskaya',\n  'Strogino',\n  'Konkovo',\n  'VDNKh',\n  'Tekstilshiki'])\n\n\n\nimport matplotlib.pyplot as plt\n\ndef gradient_descent_mds(D, dimensions=2, learning_rate=0.01, iterations=1000):\n    \"\"\"\n    Perform Multidimensional Scaling using Gradient Descent.\n\n    :param D: NxN distance matrix.\n    :param dimensions: Number of dimensions for the output coordinates.\n    :param learning_rate: Learning rate for gradient descent.\n    :param iterations: Number of iterations.\n    :return: Nx2 matrix of coordinates.\n    \"\"\"\n    N = D.shape[0]\n    # Random initialization of coordinates\n    X = np.random.rand(N, dimensions)\n\n    for iteration in range(iterations):\n        # Compute distance matrix for current coordinates\n        D_hat = np.sqrt(np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2, axis=2))\n\n        # Compute gradient\n        delta = D_hat - D\n        for i in range(N):\n            for j in range(N):\n                if i != j:\n                    grad = (delta[i, j] / D_hat[i, j]) * (X[i, :] - X[j, :])\n                    X[i, :] -= learning_rate * grad\n\n    return X\n\n# Perform MDS\ncoordinates_mds = gradient_descent_mds(distance_matrix)\n\n# Plotting the results\nplt.figure(figsize=(10, 8))\nplt.scatter(coordinates_mds[:, 0], coordinates_mds[:, 1])\nfor i, name in enumerate(station_names):\n    plt.annotate(name, (coordinates_mds[i, 0], coordinates_mds[i, 1]))\nplt.title(\"2D Representation of Moscow Metro Stations using MDS\")\nplt.xlabel(\"X Coordinate\")\nplt.ylabel(\"Y Coordinate\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom geopy.distance import geodesic\n\n# Estimated GPS coordinates for the specified Moscow Metro stations\nstations_coordinates = {\n    \"Fizteh\": (55.5518, 37.3247),\n    \"Chkalovskaya\": (55.4523, 37.3926),\n    \"Slavyanski Boulevard\": (55.4347, 37.2814),\n    \"Park Kultury\": (55.4409, 37.3529),\n    \"Komsomolskaya\": (55.4629, 37.3918),\n    \"Yugo-Zapadnaya\": (55.3949, 37.2900),\n    \"Kievskaya\": (55.4440, 37.3356),\n    \"Strogino\": (55.4814, 37.2411),\n    \"Konkovo\": (55.3800, 37.3108),\n    \"VDNKh\": (55.4916, 37.3828),\n    \"Tekstilshiki\": (55.4232, 37.4354)\n}\n\n# Calculate the pairwise distance matrix\ndef calculate_distance_matrix(coords_dict):\n    station_names = list(coords_dict.keys())\n    n_stations = len(station_names)\n    distance_matrix = np.zeros((n_stations, n_stations))\n\n    for i in range(n_stations):\n        for j in range(n_stations):\n            if i != j:\n                distance_matrix[i][j] = geodesic(coords_dict[station_names[i]], coords_dict[station_names[j]]).kilometers\n            else:\n                distance_matrix[i][j] = 0\n\n    return station_names, distance_matrix\n\nstation_names, distance_matrix = calculate_distance_matrix(stations_coordinates)\n\n# Gradient Descent MDS with data collection for animation\ndef gradient_descent_mds(D, dimensions=2, learning_rate=0.01, iterations=1000):\n    N = D.shape[0]\n    X = np.random.rand(N, dimensions)\n\n    # Records for animation\n    positions_record = []\n    loss_record = []\n\n    for iteration in range(iterations):\n        D_hat = np.sqrt(np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2, axis=2))\n        delta = D_hat - D\n        loss = np.sum(delta**2)  # Loss calculation\n        positions_record.append(X.copy())\n        loss_record.append(loss)\n\n        # Gradient descent update\n        for i in range(N):\n            for j in range(N):\n                if i != j:\n                    grad = (delta[i, j] / D_hat[i, j]) * (X[i, :] - X[j, :])\n                    X[i, :] -= learning_rate * grad\n\n    return positions_record, loss_record\n\n# Perform MDS and collect data for animation\npositions_record, loss_record = gradient_descent_mds(distance_matrix, iterations=100)\n\n# Function to update each frame in the animation\ndef update_frame(num, positions_record, loss_record, station_names, scat, line, ax1, ax2):\n    # Clear previous station labels and tails\n    ax1.clear()\n    ax1.set_xlim(-10, 10)\n    ax1.set_ylim(-10, 10)\n    ax1.set_title('Station Positions')\n    ax1.set_xlabel('X Coordinate')\n    ax1.set_ylabel('Y Coordinate')\n\n    # Update station positions and draw tails\n    scat.set_offsets(positions_record[num])\n    for i, name in enumerate(station_names):\n        # Draw the tail for each station\n        if num &gt; 0:\n            for past in range(num):\n                ax1.plot([positions_record[past][i, 0], positions_record[past+1][i, 0]],\n                         [positions_record[past][i, 1], positions_record[past+1][i, 1]],\n                         color='gray', alpha=0.5)\n\n        # Label the final position of the station\n        if num == len(positions_record) - 1:\n            ax1.text(positions_record[num][i, 0], positions_record[num][i, 1], name, fontsize=8)\n\n    # Update loss evolution plot\n    line.set_data(range(num + 1), loss_record[:num + 1])\n    ax2.set_xlim(0, 100)\n    ax2.set_ylim(min(loss_record), max(loss_record))\n    ax2.set_title('Loss Evolution')\n    ax2.set_xlabel('Iteration')\n    ax2.set_ylabel('Loss')\n\n    return scat, line\n\n# Create the figure for animation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n# Scatter plot for station positions\nscat = ax1.scatter([], [], s=30)\nax1.set_xlim(-10, 10)\nax1.set_ylim(-10, 10)\nax1.set_title('Station Positions')\nax1.set_xlabel('X Coordinate')\nax1.set_ylabel('Y Coordinate')\n\n# Line plot for loss evolution\nline, = ax2.semilogy([], [], lw=2)\nax2.set_xlim(0, 100)\nax2.set_ylim(min(loss_record), max(loss_record))\nax2.set_title('Loss Evolution')\nax2.set_xlabel('Iteration')\nax2.set_ylabel('Loss')\n\n# Creating the animation\nani = animation.FuncAnimation(fig, update_frame, frames=100, fargs=(positions_record, loss_record, station_names, scat, line, ax1, ax2), blit=False, repeat=False)\n\n# Saving the animation\nani.save('moscow_metro_mds_animation.mp4', writer='ffmpeg', fps=10)\n\n\n\n\n\n\n\n\n\npositions_record[-1]\n\narray([[ 8.02981262,  8.63001461],\n       [ 2.25518938, -2.67734712],\n       [-3.33834951,  1.74376926],\n       [-0.07801013, -1.44497825],\n       [ 3.09715112, -2.01602395],\n       [-6.46246911, -1.07594008],\n       [-0.48882653, -0.35399986],\n       [-1.32473107,  7.16441546],\n       [-6.86842661, -3.2663034 ],\n       [ 5.7281098 ,  1.15859194],\n       [ 4.50067386, -4.182293  ]])\n\n\n\nnp.max()\n\n100"
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "",
    "section": "",
    "text": "Занятие 1\n    \n        📄 Презентация • 📝 Заметки • ▶️ Youtube • 💿 Скачать\n    \n    Вспоминаем линейную алгебру. Некоторые матричные разложения. Спектр матрицы. SVD. Skeleton. Градиент. Гессиан. Матрично-векторное дифференцирование\n\n    Занятие 2\n    \n        📄 Презентация • 📝 Заметки\n    \n    Автоматическое дифференцирование. Forward\\Reverse Mode. Вычислительный граф\n\n    Занятие 3\n    \n        📄 Презентация • 📝 Заметки\n    \n    Выпуклость. Выпуклые, афинные множества. Сумма Минковского. Выпуклые функции. Неравенство Йенсена\n\n    Занятие 4\n    \n        📄 Презентация • 📝 Заметки\n    \n    Сильно выпуклые функции. Условие Поляка - Лоясиевича. Линейная регрессия. Регуляризация. Выпуклость нейронных сетей\n\n    Занятие 5\n    \n        📄 Презентация • 📝 Заметки\n    \n    Сопряженные множества. Сопряженные конусы. Многогранники. Сопряженные функции. Преобразование Лежандра. Сопряженная норма\n\n    Занятие 6\n    \n        📄 Презентация • 📝 Заметки\n    \n    Субградиент. Субдифференциал. Теоремы Моро-Рокафеллара, Дубовицкого-Милютина\n\n    Занятие 7\n    \n        📄 Презентация • 📝 Заметки\n    \n    Условия оптимальности. Функция Лагранжа. Множители Лагранжа. Теорема Каруша - Куна - Таккера\n\n    Занятие 8\n    \n        📄 Презентация • 📝 Заметки\n    \n    Двойственность. Введение в двойственность. Двойственная задача. Two-way partitioning problem. Проекция точки на вероятностный симплекс\n\n    Занятие 9\n    \n        📄 Презентация • 📝 Заметки\n    \n    Двойственность. Анализ чувствительности. Теневые цены. Матричные игры со смешанными стратегиями\n\n    Занятие 10\n    \n        📄 Презентация • 📝 Заметки\n    \n    Линейное программирование. Транспортная задача и другие формулировки прикладных задач как ЛП. Симплекс метод для решения ЛП\n\n    Занятие 11\n    \n        📄 Презентация • 📝 Заметки\n    \n    Классификация и обозначения в задачах оптимизации. Скорость сходимости. Линейный поиск. Неточная одномерная оптимизация. Правила Армихо  - Гольдштейна. Условие Вульфа\n\n    Занятие 12\n    \n        📄 Презентация • 📝 Заметки\n    \n    Методы нулевого порядка. Безградиентные методы. Оптимизация гиперпараметров модели машинного обучения. Генетический алгоритм. Эволюционные алгоритмы\n\n    Занятие 13\n    \n        📄 Презентация • 📝 Заметки\n    \n    Градиентный спуск. Теоремы сходимости в гладком случае (выпуклые, сильно выпуклые, PL). Верхние и нижние оценки сходимости\n\n    Занятие 14\n    \n        📄 Презентация • 📝 Заметки\n    \n    Ускоренные градиентные методы. Метод Поляка, Нестерова\n\n    Занятие 15\n    \n        📄 Презентация • 📝 Заметки\n    \n    Метод сопряженных направлений. Ортогонализация Грамма - Шмидта. Понятие $A$-ортогональных векторов. Метод сопряженных градиентов\n\n    Занятие 16\n    \n        📄 Презентация • 📝 Заметки\n    \n    Субградиентный метод. Теоремы сходимости в негладком случае (выпуклый случай). Особенности работы градиентного метода в практических негладких задачах. Задача наименьших квадратов с $l_1$ регуляризацией\n\n    Занятие 17\n    \n        📄 Презентация • 📝 Заметки\n    \n    Градиентные методы в условных задачах оптимизации - метод проекции градиента. Метод Франк - Вульфа. Метод зеркального спуска\n\n    Занятие 18\n    \n        📄 Презентация • 📝 Заметки\n    \n    Концепция методов адаптивной метрики. Метод Ньютона. Квазиньютоновские методы\n\n    Занятие 19\n    \n        📄 Презентация • 📝 Заметки\n    \n    Проксимальный градиентный метод\n\n    Занятие 20\n    \n        📄 Презентация • 📝 Заметки\n    \n    Введение в стохастические градиентные методы. Батч, эпоха. Сходимость SGD\n\n    Занятие 21\n    \n        📄 Презентация • 📝 Заметки\n    \n    Методы редукции дисперсии: SAG, SVRG, SAGA. Адаптивные стохастические градиентные методы\n\n    Занятие 22\n    \n        📄 Презентация • 📝 Заметки\n    \n    Обучение нейронных сетей с точки зрения методов оптимизации. Обобщающая способность моделей машинного обучения. Double Descent. Grokking. Mode connectivity\n\n    Занятие 23\n    \n        📄 Презентация • 📝 Заметки\n    \n    Удивительные сюжеты из мира обучения больших нейросетей с точки зрения методов оптимизации. Проекция функции потерь нейронной сети на прямую, плоскость. Инициализация. Grokking. Double Descent. Large batch training. Чекпоинтинг активаций.\n\n    Занятие 24\n    \n        📄 Презентация • 📝 Заметки\n    \n    Вопросы обучения больших моделей. Lars, Lamb. Learning rate schedulers. Warm-up. MultiGPU training. LoRa адаптеры. Квантизация.\n\n    Занятие 25\n    \n        📄 Презентация • 📝 Заметки\n    \n    Методы оптимизации в непрерывном времени. Gradient Flow. Accelerated Gradient Flow. Stochastic gradient flow.\n\n    Занятие 26\n    \n        📄 Презентация • 📝 Заметки\n    \n    Введение в двойственные методы оптимизации. Метод двойственного градиентного подъёма. Метод модифицированной функции Лагранжа. ADMM.\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Методы оптимизации. МФТИ 2023-2024",
    "section": "",
    "text": "Методы оптимизации. МФТИ 2023-2024\nКурс представляет собой систематическое введение в разные области оптимизации. Рассматриваются начала выпуклого анализа, излагаются современные результаты и подходы в решении прикладных задач оптимизации. Курс содержит набор теоретических основ для того, чтобы понимать почему и как определенные методы работают. В начале курса основной упор делается на теоретический аппарат, в конце уделяется большее внимание методам оптимизации. Каждое занятие начинается с тестирования по материалам предыдущего занятия. Первая часть курса больше сфокусирована не теорию, необходимую для последующего использования. Во второй части делается упор на методы оптимизации, начиная c классических результатов, заканчивая самыми актуальными приложениями.\n                        \n                                            \n\n\nКоманда курса\n\n\n    \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Даниил Меркулов\n                    \n                    Преподаватель\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Александр Тришин\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Денис Рубцов\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Илья Забара\n                    \n                    Ассистент\n                  \n                \n              \n        \n    \n\n\nNo matching items"
  }
]