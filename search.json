[
  {
    "objectID": "files/nanogpt.html",
    "href": "files/nanogpt.html",
    "title": "",
    "section": "",
    "text": "# Re-import necessary libraries and re-define data since the execution state was reset\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom datetime import datetime\nimport pandas as pd\n\n# Data from the table\ndata = {\n    \"Record Time (minutes)\": [45, 31.4, 24.9, 22.3, 15.2, 13.1, 12.0, 10.8, 8.2, 7.8, 7.2, 5.03, 4.66],\n    \"Description\": [\n        \"llm.c baseline\",\n        \"Architectural modernizations & tuned learning rate\",\n        \"Introduced the Muon optimizer\",\n        \"Muon improvements\",\n        \"Pad embeddings & architectural improvements\",\n        \"Distributed the overhead of Muon\",\n        \"Upgraded PyTorch from 2.4.1 to 2.5.0\",\n        \"Untied embed and lm_head\",\n        \"Shortcuts & tweaks\",\n        \"Bfloat16 activations\",\n        \"U-net & 2x lr\",\n        \"FlexAttention\",\n        \"Attention window warmup\"\n    ],\n    \"Date\": [\n        \"05/28/24\", \"06/06/24\", \"10/04/24\", \"10/11/24\", \"10/14/24\", \"10/18/24\", \"10/18/24\", \n        \"11/03/24\", \"11/06/24\", \"11/08/24\", \"11/10/24\", \"11/19/24\", \"11/24/24\"\n    ]\n}\n\n# Convert to a DataFrame\ndf = pd.DataFrame(data)\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%m/%d/%y\")\n\n# Sort the data by date to ensure proper plotting\ndf = df.sort_values(\"Date\")\n\n# Prepare data for plotting\ndates = df[\"Date\"]\ntimes = df[\"Record Time (minutes)\"]\ndescriptions = df[\"Description\"]\n\n# Adjust the plot to spread labels further and use more of the available space\nplt.figure(figsize=(12, 6))\nplt.plot(dates, times, linestyle=\"--\", zorder=0)\nplt.scatter(dates, times, label=\"Training Time\", marker=\"o\", s=70, zorder=100)\n\n# Add labels with arrows for clear separation, spreading them out further\nfor i, (date, time, desc) in enumerate(zip(dates, times, descriptions)):\n    # Larger offset for labels\n    if i &lt;= 1:\n        y_offset = 0\n        x_offset = pd.Timedelta(days=5)\n    elif desc == \"U-net & 2x lr\":\n        y_offset = 16 if i % 2 == 0 else 7\n        y_offset = int(y_offset*(1 - i/20))*0.8\n        x_offset = pd.Timedelta(days=0 if i % 2 == 0 else -100)\n    else:\n        y_offset = 16 if i % 2 == 0 else 7\n        y_offset = int(y_offset*(1 - i/20))\n        x_offset = pd.Timedelta(days=0 if i % 2 == 0 else -100)\n    # Plot the arrow\n    plt.annotate(\n        desc,\n        xy=(date, time),\n        xytext=(date + x_offset, time + y_offset),\n        arrowprops=dict(facecolor='black', arrowstyle=\"-\", lw=0.5),\n        fontsize=14,\n        bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"gray\", facecolor=\"white\", alpha=0.9, zorder=-100)\n    )\n\n# Formatting the plot\nplt.title(\"–£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è NanoGPT - 125M\")\nplt.xlabel(\"–î–∞—Ç–∞, 2024\")\nplt.ylabel(\"–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ 8xH100, –º–∏–Ω—É—Ç—ã\")\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%d.%m\"))\nplt.gca().xaxis.set_major_locator(mda tes.MonthLocator())\nplt.xticks(rotation=45)\nplt.grid(alpha=0.3, linestyle=\":\")\n# plt.legend()\nplt.tight_layout()\n\n# Show the plot\nplt.savefig(\"nanogpt_speedrun.pdf\")\nplt.show()"
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "",
    "section": "",
    "text": "–ó–∞–Ω—è—Ç–∏–µ 1\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –í—Å–ø–æ–º–∏–Ω–∞–µ–º –ª–∏–Ω–µ–π–Ω—É—é –∞–ª–≥–µ–±—Ä—É. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –º–∞—Ç—Ä–∏—á–Ω—ã–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è. –°–ø–µ–∫—Ç—Ä –º–∞—Ç—Ä–∏—Ü—ã. SVD. Skeleton. –ì—Ä–∞–¥–∏–µ–Ω—Ç. –ì–µ—Å—Å–∏–∞–Ω. –ú–∞—Ç—Ä–∏—á–Ω–æ-–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ\n\n    –ó–∞–Ω—è—Ç–∏–µ 2\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –ü–æ–≤—Ç–æ—Ä—è–µ–º –º–∞—Ç—Ä–∏—á–Ω—ã–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ. –ü—Ä–æ–∫–ª—è—Ç–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤ –Ω—É–ª–µ–≤–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞\n\n    –ó–∞–Ω—è—Ç–∏–µ 3\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ. Forward\\Reverse Mode. –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π –≥—Ä–∞—Ñ.\n\n    –ó–∞–Ω—è—Ç–∏–µ 4\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –í—ã–ø—É–∫–ª–æ—Å—Ç—å. –í—ã–ø—É–∫–ª—ã–µ, –∞—Ñ–∏–Ω–Ω—ã–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞. –°—É–º–º–∞ –ú–∏–Ω–∫–æ–≤—Å–∫–æ–≥–æ. –í—ã–ø—É–∫–ª—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏. –ù–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –ô–µ–Ω—Å–µ–Ω–∞\n\n    –ó–∞–Ω—è—Ç–∏–µ 5\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –°–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏. –£—Å–ª–æ–≤–∏–µ –ü–æ–ª—è–∫–∞ - –õ–æ—è—Å–∏–µ–≤–∏—á–∞. –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è.\n\n    –ó–∞–Ω—è—Ç–∏–µ 6\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –°–æ–ø—Ä—è–∂–µ–Ω–Ω—ã–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞. –°–æ–ø—Ä—è–∂–µ–Ω–Ω—ã–µ –∫–æ–Ω—É—Å—ã. –ú–Ω–æ–≥–æ–≥—Ä–∞–Ω–Ω–∏–∫–∏. –°–æ–ø—Ä—è–∂–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –õ–µ–∂–∞–Ω–¥—Ä–∞. –°—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç. –°—É–±–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª. –¢–µ–æ—Ä–µ–º—ã –ú–æ—Ä–æ-–†–æ–∫–∞—Ñ–µ–ª–ª–∞—Ä–∞, –î—É–±–æ–≤–∏—Ü–∫–æ–≥–æ-–ú–∏–ª—é—Ç–∏–Ω–∞\n\n    –ó–∞–Ω—è—Ç–∏–µ 7\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –°—É–±–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª. –£—Å–ª–æ–≤–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç–∏ –≤ —Å—É–±–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ. –§—É–Ω–∫—Ü–∏—è –õ–∞–≥—Ä–∞–Ω–∂–∞. –ú–Ω–æ–∂–∏—Ç–µ–ª–∏ –õ–∞–≥—Ä–∞–Ω–∂–∞. –ó–∞–¥–∞—á–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ —Ç–∏–ø–∞ —Ä–∞–≤–µ–Ω—Å—Ç–≤.\n\n    –ó–∞–Ω—è—Ç–∏–µ 8\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –£—Å–ª–æ–≤–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç–∏. –§—É–Ω–∫—Ü–∏—è –õ–∞–≥—Ä–∞–Ω–∂–∞. –ú–Ω–æ–∂–∏—Ç–µ–ª–∏ –õ–∞–≥—Ä–∞–Ω–∂–∞. –¢–µ–æ—Ä–µ–º–∞ –ö–∞—Ä—É—à–∞ - –ö—É–Ω–∞ - –¢–∞–∫–∫–µ—Ä–∞\n\n    –ó–∞–Ω—è—Ç–∏–µ 9\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –î–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å. –í–≤–µ–¥–µ–Ω–∏–µ –≤ –¥–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å. –î–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞. Two-way partitioning problem. –†–µ—à–µ–Ω–∏–µ –ø—Ä—è–º–æ–π –∑–∞–¥–∞—á–∏ —Å –ø–æ–º–æ—â—å—é –¥–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ–π\n\n    –ó–∞–Ω—è—Ç–∏–µ 10\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –õ–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ. –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–∞—è –∑–∞–¥–∞—á–∞ –∏ –¥—Ä—É–≥–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –ø—Ä–∏–∫–ª–∞–¥–Ω—ã—Ö –∑–∞–¥–∞—á –∫–∞–∫ –õ–ü. –°–∏–º–ø–ª–µ–∫—Å –º–µ—Ç–æ–¥ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –õ–ü\n\n    –ó–∞–Ω—è—Ç–∏–µ 11\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –î–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –≤ –ª–∏–Ω–µ–π–Ω–æ–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏. –ê–Ω–∞–ª–∏–∑ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n\n    –ó–∞–Ω—è—Ç–∏–µ 12\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –°–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏. –õ–∏–Ω–µ–π–Ω—ã–π –ø–æ–∏—Å–∫. –ù–µ—Ç–æ—á–Ω–∞—è –æ–¥–Ω–æ–º–µ—Ä–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è. –ü—Ä–∞–≤–∏–ª–∞ –ê—Ä–º–∏—Ö–æ  - –ì–æ–ª—å–¥—à—Ç–µ–π–Ω–∞. –£—Å–ª–æ–≤–∏–µ –í—É–ª—å—Ñ–∞\n\n    –ó–∞–Ω—è—Ç–∏–µ 13\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫. –¢–µ–æ—Ä–µ–º—ã —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –≥–ª–∞–¥–∫–æ–º —Å–ª—É—á–∞–µ (–≤—ã–ø—É–∫–ª—ã–µ, —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã–µ, PL).\n\n    –ó–∞–Ω—è—Ç–∏–µ 14\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –ù–∏–∂–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤. –£—Å–∫–æ—Ä–µ–Ω–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã. –ü–æ–ª–∏–Ω–æ–º—ã –ß–µ–±—ã—à–µ–≤–∞. –ú–µ—Ç–æ–¥ –ü–æ–ª—è–∫–∞, –ù–µ—Å—Ç–µ—Ä–æ–≤–∞.\n\n    –ó–∞–Ω—è—Ç–∏–µ 15\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –ú–µ—Ç–æ–¥ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π. –û—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –ì—Ä–∞–º–º–∞ - –®–º–∏–¥—Ç–∞. –ü–æ–Ω—è—Ç–∏–µ $A$-–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤. –ú–µ—Ç–æ–¥ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n\n    –ó–∞–Ω—è—Ç–∏–µ 16\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è\n    \n    –ö–æ–Ω—Ü–µ–ø—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏. –ú–µ—Ç–æ–¥ –ù—å—é—Ç–æ–Ω–∞. –ö–≤–∞–∑–∏–Ω—å—é—Ç–æ–Ω–æ–≤—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã\n\n    –ó–∞–Ω—è—Ç–∏–µ 17\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤ —É—Å–ª–æ–≤–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ - –º–µ—Ç–æ–¥ –ø—Ä–æ–µ–∫—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞. –ú–µ—Ç–æ–¥ –§—Ä–∞–Ω–∫ - –í—É–ª—å—Ñ–∞.\n\n    –ó–∞–Ω—è—Ç–∏–µ 18\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –°—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –º–µ—Ç–æ–¥. –¢–µ–æ—Ä–µ–º—ã —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –Ω–µ–≥–ª–∞–¥–∫–æ–º —Å–ª—É—á–∞–µ (–≤—ã–ø—É–∫–ª—ã–π —Å–ª—É—á–∞–π). –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –≤ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–µ–≥–ª–∞–¥–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –ó–∞–¥–∞—á–∞ –Ω–∞–∏–º–µ–Ω—å—à–∏—Ö –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ —Å $l_1$ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π. –ú–µ—Ç–æ–¥ –ø—Ä–æ–µ–∫—Ü–∏–∏ —Å—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–∞. –ú–µ—Ç–æ–¥ –∑–µ—Ä–∫–∞–ª—å–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞.\n\n    –ó–∞–Ω—è—Ç–∏–µ 19\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –ü—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –º–µ—Ç–æ–¥\n\n    –ó–∞–Ω—è—Ç–∏–µ 20\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –í–≤–µ–¥–µ–Ω–∏–µ –≤ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã. –ë–∞—Ç—á, —ç–ø–æ—Ö–∞. –°—Ö–æ–¥–∏–º–æ—Å—Ç—å SGD\n\n    –ó–∞–Ω—è—Ç–∏–µ 21\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –ú–µ—Ç–æ–¥—ã —Ä–µ–¥—É–∫—Ü–∏–∏ –¥–∏—Å–ø–µ—Ä—Å–∏–∏: SAG, SVRG\n\n    –ó–∞–Ω—è—Ç–∏–µ 22\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã. AdamW, Muon, Shampoo, NanoGPT speedrun, AlgoPerf banchmark\n\n    –ó–∞–Ω—è—Ç–∏–µ 23\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –£–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–µ —Å—é–∂–µ—Ç—ã –∏–∑ –º–∏—Ä–∞ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ü—Ä–æ–µ–∫—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –Ω–∞ –ø—Ä—è–º—É—é, –ø–ª–æ—Å–∫–æ—Å—Ç—å. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è. Grokking. Double Descent. Large batch training. –ß–µ–∫–ø–æ–∏–Ω—Ç–∏–Ω–≥ –∞–∫—Ç–∏–≤–∞—Ü–∏–π. –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –û–±–æ–±—â–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. Double Descent. Grokking.\n\n    –ó–∞–Ω—è—Ç–∏–µ 24\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –î–≤–æ–π—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ü—Ä—è–º–æ-–¥–≤–æ–π—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã. –ú–µ—Ç–æ–¥ –¥–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–¥—ä—ë–º–∞. –ú–µ—Ç–æ–¥ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –õ–∞–≥—Ä–∞–Ω–∂–∞. ADMM.\n\n    –ó–∞–Ω—è—Ç–∏–µ 25\n    \n        üìÑ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è ‚Ä¢ üìù –ó–∞–º–µ—Ç–∫–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –ú–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. Gradient Flow. Accelerated Gradient Flow. Stochastic gradient flow. Central Flow.\n\nNo matching items"
  },
  {
    "objectID": "projects.html#–≤—ã–±–æ—Ä-—Ç–µ–º—ã",
    "href": "projects.html#–≤—ã–±–æ—Ä-—Ç–µ–º—ã",
    "title": "",
    "section": "–í—ã–±–æ—Ä —Ç–µ–º—ã",
    "text": "–í—ã–±–æ—Ä —Ç–µ–º—ã\nüóì 11 –∞–ø—Ä–µ–ª—è 2025. ü¶Ñ 4 –±–∞–ª–ª–∞\n–í—ã–±—Ä–∞—Ç—å –∫–æ–º–∞–Ω–¥—É –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞. –†–∞–∑–º–µ—Ä –∫–æ–º–∞–Ω–¥—ã - 4-5 —á–µ–ª–æ–≤–µ–∫.\n–ö —ç—Ç–æ–º—É –º–æ–º–µ–Ω—Ç—É –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å —Å —Å–µ–º–∏–Ω–∞—Ä–∏—Å—Ç–æ–º/–ª–µ–∫—Ç–æ—Ä–æ–º —Ç–µ–º—É –ø—Ä–æ–µ–∫—Ç–∞ –∏ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞–±–æ—Ç—ã –Ω–∞–¥ –Ω–∏–º. –®–∞–±–ª–æ–Ω \\LaTeX –¥–ª—è —Ä–∞–±–æ—Ç—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –∑–¥–µ—Å—å, –≤—ã–ø–æ–ª–Ω—è—Ç—å —ç—Ç–∞–ø—ã –ø—Ä–æ–µ–∫—Ç–∞ –Ω–∞–¥–æ –≤ –Ω—ë–º. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å VSCode/Cursor —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º LaTeX Workshop. –¢–∞–∫ –∂–µ –¥–æ—Å—Ç—É–ø–Ω–∞ –≤–µ—Ä—Å–∏—è –≤ overleaf. –î–æ–ø—É—Å–∫–∞–µ—Ç—Å—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ –≤ Typst, –µ—Å–ª–∏ –≤—ã —É–º–µ–µ—Ç–µ, –Ω–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–∏–∂–µ –±—É–¥—É—Ç –ø—Ä–µ–¥—ä—è–≤–ª—è—Ç—å—Å—è —Ç–æ—á–Ω–æ —Ç–∞–∫–∏–µ –∂–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ). –í –¥–∞–ª—å–Ω–µ–π—à–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–π —à–∞–±–ª–æ–Ω, –¥–æ–ø–æ–ª–Ω—è—è –µ–≥–æ –ø–æ –º–µ—Ä–µ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –≤ –ø—Ä–æ–µ–∫—Ç–µ. –í—Å–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏ —Å—Å—ã–ª–∫–∏ –ø–æ —Ç–µ–º–µ —É–¥–æ–±–Ω–æ —Å–æ–±–∏—Ä–∞—Ç—å –≤ notion, —á—Ç–æ–±—ã –ø—Ä–∏ –Ω–∞—à–µ–º –æ–±—Å—É–∂–¥–µ–Ω–∏–∏ –æ–Ω–∏ –±—ã–ª–∏ –ø–æ–¥ —Ä—É–∫–æ–π.\n\n–í–æ–∑–º–æ–∂–Ω—ã–µ —Ç–µ–º—ã\n–ì–ª–∞–≤–Ω–æ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∫ —Ç–µ–º–µ –ø—Ä–æ–µ–∫—Ç–∞ - –≤–∞–º –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø—Ä–∏–∫–æ–ª—å–Ω–æ –µ–≥–æ –¥–µ–ª–∞—Ç—å, —Ç–µ–º–∞ –¥–æ–ª–∂–Ω–∞ –≤–∞—Å –∂–∏–≤–æ –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å. –í—Ç–æ—Ä–æ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ - –æ–Ω –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–≤—è–∑–∞–Ω —Å —Ç–µ–æ—Ä–∏–µ–π –∏–ª–∏ –º–µ—Ç–æ–¥–∞–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (—Ö–æ—Ç—è –±—ã –∫–∞–∫-—Ç–æ üôÇ). –¢–µ–º—É –ø—Ä–æ–µ–∫—Ç–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–∏–¥—É–º–∞—Ç—å/ –Ω–∞–π—Ç–∏/ –≤—ã–±—Ä–∞—Ç—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ.\n\n–í–∑—è—Ç—å –Ω–µ–¥–∞–≤–Ω–æ (–º–æ–∂–Ω–æ —Å–º–µ–ª–æ —Å–º–æ—Ç—Ä–µ—Ç—å –ø–æ keywords SVD, Schur decomposition, low-rank –∏ —Ç.–¥. –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ª–µ—Ç) –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—É—é —Å—Ç–∞—Ç—å—é –Ω–∞ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ NeurIPS, ICML, ICLR. –î–µ—Ç–∞–ª—å–Ω–æ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è. –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏. –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–∞ –¥—Ä—É–≥–∏—Ö –¥–∞–Ω–Ω—ã—Ö/–º–æ–¥–µ–ª—è—Ö/–º–µ—Ç–æ–¥–∞—Ö. –í–æ–∑–º–æ–∂–Ω–æ, –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å/–ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —á—Ç–æ-–Ω–∏–±—É–¥—å –Ω–æ–≤–æ–µ.\n–ü—Ä–µ–∫—Ä–∞—Å–Ω—ã–º –ø—Ä–∏–º–µ—Ä–æ–º –ø—Ä–æ–µ–∫—Ç–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π –∑–∞–¥–∞—á–∏. –ù–∞–ø—Ä–∏–º–µ—Ä,\n\n–û–±—É—á–µ–Ω–∏–µ –æ–≥—Ä–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–≥—Ä–æ–º–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\n–ú–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è –æ—á–µ–Ω—å —Å–ª–æ–∂–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏\n–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å–≤–æ–π—Å—Ç–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ —Å –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π —Å—Ç–æ—Ö.–≥—Ä–∞–¥–∏–µ–Ω—Ç–∞\n\n–í —Ä–∞–º–∫–∞—Ö —Ç–∞–∫–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –æ—á–µ–Ω—å –ø–æ–Ω—è—Ç–Ω–∞—è –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏ –∏ —Ç–≤–æ—Ä—á–µ—Å–∫–∞—è —Å–≤–æ–±–æ–¥–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ª—é–±—ã–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –µ—ë —Ä–µ—à–µ–Ω–∏—é - —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–µ, —Ç—Ä–µ–±—É—é—â–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –≤—Ö–æ–¥–æ–≤, –∏ —Ç.–¥.\n–í —Å—Ç–∞—Ç—å–µ Old Optimizer, New Norm: An Anthology –∞–≤—Ç–æ—Ä—ã –¥–µ–ª–∞—é—Ç —É–ø–æ—Ä –Ω–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é —Ä–µ—à–µ–Ω–∏—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –∑–∞–¥–∞—á–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –í–∞–∂–Ω–æ, —á—Ç–æ –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –µ–¥–∏–Ω—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —Å –ø–æ–º–æ—â—å—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ç–æ—Ä–Ω—ã—Ö –Ω–æ—Ä–º. –í —Ä–∞–º–∫–∞—Ö –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –æ–ø–µ—Ä–∞—Ç–æ—Ä–Ω—ã–µ –Ω–æ—Ä–º—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, —è–¥–µ—Ä–Ω—É—é), –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å –º–µ—Ç–æ–¥—ã —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –∏–ª–∏ —á–∏—Å–ª–µ–Ω–Ω–æ. –ú–æ–∂–Ω–æ –≤–æ—Ç –∏–∑—É—á–∏—Ç—å –ø–æ—Å—Ç –∞–≤—Ç–æ—Ä–∞ –ø–æ —Ç–µ–º–µ –≤—ã–≤–æ–¥–∞ –æ–¥–Ω–æ–≥–æ –∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Å–µ–≥–æ–¥–Ω—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è LLM.\n–í –Ω–∞—à–µ–π —Å—Ç–∞—Ç—å–µ –º—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã –≤–µ—Å–æ–≤ –Ω–∞ –¥–≤–∞ —Ñ–∞–∫—Ç–æ—Ä–∞, —Ñ–æ—Ä–º–∏—Ä—É—é—â–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ö–∞—à–∏–Ω–∞. –û–¥–Ω–∞–∫–æ, –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —É–Ω–∏—Ç–∞—Ä–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã –ø–ª–æ—Ö–æ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ç–∞–∫–∏—Ö —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–π. –í —Ä–∞–º–∫–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è —ç—Ç–æ–≥–æ —Ñ–µ–Ω–æ–º–µ–Ω–∞.–ú–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞—à –Ω–µ–¥–∞–≤–Ω–∏–π –¥–æ–∫–ª–∞–¥.\n–í —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è –º–µ—Ç–æ–¥–∞ —Ä–µ–¥—É–∫—Ü–∏–∏ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–π –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏. –í —Ä–∞–º–∫–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å —Å –¥—Ä—É–≥–∏–º–∏ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–º–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –í–æ–∑–º–æ–∂–Ω–æ, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—É—é –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é –¥–ª—è –¥—Ä—É–≥–∏—Ö –º–µ—Ç–æ–¥–æ–≤.\n–°–æ–∑–¥–∞–Ω–∏–µ –ø–∏—Ç–æ–Ω –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ - —á–µ—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞ –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞ –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –∑–∞–ø—É—Å–∫–∞–º–∏, –µ–¥–∏–Ω—ã–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º, –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ–º –≥—Ä–∞—Ñ–∏–∫–æ–≤.\n–ò–∑—É—á–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏\n\n–°—Ç–∞—Ç—å—è\n–°—Ç–∞—Ç—å—è\n\n–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –º–æ–∂–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∫–∞–∫ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—é –≠–π–ª–µ—Ä–∞ –æ–±—ã–∫–Ω–æ–≤–µ–Ω–Ω–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —É—Ä–∞–≤–Ω–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞. –û–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è, —É—Å–∫–æ—Ä–µ–Ω–Ω—ã–º –º–µ—Ç–æ–¥–∞–º —Ç–æ–∂–µ –º–æ–∂–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏—Ö –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –∞–Ω–∞–ª–æ–≥–∏. –í —Ä–∞–º–∫–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏–∑—É—á–∏—Ç—å –≥–¥–µ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω—ã –º–æ–≥—É—Ç –±—ã—Ç—å —Ç–∞–∫–∏–µ –∞–Ω–∞–ª–æ–≥–∏–∏, –¥–æ–∫–∞–∑–∞—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —É—Å–∫–æ—Ä–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –≤ –≤—ã–ø—É–∫–ª–æ–º —Å–ª—É—á–∞–µ, –∏–∑—É—á–∏—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è —Å–ª—É—á–∞—è –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏.\n–î–∞–≤–∞–π—Ç–µ –∑–∞–ø—É—Å—Ç–∏–º LLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞-—á–µ—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞. –ë—É–¥–µ–º –¥–∞–≤–∞—Ç—å –µ–º—É –ª–æ–∫–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–µ, –ø—Ä–æ—Å–∏—Ç—å –≤–µ—Ä–Ω—É—Ç—å —Å–ª–µ–¥—É—é—â—É—é –∏—Ç–µ—Ä–∞—Ü–∏—é –∏ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å, –∫–∞–∫ –æ–Ω –±—É–¥–µ—Ç —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –Ω–∞ —á—Ç–æ –ø–æ—Ö–æ–∂–∞ —ç—Ç–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è.\n–í —Å—Ç–∞—Ç—å–µ Dataset distillation –∞–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å 10 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ—Ö–æ–∂–∏—Ö –Ω–∞ MNIST —Ç–∞–∫, —á—Ç–æ —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –Ω–∞ MNIST –±—É–¥–µ—Ç 94% –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –Ω–∞ —ç—Ç–∏—Ö 10 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. –í —Ä–∞–º–∫–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —ç—Ç–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –¥–ª—è –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ (Tiny Stories).\n–í–∑—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–≤ –∏ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–∞—Ä—Ç—ã loss surface –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞.\n–í –Ω–µ–¥–∞–≤–Ω–µ–π —Å—Ç–∞—Ç—å–µ –∞–≤—Ç–æ—Ä—ã –∏—Å—Å–ª–µ–¥—É—é—Ç –∫–∞–∫ –Ω–∞–¥–æ –∏–∑–º–µ–Ω—è—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è (batch size, learning rate, etc). –ú–æ–∂–Ω–æ –∏–∑—É—á–∏—Ç—å –∏ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –¥—Ä—É–≥–∏—Ö/–º–∞–ª–µ–Ω—å–∫–∏—Ö –º–æ–¥–µ–ª—è—Ö.\n–í —Å—Ç–∞—Ç—å–µ –∞–≤—Ç–æ—Ä—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–ª–∏ —Ñ–µ–Ω–æ–º–µ–Ω —Ç–æ–≥–æ, —á—Ç–æ Adam –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è —Ä–∞–∑–Ω–∏—Ü–∞ –Ω–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞. –ì–∏–ø–æ—Ç–µ–∑–∞ –∞–≤—Ç–æ—Ä–æ–≤ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ —è–∑—ã–∫–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –∏–º–µ—é—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É. –í —Ä–∞–º–∫–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏–∑–º–µ–Ω–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é —Ç–∞–∫, —á—Ç–æ–±—ã –∫–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–∞ –±—ã–ª –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –ø–æ —Ç–æ–∫–µ–Ω–∞–º –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –≤–ª–∏—è–µ—Ç –ª–∏ —ç—Ç–æ –Ω–∞ —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞–º–∏.\n–ù–∞ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ —á—É—Ç—å –ø–æ–∑–∂–µ –ø–æ—è–≤—è—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω—ã–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –≤–∑—è—Ç—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ç–µ–º—ã –ø—Ä–æ–µ–∫—Ç–∞.\n\n–ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —á–µ—Ç–∫–æ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–µ—à–∞–µ–º—É—é –∑–∞–¥–∞—á—É. –° –±–æ–ª—å—à–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é, –≤ —ç—Ç–æ–º –ø—É–Ω–∫—Ç–µ –Ω—É–∂–Ω–æ –Ω–∞–ø–∏—Å–∞—Ç—å –∑–∞–¥–∞—á—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ –¥—Ä—É–≥—É—é —Ä–µ—à–∞–µ–º—É—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∑–∞–¥–∞—á—É.\n–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –µ—Å–ª–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–æ–µ–∫—Ç–∞ –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å—Ç–∞—Ç—å–µ–π –≤—ã —Å–Ω–∞—á–∞–ª–∞ –¥–æ–ª–∂–Ω—ã –Ω–∞–ø–∏—Å–∞—Ç—å —Å–≤–æ—é –∑–∞–¥–∞—á—É –≤ —Ä–∞–º–∫–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞ - —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è –≤ —á—ë–º-—Ç–æ, –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏, –ø–æ–≤—Ç–æ—Ä–∏—Ç—å —á–∏—Å–ª–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø—Ä–∏–¥—É–º–∞—Ç—å –¥—Ä—É–≥–∏–µ —á–∏—Å–ª–µ–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ê –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Ç–∞–∫ –∂–µ –ø—Ä–∏–≤–µ—Å—Ç–∏ –≤ –Ω–∞–∏–±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–º –≤–∏–¥–µ —Ä–µ—à–∞–µ–º—É—é –∑–∞–¥–∞—á—É –∏–∑ —Å—Ç–∞—Ç—å–∏. –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –∏ –¥–∞–ª–µ–µ –≤–∞–∂–Ω–æ –Ω–µ –ø—Ä–∏—Å–≤–∞–∏–≤–∞—Ç—å —Å–µ–±–µ —á—É–∂–∏–µ –∑–∞—Å–ª—É–≥–∏. –ü–æ—Å—Ç–∞—Ä–∞–π—Ç–µ—Å—å –∏–∑–±–µ–≥–∞—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –≤–∏–¥–∞: –º—ã —Ä–µ—à–∞–µ–º –∑–∞–¥–∞—á—É (–≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –º–æ–∂–Ω–æ –Ω–∞–ø–∏—Å–∞—Ç—å –∞–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ —Ä–µ—à–∞—é—Ç –∑–∞–¥–∞—á—É), –º—ã —Ö–æ—Ç–∏–º –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—É/ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏/ –ø—Ä–∏–º–µ–Ω–∏—Ç—å —á–∏—Å–ª–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –∏–ª–∏ –º–∞—Ç—Ä–∏—á–Ω–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ/ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å, —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å (–≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –∞–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç/ –∏—Å—Å–ª–µ–¥—É—é—Ç –∏ —Ç.–¥.)\n\n\n–§–æ—Ä–º–∞—Ç —Å–¥–∞—á–∏\n–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤ —Ç–∞–±–ª–∏—Ü—É –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –ø–æ–ª–µ —Å—Å—ã–ª–∫—É –Ω–∞ pdf —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Ç–µ–º—ã –∏ –∞–±—Å—Ç—Ä–∞–∫—Ç–æ–º. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –Ω–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –≤—ã —É–¥–∞–ª–∏–ª–∏ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—É–Ω–∫—Ç—ã –∏–∑ —à–∞–±–ª–æ–Ω–∞ –≤—ã—à–µ. –ü–¥—Ñ–∫–∞ –¥–æ–ª–∂–Ω–∞ –≤—ã–≥–ª—è–¥–µ—Ç—å –∫–∞–∫-—Ç–æ —Ç–∞–∫:\n\n\n\nExample of this stage of the project\n\n\n\n\n–ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—è\n\n0 –±–∞–ª–ª–æ–≤ - —Ñ–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω –≤–æ–≤—Ä–µ–º—è/ —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –≤ –¥—Ä—É–≥–æ–º —Ñ–æ—Ä–º–∞—Ç–µ\n1-2 –±–∞–ª–ª–∞ - —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –≤–æ–≤—Ä–µ–º—è, —Ç–µ–º–∞ –Ω–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∞ –∏–ª–∏ —Ä–µ—à–∞–µ–º–∞—è –∑–∞–¥–∞—á–∞ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∞ –Ω–µ —á—ë—Ç–∫–æ\n3-4 –±–∞–ª–ª–∞ - —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –≤–æ–≤—Ä–µ–º—è, —Ç–µ–º–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∞, —Ä–µ—à–∞–µ–º–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —è—Å–Ω–∞ –∏ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∞ —á—ë—Ç–∫–æ"
  },
  {
    "objectID": "projects.html#project-proposal",
    "href": "projects.html#project-proposal",
    "title": "",
    "section": "Project proposal",
    "text": "Project proposal\nüóì 26 –∞–ø—Ä–µ–ª—è 2025. ü¶Ñ 16 –±–∞–ª–ª–æ–≤\n–ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ–Ω—è—Ç—å –Ω–∞—É—á–Ω—ã–π –ª–∞–Ω–¥—à–∞—Ñ—Ç –≤–æ–∫—Ä—É–≥ –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–¥–∞—á–∏. –î–ª—è —ç—Ç–æ–≥–æ –ø–æ—Å—Ç–∞—Ä–∞–π—Ç–µ—Å—å, —á—Ç–æ–±—ã –ø–æ—Å–ª–µ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–≥–æ –æ–±–∑–æ—Ä–∞ –±—ã–ª–∏ —è—Å–Ω—ã –æ—Ç–≤–µ—Ç—ã –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã:\n\n–ö–∞–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—ã–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã –≤ –ø–æ—Ö–æ–∂–∏—Ö —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞—Ö?\n–ï—Å—Ç—å –ª–∏ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Å—Ö–æ–∂–µ–π —Ç–µ–º–∞—Ç–∏–∫–µ?\n–ù–∞ –∫–∞–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ–ø–∏—Ä–∞—Ç—å—Å—è?\n–ö–∞–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –æ–±—É—Å–ª–∞–≤–ª–∏–≤–∞—é—Ç –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º–æ–π –∑–∞–¥–∞—á–∏?\n–° –∫–∞–∫–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è –¥–ª—è —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∑–∞–¥–∞—á–∏?\n–ï—Å—Ç—å –ª–∏ –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –¥–æ—Å—Ç—É–ø–µ –∫–æ–¥ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º–æ–π –∑–∞–¥–∞—á–∏?\n\n–ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ, –ø–æ–∏—Å–∫ –ø–æ google scholar, –ø–æ–∏—Å–∫ –ø–æ perplexity.ai, –ø–æ–∏—Å–∫ –ø–æ —Å—Å—ã–ª–∫–∞–º –≤ —Å—Ç–∞—Ç—å–µ. –í –∏–¥–µ–∞–ª–µ —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ —Ä–µ—Ü–µ–Ω–∑–∏—Ä—É–µ–º—ã–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏¬†–º–æ–Ω–æ–≥—Ä–∞—Ñ–∏–∏. –û–¥–Ω–∞–∫–æ, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏, –º–æ–∂–Ω–æ —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ —Å—Ç–∞—Ç—å–∏ –Ω–∞ arxiv, –±–ª–æ–≥–ø–æ—Å—Ç—ã –∏ –¥—Ä—É–≥–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏, —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏ –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –¥–ª—è –∑–∞–¥–∞—á–∏. –¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–µ–ª–∞—Ç—å —Å –ø–æ–º–æ—â—å—é bibtex. –ü—Ä–∏–º–µ—Ä –æ—Ç–∫—É–¥–∞ –µ–≥–æ –±—Ä–∞—Ç—å –ø—Ä–∏–≤–µ–¥—ë–Ω –Ω–∏–∂–µ:\n\n\n\n–ó–¥–µ—Å—å –ø–æ–∫–∞–∑–∞–Ω –æ–¥–∏–Ω –∏–∑ —Å–ø–æ—Å–æ–±–æ–≤ –Ω–∞–π—Ç–∏ bibtex –¥–ª—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n\n\n–ü–æ—Å–ª–µ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–≥–æ –æ–±–∑–æ—Ä–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–æ–±—Ä–∞—Ç—å –≤–æ–µ–¥–∏–Ω–æ –≤—Å—ë, —á—Ç–æ –±—ã–ª–æ —Ä–∞–Ω—å—à–µ, —Å–ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –∏ –ø—Ä–æ–≤–µ—Å—Ç–∏ —Ñ–∞–∑—É –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è. –ù–∞ –º–æ–π –≤–∑–≥–ª—è–¥ - —ç—Ç–æ –≤–∞–∂–Ω–µ–π—à–∏–π —ç—Ç–∞–ø –ø—Ä–æ–µ–∫—Ç–∞. –¢—É—Ç –Ω—É–∂–Ω–æ –æ—á–µ–Ω—å —á–µ—Ç–∫–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫—É–¥–∞ –∏ –∫–∞–∫ –¥–≤–∏–≥–∞—Ç—å—Å—è, –∫–∞–∫–∏–µ —Ç—Ä–æ–ø—ã —É–∂–µ –ø—Ä–æ–π–¥–µ–Ω—ã –¥—Ä—É–≥–∏–º–∏ –ª—é–¥—å–º–∏. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ –∞—Å–ø–µ–∫—Ç—ã:\n\n–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞.\nAbstract (–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ –≤ –æ–¥–∏–Ω –∞–±–∑–∞—Ü).\n–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ (–æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—Å—Ç—å –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–¥–∞—á–∏ –∏ –µ—ë —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å).\nOutcomes - –æ–ø–∏—à–∏—Ç–µ, —á—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –±—É–¥–µ—Ç –≤—ã—Ö–æ–¥–æ–º –í–∞—à–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ (–∫–æ–¥, —Ç–µ–æ—Ä–µ–º–∞, —á–∏—Å–ª–µ–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, —Ç–µ–ª–µ–≥—Ä–∞–º –±–æ—Ç, –≤–µ–± —Å–∞–π—Ç, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ, —Ä–∞—Å—Å–∫–∞–∑).\n–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–π –æ–±–∑–æ—Ä.\n–î–µ—Ç–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω —Ä–∞–±–æ—Ç. –Ø—Å–Ω–æ, —á—Ç–æ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –æ–Ω –±—É–¥–µ—Ç –º–µ–Ω—è—Ç—å—Å—è, –æ–¥–Ω–∞–∫–æ –Ω–∞–ª–∏—á–∏–µ –ø–ª–∞–Ω–∞ –∑–¥–µ—Å—å –ª—É—á—à–µ –µ–≥–æ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è.\n–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –ü–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏, –ø—Ä–∏–≤–µ–¥–∏—Ç–µ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –∏ –∏–∑–º–µ—Ä—è–µ–º—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏, –ø–æ –∫–æ—Ç–æ—Ä—ã–º –º–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –í–∞—à–µ —Ä–µ—à–µ–Ω–∏–µ¬†–ø—Ä–æ–µ–∫—Ç - —ç—Ç–æ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, —Å–æ—Ü. –æ–ø—Ä–æ—Å, –ª–æ–≥–∏—á–µ—Å–∫–æ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –∏ —Ç.–¥. –û—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ —ç—Ç–æ–≥–æ –ø—É–Ω–∫—Ç–∞ - –¥–æ–≥–æ–≤–æ—Ä–∏—Ç—å—Å—è –Ω–∞ –±–µ—Ä–µ–≥—É –æ —Ç–æ–º, –∫–∞–∫ –º—ã —Å–º–æ–∂–µ–º –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å —Ä–∞–±–æ—Ç—É, –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω—É—é –≤ –ø—Ä–æ–µ–∫—Ç–µ. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–µ–∫—Ç–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å ‚Äú–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º‚Äù –≤ —Ç–æ–º —Å–º—ã—Å–ª–µ, —á—Ç–æ –º—ã —Å–æ–±—Ä–∞–ª–∏—Å—å –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ –∫ –∫–∞–∫–æ–º—É-—Ç–æ –∫–ª–∞—Å—Å—É –∑–∞–¥–∞—á –∏ —É –Ω–∞—Å –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å. –≠—Ç–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ, —Ç–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ø—Ä–æ—Å—Ç–æ –æ–ø–∏—Å–∞—Ç—å —ç—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å (–º—ã –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª–∏ –∏ –Ω–µ –≤—ã—à–ª–æ, –Ω–æ –∑–∞—Ç–æ –≤–æ—Ç —Ç–∞–∫–æ–µ –≤–æ—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ –Ω–∞–±–ª—é–¥–∞–ª–∏).\n–û—Ç—á—ë—Ç –æ —Ñ–∞–∑–µ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è. –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —à–∏—Ä–æ–∫–∏–º–∏ –º–∞–∑–∫–∞–º–∏ –ø—Ä–∏—Å—Ç—É–ø–∏—Ç—å –∫ —Ä–∞–±–æ—Ç–µ –Ω–∞–¥ –ø—Ä–æ–µ–∫—Ç–æ–º. –ï—Å–ª–∏ –µ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ - –Ω—É–∂–Ω–æ –µ–≥–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å, –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞—à–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, –ø–æ–∫–∞–∑–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –≤—ã —Å—Ç–æ–ª–∫–Ω—É–ª–∏—Å—å. –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –ø—Ä–µ–¥–ø—Ä–∏–Ω—è—Ç—å –ø–µ—Ä–≤—ã–µ —à–∞–≥–∏ –∫ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–µ–∫—Ç–∞. –°–¥–µ–ª–∞—Ç—å —á—Ç–æ-–Ω–∏–±—É–¥—å —Å –Ω–∞—Å–∫–æ–∫–∞. –°–æ–≤—Å–µ–º –∏–¥–µ–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å –∫–∞–∫–æ–π-–Ω–∏–±—É–¥—å –ø—Ä–æ—Ç–æ—Ç–∏–ø (–µ—Å–ª–∏ —ç—Ç–æ –ø—Ä–∏–º–µ–Ω–∏–º–æ –∫ –ø—Ä–æ–µ–∫—Ç—É).\n\n\n–§–æ—Ä–º–∞—Ç —Å–¥–∞—á–∏\n–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤ —Ç–∞–±–ª–∏—Ü—É –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –ø–æ–ª–µ —Å—Å—ã–ª–∫—É –Ω–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π pdf —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–º –æ–±–∑–æ—Ä–æ–º –∏ project proposal —Å —É—á—ë—Ç–æ–º —Ñ–∏–¥–±–µ–∫–∞ –ø–æ –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É —ç—Ç–∞–ø—É.\n\n\n–ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—è\n–ë–∞–ª–ª—ã –±—É–¥—É—Ç —Å–Ω–∏–º–∞—Ç—å—Å—è –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Å–ª—É—á–∞—è—Ö (—Å–ø–∏—Å–æ–∫ –Ω–µ –ø–æ–ª–Ω—ã–π):\n\n–ú–µ–Ω–µ–µ 5 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ —Ç–µ–º–µ.\n–†–∞–±–æ—Ç–∞ —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –±—ã–ª–∞ –ø—Ä–æ–≤–µ–¥–µ–Ω–∞ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ, —Å—Å—ã–ª–∫–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã —Ä–∞–¥–∏ —Å—Å—ã–ª–æ–∫, –∞ –Ω–µ —Ä–∞–¥–∏ —Å—É—Ç–∏.\n–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–≥–æ –æ–±–∑–æ—Ä–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –Ω–µ –ø–æ–Ω—è—Ç–Ω–æ, –∫–∞–∫–æ–µ –º–µ—Å—Ç–æ –∑–∞–Ω–∏–º–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç –Ω–∞ –Ω–∞—É—á–Ω–æ–º –ª–∞–Ω–¥—à–∞—Ñ—Ç–µ.\n\n–ù–µ —É—á—Ç–µ–Ω—ã –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –ø–æ –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É —ç—Ç–∞–ø—É, –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏.\n–ü–ª–∞–Ω —Ä–∞–±–æ—Ç –Ω–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π, –æ—á–µ–Ω—å –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–π. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ —Ç—è–∂–µ–ª–æ —É–≤–µ—Ä–µ–Ω–Ω–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —Ç–≤–æ—Ä—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ (–¥–æ–∫–∞–∑–∞—Ç—å —Ç–µ–æ—Ä–µ–º—É). –ó–¥–µ—Å—å –ª—É—á—à–µ –ø–∏—Å–∞—Ç—å —á—É—Ç—å –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥–æ–∫–∞–∑–∞—Ç—å/ –æ–±–æ–±—â–∏—Ç—å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –∏–∑ –¥—Ä—É–≥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞).\n–ù–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞.\n–ù–µ—Ç –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–≥–æ –æ–±–∑–æ—Ä–∞.\n–ù–µ –Ω–∞–ø–∏—Å–∞–Ω —á—ë—Ç–∫–∏–π –≤—ã—Ö–æ–¥ (outcomes) –ø—Ä–æ–µ–∫—Ç–∞.\n–ù–µ—Ç –æ—Ç—á—ë—Ç–∞ –æ —Ñ–∞–∑–µ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è.\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ proposal –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞¬†–ø–ª–æ—Ö–æ –ø–æ–¥–ø–∏—Å–∞–Ω—ã."
  },
  {
    "objectID": "projects.html#–ø–µ—Ä–≤—ã–π-—á–µ—Ä–Ω–æ–≤–∏–∫-–ø–æ—Å—Ç–µ—Ä–∞",
    "href": "projects.html#–ø–µ—Ä–≤—ã–π-—á–µ—Ä–Ω–æ–≤–∏–∫-–ø–æ—Å—Ç–µ—Ä–∞",
    "title": "",
    "section": "–ü–µ—Ä–≤—ã–π —á–µ—Ä–Ω–æ–≤–∏–∫ –ø–æ—Å—Ç–µ—Ä–∞",
    "text": "–ü–µ—Ä–≤—ã–π —á–µ—Ä–Ω–æ–≤–∏–∫ –ø–æ—Å—Ç–µ—Ä–∞\nüóì 6 –º–∞—è 2025. ü¶Ñ 10 –±–∞–ª–ª–æ–≤\n–ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —á–µ—Ä–Ω–æ–≤–∏–∫ –ø–æ—Å—Ç–µ—Ä–∞ –≤ –ª–∞—Ç–µ—Ö–µ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ —Ä–∞–∑–¥–µ–ª—ã, –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º - —á—Ç–æ –≤ –∫–∞–∫–æ–º —Ä–∞–∑–¥–µ–ª–µ –≥–¥–µ –±—É–¥–µ—Ç. –î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –ø—Ä–∏–≤–µ–¥–µ–Ω üìù \\LaTeX —à–∞–±–ª–æ–Ω —Å üìú –ø—Ä–∏–º–µ—Ä–æ–º.\n–ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Ç–∞–∫–∂–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–ª–∞–Ω –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤–∞—à–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞. –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å:\n\n—Å—Ç–∞—Ç—å—è –≤ –∂—É—Ä–Ω–∞–ª–µ\n—Å—Ç–∞—Ç—å—è –Ω–∞ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏\n—Å—Ç–∞—Ç—å—è –¥–ª—è –ª–µ—Ç–Ω–µ–π —à–∫–æ–ª—ã\n–¥–æ–∫–ª–∞–¥ –Ω–∞ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏\n–ø—É–±–ª–∏–∫–∞—Ü–∏—è –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ\n—Å—Ç–∞—Ç—å—è –≤ –±–ª–æ–≥–µ\n–≤–∏–¥–µ–æ –Ω–∞ YouTube–∫–∞–Ω–∞–ª–µ –∏ —Ç.–¥.\n\n–í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, —ç—Ç–æ—Ç –ø–ª–∞–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞—Ç—ã, –≤—ã–±—Ä–∞–Ω–Ω—ã–π –∂—É—Ä–Ω–∞–ª, –∫—É–¥–∞ —ç—Ç–æ –±—É–¥–µ—Ç –ø–æ–¥–∞–≤–∞—Ç—å—Å—è.\n\n–§–æ—Ä–º–∞—Ç —Å–¥–∞—á–∏\n–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤ —Ç–∞–±–ª–∏—Ü—É –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –ø–æ–ª–µ —Å—Å—ã–ª–∫—É –Ω–∞ —á–µ—Ä–Ω–æ–≤–∏–∫ –ø–æ—Å—Ç–µ—Ä–∞ –≤ pdf –∏ –ø–ª–∞–Ω –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ —Å —É—á—ë—Ç–æ–º —Ñ–∏–¥–±–µ–∫–∞ –ø–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–º —ç—Ç–∞–ø–∞–º. –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —Ç–∞–∫–∂–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–º –≤ —Ç–∞–±–ª–∏—Ü–µ —É–∫–∞–∑–∞—Ç—å, –∫—Ç–æ –∏–º–µ–Ω–Ω–æ —á—Ç–æ —Å–¥–µ–ª–∞–ª (–ª–∏—á–Ω—ã–π –≤–∫–ª–∞–¥ –∫–∞–∂–¥–æ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞ –∫–æ–º–∞–Ω–¥—ã).\n\n\n–ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—è\n–ë–∞–ª–ª—ã –±—É–¥—É—Ç —Å–Ω–∏–º–∞—Ç—å—Å—è –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Å–ª—É—á–∞—è—Ö (—Å–ø–∏—Å–æ–∫ –Ω–µ –ø–æ–ª–Ω—ã–π):\n\n–ß–µ—Ä–Ω–æ–≤–∏–∫ –ø–æ—Å—Ç–µ—Ä–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω –Ω–µ –≤ –ª–∞—Ç–µ—Ö–µ\n–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–æ—Å—Ç–µ—Ä–∞ –Ω–µ–ª–æ–≥–∏—á–Ω–∞ –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —á–µ—Ç–∫–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ä–∞–∑–¥–µ–ª—ã\n–ù–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ø–ª–∞–Ω –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞\n–ù–µ —É—á—Ç–µ–Ω—ã –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –ø–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–º —ç—Ç–∞–ø–∞–º, –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏"
  },
  {
    "objectID": "projects.html#–ø–æ—Å—Ç–µ—Ä",
    "href": "projects.html#–ø–æ—Å—Ç–µ—Ä",
    "title": "",
    "section": "–ü–æ—Å—Ç–µ—Ä",
    "text": "–ü–æ—Å—Ç–µ—Ä\nüóì 13 –º–∞—è 2025. ü¶Ñ 20 –±–∞–ª–ª–æ–≤\n–ö —ç—Ç–æ–º—É —ç—Ç–∞–ø—É –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≥–æ—Ç–æ–≤ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç–µ—Ä –≤ –ª–∞—Ç–µ—Ö–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–æ–µ–∫—Ç–∞. –ü–æ–¥–≤–µ–¥–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤, –ø—É–±–ª–∏—á–Ω–∞—è –∑–∞—â–∏—Ç–∞ –ø—Ä–æ–µ–∫—Ç–∞. –û—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏–µ —Å—Ç—É–¥–µ–Ω—Ç–∞ –∏ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –í—ã—Å—Ç—É–ø–ª–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–Ω—è—Ç–Ω—ã–º, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º, –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–º.\n\n–§–æ—Ä–º–∞—Ç —Å–¥–∞—á–∏\n–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤ —Ç–∞–±–ª–∏—Ü—É –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –ø–æ–ª–µ —Å—Å—ã–ª–∫—É –Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç–µ—Ä –≤ pdf –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å—Å—è –∫ –ø—É–±–ª–∏—á–Ω–æ–π –∑–∞—â–∏—Ç–µ –ø—Ä–æ–µ–∫—Ç–∞. –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —Ç–∞–∫–∂–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–º –≤ —Ç–∞–±–ª–∏—Ü–µ —É–∫–∞–∑–∞—Ç—å, –∫—Ç–æ –∏–º–µ–Ω–Ω–æ —á—Ç–æ —Å–¥–µ–ª–∞–ª (–ª–∏—á–Ω—ã–π –≤–∫–ª–∞–¥ –∫–∞–∂–¥–æ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞ –∫–æ–º–∞–Ω–¥—ã).\n\n\n–ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—è\n–ë–∞–ª–ª—ã –±—É–¥—É—Ç —Å–Ω–∏–º–∞—Ç—å—Å—è –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Å–ª—É—á–∞—è—Ö (—Å–ø–∏—Å–æ–∫ –Ω–µ –ø–æ–ª–Ω—ã–π): * –§–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç–µ—Ä –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç * –í—ã—Å—Ç—É–ø–ª–µ–Ω–∏–µ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–æ, –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ –∏–ª–∏ –Ω–µ–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ * –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–µ–∫—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ –∏–ª–∏ –Ω–µ–ø–æ–ª–Ω–æ * –ù–µ —É—á—Ç–µ–Ω—ã –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –ø–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–º —ç—Ç–∞–ø–∞–º\n–í—Å–µ –¥–µ–¥–ª–∞–π–Ω—ã –ø–æ–Ω–∏–º–∞—é—Ç—Å—è –∫–∞–∫ 23:59:59 –ø–æ –ú–æ—Å–∫–æ–≤—Å–∫–æ–º—É –≤—Ä–µ–º–µ–Ω–∏."
  },
  {
    "objectID": "exam/spring_exam_keys.html",
    "href": "exam/spring_exam_keys.html",
    "title": "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏",
    "section": "",
    "text": "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏\n\n–ü–æ–∫–∞–∑–∞—Ç—å, —á—Ç–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–Ω—Ç–∏–≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ - –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞–∏—Å–∫–æ—Ä–µ–π—à–µ–≥–æ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —É–±—ã–≤–∞–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏.\n\n\n\n\n\n\n–ü—É—Å—Ç—å f –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞, –∑–∞–¥–∞–¥–∏–º –∏—Å–∫–æ–º–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —É–±—ã–≤–∞–Ω–∏—è - h - \\| h \\| = 1. –¢–æ–≥–¥–∞ –µ—ë –∞–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è: f(x + \\alpha h) = f(x) + \\alpha \\langle \\nabla f(x), h \\rangle + o(\\alpha) \nf(x + \\alpha h) &lt; f(x) \\Rightarrow \\alpha  \\langle \\nabla f(x), h \\rangle + o(\\alpha) &lt; 0.\n –ü—Ä–∏ \\alpha \\rightarrow +0 –ø–æ–ª—É—á–∞–µ–º: \\alpha \\langle \\nabla f(x), h \\rangle \\leqslant 0 \n\\| \\langle \\nabla f(x), h \\rangle \\| \\leqslant \\| \\nabla f(x) \\| \\| h \\| \\leqslant \\| \\nabla f(x) \\|\n \n\\langle \\nabla f(x), h \\rangle \\geqslant -\\| \\nabla f(x) \\| \\Rightarrow h = \\frac{-\\nabla f(x)}{\\| \\nabla f(x) \\|}, \\text{ —á.—Ç.–¥.}\n\n\n\n\n–ú–µ—Ç–æ–¥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞.\n\n\n\n\n\n\n–†–µ—à–∞–µ–º –∑–∞–¥–∞—á—É –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ \nf(x) \\to \\min_{x \\in \\mathbb{R}^d}\n –ï—Å–ª–∏ f –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞, —Ç–æ —Ç–æ–≥–¥–∞ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞: \nx_{k + 1} = x_k - \\alpha \\nabla f(x_k)\n\n\n\n\n–ù–∞–∏—Å–∫–æ—Ä–µ–π—à–∏–π —Å–ø—É—Å–∫.\n\n\n\n\n\n\n–†–µ—à–∞–µ–º –∑–∞–¥–∞—á—É –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ \nf(x) \\to \\min_{x \\in \\mathbb{R}^d}\n –ï—Å–ª–∏ f –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞, —Ç–æ —Ç–æ–≥–¥–∞ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ –Ω–∞–∏—Å–∫–æ—Ä–µ–π—à–µ–≥–æ —Å–ø—É—Å–∫–∞: \nx_{k + 1} = x_k - \\alpha_k \\nabla f(x_k)\n \n\\alpha_k = \\arg\\min\\limits_{\\alpha \\in \\mathbb{R}^+} f(x_k - \\alpha \\nabla f(x_k)),\n —Ç.–µ. –≤—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–ª—É—á—à–∏–π —à–∞–≥ —Å–ø—É—Å–∫–∞ –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –º–µ—Ç–æ–¥–∞.\n\n\n\n–õ–∏–ø—à–∏—Ü–µ–≤–∞ –ø–∞—Ä–∞–±–æ–ª–∞ –¥–ª—è –≥–ª–∞–¥–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏.\n\n\n\n\n\n\n–ï—Å–ª–∏ f: \\mathbb{R}^n \\rightarrow \\mathbb{R} - –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞ –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç –õ–∏–ø—à–∏—Ü–µ–≤ —Å –∫–æ–Ω—Å—Ç–∞–Ω—Ç–æ–π L, —Ç–æ \\forall x, y \\in \\mathbb{R}^n: \n\\| f(y) - f(x) - \\langle \\nabla f(x), y - x \\rangle \\| \\leqslant \\frac{L}{2}\\| y - x \\|^2\n –ï—Å–ª–∏ –∑–∞—Ñ–∏–∫—Å–∏—Ä—É–µ–º x_0 \\in \\mathbb{R}^n, —Ç–æ: \n\\varphi_1(x) = f(x_0) + \\langle \\nabla f(x_0), x - x_0 \\rangle - \\frac{L}{2}\\| x - x_0 \\|^2\n \n\\varphi_2(x) = f(x_0) + \\langle \\nabla f(x_0), x - x_0 \\rangle + \\frac{L}{2}\\| x - x_0 \\|^2\n –≠—Ç–æ –¥–≤–µ –ø–∞—Ä–∞–±–æ–ª—ã, –∏ –¥–ª—è –Ω–∏—Ö –≤–µ—Ä–Ω–æ, —á—Ç–æ \\varphi_1(x) \\leqslant f(x) \\leqslant \\varphi_2(x) \\forall x\n\n\n\n–ò–ª–ª—é—Å—Ç—Ä–∞—Ü–∏—è –õ–∏–ø—à–∏—Ü–µ–≤—ã—Ö –ø–∞—Ä–∞–±–æ–ª, –º–µ–∂–¥—É –∫–æ—Ç–æ—Ä—ã–º–∏ –∑–∞–∂–∞—Ç–∞ –≥–ª–∞–¥–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è. –ß–∞—â–µ –Ω–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç –º–∞–∂–æ—Ä–∏—Ä—É—é—â–∞—è –∏–∑ –Ω–∏—Ö.\n\n\n\n\n\n–†–∞–∑–º–µ—Ä —à–∞–≥–∞ –Ω–∞–∏—Å–∫–æ—Ä–µ–π—à–µ–≥–æ —Å–ø—É—Å–∫–∞ –¥–ª—è –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏.\n\n\n\n\n\n\n–†–µ—à–∞–µ–º –∑–∞–¥–∞—á—É –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ –º–µ—Ç–æ–¥–æ–º –Ω–∞–∏—Å–∫–æ—Ä–µ–π—à–µ–≥–æ —Å–ø—É—Å–∫–∞ \nf(x) = \\frac{1}{2}x^TAx - b^Tx + c \\to \\min_{x \\in \\mathbb{R}^d}\n \n\\nabla f = \\frac{1}{2}(A + A^T)x - b\n\n–ò–∑ —É—Å–ª–æ–≤–∏—è \\nabla f(x_{k + 1})^T \\nabla f(x_k) = 0 –ø–æ–ª—É—á–∞–µ–º: \n\\alpha_k = \\frac{2 \\nabla f(x_k)^T \\nabla f(x_k)}{\\nabla f(x_k)^T (A + A^T) \\nabla f(x_k)} = \\frac{\\nabla f(x_k)^T \\nabla f(x_k)}{\\nabla f(x_k)^T \\nabla^2 f(x_k) \\nabla f(x_k)}.\n\n\n\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –∫ –ª–æ–∫–∞–ª—å–Ω–æ–º—É —ç–∫—Å—Ç—Ä–µ–º—É–º—É –¥–ª—è –≥–ª–∞–¥–∫–∏—Ö –Ω–µ–≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞.\n\n\n\n\n\n\n\\|\\nabla f(x_k)\\|^2 \\sim \\mathcal{O} \\left( \\frac{1}{k} \\right).\n\n\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –¥–ª—è –≥–ª–∞–¥–∫–∏—Ö –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞.\n\n\n\n\n\n\nf(x_k) - f^* \\sim  \\mathcal{O} \\left( \\frac{1}{k} \\right).\n\n\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –¥–ª—è –≥–ª–∞–¥–∫–∏—Ö –∏ —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞.\n\n\n\n\n\n\n\\|x_k - x^*\\|^2 \\sim \\mathcal{O} \\left( \\left(1 - \\frac{\\mu}{L}\\right)^k \\right).\n\n\n\n–°–≤—è–∑—å —Å–ø–µ–∫—Ç—Ä–∞ –≥–µ—Å—Å–∏–∞–Ω–∞ —Å –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞–º–∏ —Å–∏–ª—å–Ω–æ–π –≤—ã–ø—É–∫–ª–æ—Å—Ç–∏ –∏ –≥–ª–∞–¥–∫–æ—Å—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏.\n\n\n\n\n\n\n\\quad \\mu = \\min\\limits_{x \\in \\text{dom} f}\\lambda_{\\min}(\\nabla^2 f(x)), \\quad L = \\max\\limits_{x \\in \\text{dom} f}\\lambda_{\\max}(\\nabla^2 f(x)).\n\n\n\n–°–≤—è–∑—å —á–∏—Å–ª–∞ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–Ω–æ—Å—Ç–∏ –º–∞—Ç—Ä–∏—Ü—ã –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Å–∏–ª—å–Ω–æ–π –≤—ã–ø—É–∫–ª–æ—Å—Ç–∏ –∏ –≥–ª–∞–¥–∫–æ—Å—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏.\n\n\n\n\n\n\n\\varkappa = \\frac{L}{\\mu} = \\frac{\\lambda_{\\max}(\\nabla^2 f(x))}{\\lambda_{\\min}(\\nabla^2 f(x))} = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)} \\geq 1.\n\n\n\n–£—Å–ª–æ–≤–∏–µ –ü–æ–ª—è–∫–∞-–õ–æ—è—Å–∏–µ–≤–∏—á–∞ (–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è) –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–π.\n\n\n\n\n\n\n\\exists \\mu &gt; 0: \\quad \\| \\nabla f(x) \\|^2 \\geqslant 2\\mu(f(x) - f^*) \\quad \\forall x, –≥–¥–µ f^* - –º–∏–Ω–∏–º—É–º —Ñ—É–Ω–∫—Ü–∏–∏ f(x).\n\n\n\n–°—Ö–æ–¥–∏–º–æ—Å—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –¥–ª—è —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã—Ö –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π. –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã.\n\n\n\n\n\n\n–†–µ—à–∞–µ–º –∑–∞–¥–∞—á—É –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ –º–µ—Ç–æ–¥–æ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞. –ü—É—Å—Ç—å A \\in \\mathbb{S}_{++}^n \\Rightarrow \\nabla f = Ax - b. \n\\begin{aligned}\nf(x) &= \\frac{1}{2}x^TAx - b^Tx + c \\to \\min_{x \\in \\mathbb{R}^d} \\\\\nx_{k + 1} &= x_k - \\alpha (Ax_k - b) \\\\\n\\alpha_{opt} &= \\frac{2}{\\mu + L}, \\text{ –≥–¥–µ } \\mu = \\lambda_{\\min}(A), L = \\lambda_{\\max}(A) \\\\\n\\kappa &= \\frac{L}{\\mu} \\geqslant 1 \\\\\n\\rho &= \\frac{\\kappa - 1}{\\kappa + 1} \\\\\n\\| x_k - x^* \\| &\\leqslant \\rho^k \\| x_0 - x^* \\| \\\\\n\\end{aligned}\n\n\n\n\n–°–≤—è–∑—å PL-—Ñ—É–Ω–∫—Ü–∏–π –∏ —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π.\n\n\n\n\n\n\n–ü—É—Å—Ç—å f \\mu-—Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–∞—è –∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞—è \\Rightarrow f \\in PL.\n–û–±—Ä–∞—Ç–Ω–æ–µ –Ω–µ–≤–µ—Ä–Ω–æ - f(x) = x^2 + 3\\sin^2x \\in PL, –Ω–æ –Ω–µ —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–∞—è (–æ–Ω–∞ –≤–æ–æ–±—â–µ –Ω–µ –≤—ã–ø—É–∫–ª–∞—è).\n\n\n\n–ü—Ä–∏–º–µ—Ä –Ω–µ–≤—ã–ø—É–∫–ª–æ–π PL —Ñ—É–Ω–∫—Ü–∏–∏\n\n\n\n\n\n–ü—Ä–∏–≤–µ—Å—Ç–∏ –ø—Ä–∏–º–µ—Ä –≤—ã–ø—É–∫–ª–æ–π, –Ω–æ –Ω–µ —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–æ–π –∑–∞–¥–∞—á–∏ –ª–∏–Ω–µ–π–Ω—ã—Ö –Ω–∞–∏–º–µ–Ω—å—à–∏—Ö –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ (–≤–æ–∑–º–æ–∂–Ω–æ, —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π).\n\n\n\n\n\n\n–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –∑–∞–¥–∞—á—É –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–∏: \n\\| A x - b \\|^2 \\to \\min_{x \\in \\mathbb{R}^d},\n –≥–¥–µ –º–∞—Ç—Ä–∏—Ü–∞ A \\in \\mathbb{R}^{m\\times n}, \\ b\\in \\mathbb{R}^{m}, m &lt; n (–ª–µ–∂–∞—á–∞—è).\n\n\n\n–ü—Ä–∏–≤–µ—Å—Ç–∏ –ø—Ä–∏–º–µ—Ä —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–æ–π –∑–∞–¥–∞—á–∏ –ª–∏–Ω–µ–π–Ω—ã—Ö –Ω–∞–∏–º–µ–Ω—å—à–∏—Ö –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ (–≤–æ–∑–º–æ–∂–Ω–æ, —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π).\n\n\n\n\n\n\n–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –∑–∞–¥–∞—á—É –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–∏: \nf(x) = \\|Ax - b\\|_2^2,\n –≥–¥–µ A \\in \\mathbb{R}^{n \\times n} (—Ä–∞–Ω–≥ A = n). –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–∞, —Ç–∞–∫ –∫–∞–∫ –≥–µ—Å—Å–∏–∞–Ω –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω.\n\n\n\n–ü—Ä–∏–≤–µ—Å—Ç–∏ –ø—Ä–∏–º–µ—Ä –≤—ã–ø—É–∫–ª–æ–π –Ω–µ–≥–ª–∞–¥–∫–æ–π –∑–∞–¥–∞—á–∏ –ª–∏–Ω–µ–π–Ω—ã—Ö –Ω–∞–∏–º–µ–Ω—å—à–∏—Ö –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ (–≤–æ–∑–º–æ–∂–Ω–æ, —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π).\n\n\n\n\n\n\n–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –∑–∞–¥–∞—á—É –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–∏: \nf(x) = \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1,\n –≥–¥–µ A \\in \\mathbb{R}^{n \\times n}, \\lambda &gt; 0. –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–ø—É–∫–ª–∞, –Ω–æ –Ω–µ–≥–ª–∞–¥–∫–∞—è –∏–∑-–∑–∞ –Ω–∞–ª–∏—á–∏—è \\ell_1-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏.\n\n\n\n–ù–∏–∂–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –Ω–µ–≥–ª–∞–¥–∫–æ–π –≤—ã–ø—É–∫–ª–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤ –ø–µ—Ä–≤–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞.\n\n\n\n\n\n\nf^{\\text{best}}_k - f^* \\sim \\mathcal{O} \\left( \\frac{1}{\\sqrt{k}} \\right)\n\n\n\n–ù–∏–∂–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –Ω–µ–≥–ª–∞–¥–∫–æ–π —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤ –ø–µ—Ä–≤–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞.\n\n\n\n\n\n\nf^{\\text{best}}_k - f^* \\sim \\mathcal{O} \\left( \\frac{1}{k} \\right)\n\n\n\n–û—Ç–ª–∏—á–∏–µ —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–π –∏ –Ω–µ—É—Å–∫–æ—Ä–µ–Ω–Ω–æ–π –ª–∏–Ω–µ–π–Ω–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–ª—è –º–µ—Ç–æ–¥–æ–≤ –ø–µ—Ä–≤–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–§—É–Ω–∫—Ü–∏—è\n–ù–µ—É—Å–∫–æ—Ä–µ–Ω–Ω–∞—è\n–£—Å–∫–æ—Ä–µ–Ω–Ω–∞—è\n\n\n\n\n–ì–ª–∞–¥–∫–∞—è –∏ –≤—ã–ø—É–∫–ª–∞—è\n\\mathcal{O}\\left(\\frac{1}{k}\\right)\n\\mathcal{O}\\left(\\frac{1}{k^2}\\right)\n\n\n–ì–ª–∞–¥–∫–∞—è –∏ —Å–∏–ª—å–Ω–æ-–≤—ã–ø—É–∫–ª–∞—è (–∏–ª–∏ PL)\n\\mathcal{O}\\left((1 - \\frac{\\mu}{L})^k\\right)\n\\mathcal{O}\\left((1 - \\sqrt{\\frac{\\mu}{L}})^k\\right)\n\n\n\n\n\n\n–§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –º–µ—Ç–æ–¥–∞ —Ç—è–∂–µ–ª–æ–≥–æ —à–∞—Ä–∏–∫–∞ (–ü–æ–ª—è–∫–∞) –¥–ª—è –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏. –•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏. –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã.\n\n\n\n\n\n\n–ó–∞–¥–∞—á–∞: f(x) = \\frac{1}{2} x^T A x - b^T x \\rightarrow \\min\\limits_{x \\in \\mathbb{R}^d}, \\lambda(A) \\in [\\mu; L]. \nx_{k+1} = x_{k} ‚àí\\alpha\\nabla f(x_{k}) + \\beta(x_k‚àíx_{k‚àí1}), \\qquad 0 &lt; \\beta &lt; 1.\n –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã: \n\\alpha^* = \\dfrac{4}{(\\sqrt{L} + \\sqrt{\\mu})^2} \\qquad \\beta^* = \\left(\\dfrac{\\sqrt{L} - \\sqrt{\\mu}}{\\sqrt{L} + \\sqrt{\\mu}}\\right)^2.\n –•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏: \n\\|x_k - x^*\\|_2 \\leq \\left( \\dfrac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right)^k \\|x_0 - x^*\\|\n\n\n\n\n–°—Ö–æ–¥–∏–º–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ —Ç—è–∂–µ–ª–æ–≥–æ —à–∞—Ä–∏–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç \\alpha –∏ \\beta\n\n\n\n\n\n–§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –º–µ—Ç–æ–¥–∞ —Ç—è–∂–µ–ª–æ–≥–æ —à–∞—Ä–∏–∫–∞ (–ü–æ–ª—è–∫–∞) –¥–ª—è –≥–ª–∞–¥–∫–æ–π –≤—ã–ø—É–∫–ª–æ–π/—Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–æ–π —Ñ—É–Ω–∫—Ü–∏–π. –•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞.\n\n\n\n\n\n\n–ó–∞–¥–∞—á–∞: f(x) = \\frac{1}{2} x^T A x - b^T x \\rightarrow \\min\\limits_{x \\in \\mathbb{R}^d}, \\ f(x), \\lambda(A) \\in [\\mu; L]. \nx_{k+1} = x_{k} ‚àí\\alpha\\nabla f(x_{k}) + \\beta(x_k‚àíx_{k‚àí1}), \\qquad 0 &lt; \\beta &lt; 1.\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≤—ã–ø—É–∫–ª—ã—Ö –≥–ª–∞–¥–∫–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π: –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å—É–±–ª–∏–Ω–µ–π–Ω—ã–π \\mathcal{O}(\\frac1k). –î–ª—è —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã—Ö –≥–ª–∞–¥–∫–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–π –ª–∏–Ω–µ–π–Ω—ã–π. –ú–µ—Ç–æ–¥ –Ω–µ –∏–º–µ–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω–æ–π —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–ª–∞–¥–∫–∏—Ö (—Å–∏–ª—å–Ω–æ) –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π.\n\n\n\n–£—Å–∫–æ—Ä–µ–Ω–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –ù–µ—Å—Ç–µ—Ä–æ–≤–∞ –¥–ª—è –≤—ã–ø—É–∫–ª—ã—Ö –≥–ª–∞–¥–∫–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π.\n\n\n\n\n\n\n–ü—É—Å—Ç—å f : \\mathbb{R}^n \\rightarrow \\mathbb{R} –≤—ã–ø—É–∫–ª–∞—è –∏ L-–≥–ª–∞–¥–∫–∞—è. –£—Å–∫–æ—Ä–µ–Ω–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –ù–µ—Å—Ç–µ—Ä–æ–≤–∞ (NAG) –∏–º–µ–µ—Ç –≤–∏–¥ (x_0 = y_0,\\ \\lambda_0 = 0): \n\\begin{aligned}\n&\\textbf{–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —à–∞–≥: } &y_{k+1} &= x_k - \\frac{1}{L} \\nabla f(x_k) \\\\\n&\\textbf{–≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è: } &x_{k+1} &= (1 - \\gamma_k)y_{k+1} + \\gamma_k y_k \\\\\n&\\textbf{–í–µ—Å —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏: } &\\lambda_{k+1} &= \\frac{1 + \\sqrt{1 + 4\\lambda_k^2}}{2} \\\\\n&\\textbf{–í–µ—Å —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏: } &\\gamma_k &= \\frac{1 - \\lambda_k}{\\lambda_{k+1}}\n\\end{aligned}\n –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å \\{f(y_k)\\}_{k\\in\\mathbb{N}} —Å—Ö–æ–¥–∏—Ç—Å—è –∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é f^* —Å–æ —Å–∫–æ—Ä–æ—Å—Ç—å—é \\mathcal{O}\\left(\\frac{1}{k^2}\\right), –∞ –∏–º–µ–Ω–Ω–æ: \nf(y_k) - f^* \\leq \\frac{2L \\|x_0 - x^*\\|^2}{k^2}\n\n\n\n\n\n–£—Å–∫–æ—Ä–µ–Ω–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –ù–µ—Å—Ç–µ—Ä–æ–≤–∞ –¥–ª—è —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã—Ö –≥–ª–∞–¥–∫–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π.\n\n\n\n\n\n\n–ü—É—Å—Ç—å f : \\mathbb{R}^n \\rightarrow \\mathbb{R} \\mu-—Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–∞—è –∏ L-–≥–ª–∞–¥–∫–∞—è. –£—Å–∫–æ—Ä–µ–Ω–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –ù–µ—Å—Ç–µ—Ä–æ–≤–∞ (NAG) –∏–º–µ–µ—Ç –≤–∏–¥ (x_0 = y_0,\\ \\lambda_0 = 0): \n\\begin{aligned}\n&\\textbf{–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —à–∞–≥: } &y_{k+1} &= x_k - \\frac{1}{L} \\nabla f(x_k) \\\\\n&\\textbf{–≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è: } &x_{k+1} &= (1 + \\gamma_k)y_{k+1} - \\gamma_k y_k \\\\\n&\\textbf{–í–µ—Å —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏: } &\\gamma_k &= \\frac{\\sqrt{L} - \\sqrt{\\mu}}{\\sqrt{L} + \\sqrt{\\mu}}\n\\end{aligned}\n –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å \\{f(y_k)\\}_{k\\in\\mathbb{N}} —Å—Ö–æ–¥–∏—Ç—Å—è –∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é f^* –ª–∏–Ω–µ–π–Ω–æ: \nf(y_k) - f^* \\leq \\frac{\\mu + L}{2}\\|x_0 - x^*\\|^2_2 \\exp \\left(-\\frac{k}{\\sqrt{\\kappa}}\\right)\n\n\n\n\nA-—Å–æ–ø—Ä—è–∂–µ–Ω–Ω–æ—Å—Ç—å –¥–≤—É—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤. A-–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å. –°–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ \\langle \\cdot, \\cdot \\rangle_A.\n\n\n\n\n\n\nA-–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å (—Å–æ–ø—Ä—è–∂–µ–Ω–Ω–æ—Å—Ç—å): \nx \\perp_A y \\iff x^T A y = 0.\n\n\n\n\n–ü—Ä–æ—Ü–µ–¥—É—Ä–∞ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –ì—Ä–∞–º–∞-–®–º–∏–¥—Ç–∞.\n\n\n\n\n\n\n–ü—É—Å—Ç—å a_1, \\ldots, a_n - –õ–ù–ó –≤–µ–∫—Ç–æ—Ä—ã –∏ \\text{proj}_ba - –æ–ø–µ—Ä–∞—Ç–æ—Ä –ø—Ä–æ–µ–∫—Ü–∏–∏ a –Ω–∞ b, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –∫–∞–∫ \n\\text{proj}_ba = \\frac{\\langle a, b \\rangle}{\\langle b, b \\rangle} b,\n –û—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –ì—Ä–∞–º–∞-–®–º–∏–¥—Ç–∞: \n\\begin{aligned}\nb_1 &= a_1 \\\\\nb_2 &= a_2 - \\text{proj}_{b_1}a_2 \\\\\nb_3 &= a_3 - \\text{proj}_{b_1}a_3 - \\text{proj}_{b_2} a_3 \\\\\n\\ldots \\\\\nb_n &= a_n - \\sum_{i=1}^{n-1}\\text{proj}_{b_i}a_n\n\\end{aligned}\n\n\n\n\n\n–ú–µ—Ç–æ–¥ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π.\n\n\n\n\n\n\n–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –∑–∞–¥–∞—á—É \nf(x) = \\frac12 x^T Ax - b^T x + c \\rightarrow \\min\\limits_{x\\in\\mathbb{R}^d}\n\n–ò–¥–µ—è\n\n–í –∏–∑–æ—Ç—Ä–æ–ø–Ω–æ–º A=I –º–∏—Ä–µ, –Ω–∞–∏—Å–∫–æ—Ä–µ–π—à–∏–π —Å–ø—É—Å–∫ —Å—Ç–∞—Ä—Ç—É—é—â–∏–π –∏–∑ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π —Ç–æ—á–∫–∏ –≤ –ª—é–±–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –Ω–∞—Ç—è–Ω—É—Ç–æ–º –Ω–∞ –ª–∏–Ω–µ–π–Ω—É—é –æ–±–æ–ª–æ—á–∫—É –∏–∑ n –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã—Ö –õ–ù –≤–µ–∫—Ç–æ—Ä–æ–≤ –±—É–¥–µ—Ç —Å—Ö–æ–¥–∏—Ç—Å—è –∑–∞ n —à–∞–≥–æ–≤ –≤ —Ç–æ—á–Ω–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–µ. –ú—ã –ø–æ–ø—ã—Ç–∞–µ–º—Å—è –≤ —Å–ª—É—á–∞–µ A \\neq I –ø—Ä–æ–≤–µ—Å—Ç–∏ A-–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–∞–ª–∏–∑–∞—Ü–∏—é, —á—Ç–æ–±—ã ‚Äú–Ω–∞–∏—Å–∫–æ—Ä–µ–π—à–∏–º‚Äù –æ–±—Ä–∞–∑–æ–º —Å–ø—É—Å–∫–∞—Ç—å—Å—è –≤ –∏–∑–º–µ–Ω–µ–Ω–Ω–æ–º –±–∞–∑–∏—Å–µ.\n–ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º –∏–º–µ–µ—Ç—Å—è –Ω–∞–±–æ—Ä –∏–∑ n –ª–∏–Ω–µ–π–Ω–æ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö A-–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤(–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π) d_0, \\ldots, d_{n-1} (–∫–æ—Ç–æ—Ä—ã–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, –±—ã–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã –≤ —Ö–æ–¥–µ A-–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –ì-–®).\n–ú—ã —Ö–æ—Ç–∏–º —Å–æ–∑–¥–∞—Ç—å –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç –æ—Ç x_0 –∫ x^* –ø–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ —à–∞–≥–∞–º–∏, —Ç.–µ. x_0 - x^* = \\sum\\limits_{i=0}^{d-1}\\alpha_i d_i, –≥–¥–µ \\alpha_i - –∏–∑ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞.\n\n\n\n\n–ú–µ—Ç–æ–¥ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.\n\n\n\n\n\n\n–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –∑–∞–¥–∞—á—É \nf(x) = \\frac12 x^T Ax - b^T x + c \\rightarrow \\min\\limits_{x\\in\\mathbb{R}^d}\n –ú–µ—Ç–æ–¥ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤:\n\nr_0:= b - A x_0\n\\text{if } r_0 \\text{sufficiently small, then return } x_0 \\text{ as result}\nd_0 := r_0\nk := 0\n\\text{while } r_{k+1} \\text{ is not sufficiently small}:\n\n\\alpha_k := \\frac{r_k^Tr_k}{d_k^TAd_k}\nx_{k+1} := x_k + \\alpha_k d_k\nr_{k+1} := r_k - \\alpha_k A d_k\n\\beta_k := \\frac{r^T_{k+1}r_{k + 1}}{r_k^Tr_k}\nd_{k + 1} := r_{k + 1} + \\beta_k d_k\nk := k + 1\n\n\\text{return } x_{k+1} \\text{ as result.}\n\n\n\n\n–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≤ –æ—Ç —Å–ø–µ–∫—Ç—Ä–∞ –º–∞—Ç—Ä–∏—Ü—ã.\n\n\n\n\n\n\n–ï—Å–ª–∏ –º–∞—Ç—Ä–∏—Ü–∞ A –∏–º–µ–µ—Ç —Ç–æ–ª—å–∫–æ r —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö —á–∏—Å–µ–ª, —Ç–æ–≥–¥–∞ –º–µ—Ç–æ–¥ —Å–æ–ø—Ä—è–∂—ë–Ω–Ω—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —Å—Ö–æ–¥–∏—Ç—Å—è –∑–∞ r –∏—Ç–µ—Ä–∞—Ü–∏–π.\n \n\n\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞.\n\n\n\n\n\n\n\n\\| x_k  - x^* \\|_{A} \\leqslant2 \\left(\\frac{\\sqrt{\\kappa(A)} - 1}{\\sqrt{\\kappa (A)} + 1}\\right)^k \\|x_0 - x^*\\|_{A}\n –ò–º–µ–µ—Ç –º–µ—Å—Ç–æ –æ—Ü–µ–Ω–∫–∞ —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –ø—Ä–∏ –∑–∞–¥–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ \\varepsilon: \\|x_k - x^*\\|_A \\leqslant \\varepsilon \\|x_0 - x^*\\|_A \nk \\leqslant \\left\\lceil \\frac{1}{2} \\sqrt{\\kappa(A)} \\ln\\Bigr( \\frac{2}{\\varepsilon} \\Bigl) \\right\\rceil\n\n\n\n\n–ú–µ—Ç–æ–¥ –ü–æ–ª–∞–∫–∞-–†–∏–±—å–µ—Ä–∞.\n\n\n\n\n\n\n–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã—Ö –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π.\n–ë–µ–∑ –∑–Ω–∞–Ω–∏—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è —à–∞–≥ 2 –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –º–µ—Ç–æ–¥–∞ —Å–æ–ø—Ä—è–∂—ë–Ω–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –≤–º–µ—Å—Ç–æ –ø–æ–¥—Å—á—ë—Ç–∞ \\alpha –∏–∑ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ f(x_k + \\alpha_k d_k) –Ω–∞—Ö–æ–¥–∏—Ç \\alpha –æ–±—ã—á–Ω—ã–º –ª–∏–Ω–µ–π–Ω—ã–º –ø–æ–∏—Å–∫–æ–º. \n\\beta_k = \\frac{\\nabla f(x_{k+1})^T ( \\nabla f(x_{k+1}) - \\nabla f(x_k))}{d_k ^T ( \\nabla f(x_{k+1}) - \\nabla f(x_k))}\n\n \n\n\n\n–ú–µ—Ç–æ–¥ –ù—å—é—Ç–æ–Ω–∞.\n\n\n\n\n\n\n–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∑–∞–¥–∞—á–∞ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–∏ —Å –Ω–µ–≤—ã—Ä–æ–∂–¥–µ–Ω–Ω—ã–º –≥–µ—Å—Å–∏–∞–Ω–æ–º. \nx_{k+1} = x_k - [\\nabla^2f(x_k)]^{-1} \\nabla f(x_k)\n\n\n\n\n–°—Ö–æ–¥–∏–º–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –ù—å—é—Ç–æ–Ω–∞ –¥–ª—è –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏.\n\n\n\n\n\n\n–ú–µ—Ç–æ–¥ –ù—å—é—Ç–æ–Ω–∞ —Å—Ö–æ–¥–∏—Ç—Å—è –¥–ª—è –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –∑–∞ –æ–¥–Ω—É –∏—Ç–µ—Ä–∞—Ü–∏—é, –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏, —á—Ç–æ –≥–µ—Å—Å–∏–∞–Ω –Ω–µ–≤—ã—Ä–æ–∂–¥–µ–Ω. –°–ª–µ–¥—É–µ—Ç –∏–∑ –º–µ—Ç–æ–¥–∞ –ù—å—é—Ç–æ–Ω–∞ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Ç–µ–π–ª–æ—Ä–æ–≤—Å–∫–æ–π –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏: \nf(x) = f(x_k) + \\nabla f(x_k)^T(x - x_k) + \\frac{1}{2} (x-x_k)^T \\nabla^2 f(x_k) (x-x_k), \\quad \\nabla f(x_{k+1}) = 0\n\n\n\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞ –ù—å—é—Ç–æ–Ω–∞ –¥–ª—è —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã—Ö –≥–ª–∞–¥–∫–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π - –∫—É–¥–∞ –∏ –∫–∞–∫ —Å—Ö–æ–¥–∏—Ç—Å—è.\n\n\n\n\n\n\n–ü—É—Å—Ç—å f(x) —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–∞ –¥–≤–∞–∂–¥—ã –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞ –Ω–∞ \\mathbb{R}^n –∏ –≤—ã–ø–æ–Ω—è—é—Ç—Å—è –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞: \\mu I_n\\preceq \\nabla ^2f(x) \\preceq L I_n. –¢–æ–≥–¥–∞ –º–µ—Ç–æ–¥ –ù—å—é—Ç–æ–Ω–∞ —Å –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º —à–∞–≥–æ–º –ª–æ–∫–∞–ª—å–Ω–æ —Å—Ö–æ–¥–∏—Ç—Å—è –∫ —Ä–µ—à–µ–Ω–∏—é —Å–æ —Å–≤–µ—Ä—Ö–ª–∏–Ω–µ–π–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é. –ï—Å–ª–∏ –≤–¥–æ–±–∞–≤–æ–∫, –ì–µ—Å—Å–∏–∞–Ω M-–õ–∏–ø—à–∏—Ü–µ–≤, —Ç–æ–≥–¥–∞ –º–µ—Ç–æ–¥ —Å—Ö–æ–¥–∏—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–æ –∫ x^* —Å –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é.\n\n\n\n–î–µ–º–ø—Ñ–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ù—å—é—Ç–æ–Ω–∞.\n\n\n\n\n\n\n\nx_{k+1} = x_k - \\alpha_k \\left[\\nabla^2f(x_k)\\right]^{-1} \\nabla f(x_k),\\;\\;\\; \\alpha_k \\in [0,1]\n –≥–¥–µ \\alpha_k –Ω–∞—Ö–æ–¥—è—Ç —Å –ø–æ–º–æ—â—å—é –ª–∏–Ω–µ–π–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞. –°—Ö–æ–¥–∏–º–æ—Å—Ç—å –≥–ª–æ–±–∞–ª—å–Ω–∞—è.\n\n\n\n–ò–¥–µ—è –∫–≤–∞–∑–∏–Ω—å—é—Ç–æ–Ω–æ–≤—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤. –ú–µ—Ç–æ–¥ SR-1.\n\n\n\n\n\n\n\n\\min_{x\\in \\R^{d}} f(x)\n –ü—É—Å—Ç—å x_{0} \\in \\mathbb{R}^n, B_{0} \\succ 0. –î–ª—è k = 1, 2, 3, \\dots, –ø–æ–≤—Ç–æ—Ä–∏–º:\n\n–†–µ—à–∏—Ç—å B_{k} d_{k} = -\\nabla f(x_{k}) –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ d_k.\n–û–±–Ω–æ–≤–∏—Ç—å x_{k+1} = x_{k} + \\alpha_k d_{k}.\n–í—ã—á–∏—Å–ª–∏—Ç—å B_{k+1} –∏–∑ B_{k}. \nB_{k+1} = B_k + \\frac{(\\Delta y_k - B_k d_k)(\\Delta y_k - B_k d_k)^T}{(\\Delta y_k - B_k d_k)^T d_k}, \\quad \\Delta y_k = \\nabla f(x_{k+1}) - \\nabla f(x_{k}).\n\n\n\n\n\n–ü—Ä–æ–µ–∫—Ü–∏—è.\n\n\n\n\n\n\n–ü—Ä–æ–µ–∫—Ü–∏—è —Ç–æ—á–∫–∏ y \\in \\mathbb{R}^n \\text{ –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–æ } S \\subseteq \\mathbb{R}^n \\text{ —ç—Ç–æ —Ç–æ—á–∫–∞ }\\text{proj}_S(y) \\in S: \n\\text{proj}_S(y) = \\arg \\min\\limits_{x \\in S}\\frac{1}{2}||x - y||^2_2\n\n\n\n\n–î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —É—Å–ª–æ–≤–∏–µ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ü–∏–∏ —Ç–æ—á–∫–∏ –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–æ.\n\n\n\n\n\n\n–ï—Å–ª–∏ S \\subseteq \\mathbb{R}^n - –∑–∞–º–∫–Ω—É—Ç–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ, —Ç–æ–≥–¥–∞ –ø—Ä–æ–µ–∫—Ü–∏—è –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–æ S —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –¥–ª—è –ª—é–±–æ–π —Ç–æ—á–∫–∏.\n\n\n\n–î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —É—Å–ª–æ–≤–∏–µ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–æ–µ–∫—Ü–∏–∏ —Ç–æ—á–∫–∏ –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–æ.\n\n\n\n\n\n\n–ï—Å–ª–∏ S \\subseteq \\mathbb{R}^n - –∑–∞–º–∫–Ω—É—Ç–æ–µ –≤—ã–ø—É–∫–ª–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ, —Ç–æ–≥–¥–∞ –ø—Ä–æ–µ–∫—Ü–∏—è –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–æ S –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏.\n\n\n\n–ú–µ—Ç–æ–¥ –ø—Ä–æ–µ–∫—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞.\n\n\n\n\n\n\n–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∑–∞–¥–∞—á–∞ f(x) \\rightarrow \\min\\limits_{x \\in S}, –≥–¥–µ S \\subseteq \\mathbb{R}^n. –ú–µ—Ç–æ–¥ –ø—Ä–æ–µ–∫—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å –ø—Ä–æ–µ–∫—Ü–∏–µ–π –Ω–∞ –±—é–¥–∂–µ—Ç–Ω–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ S: \nx_{k+1} = \\text{proj}_S(x_{k} - \\alpha_k \\nabla f(x_{k})),\n –≥–¥–µ \\alpha_k ‚Äî learning rate.\n\n\n\n–ö—Ä–∏—Ç–µ—Ä–∏–π –ø—Ä–æ–µ–∫—Ü–∏–∏ —Ç–æ—á–∫–∏ –Ω–∞ –≤—ã–ø—É–∫–ª–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ (–ù–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –ë—É—Ä–±–∞–∫–∏-–ß–µ–π–Ω–∏-–ì–æ–ª—å–¥—à—Ç–µ–π–Ω–∞).\n\n\n\n\n\n\n–ü—Ä–æ–µ–∫—Ü–∏—è \\text{proj}_S(x) —Ç–æ—á–∫–∏ x –Ω–∞ –≤—ã–ø—É–∫–ª–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ S —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è–µ—Ç: \n\\langle x - \\text{proj}_S(x), y - \\text{proj}_S(x) \\rangle \\leqslant0 \\quad \\forall y \\in S.\n\n\n\n\n–ü—Ä–æ–µ–∫—Ü–∏—è –∫–∞–∫ –Ω–µ—Ä–∞—Å—Ç—è–≥–∏–≤–∞—é—â–∏–π –æ–ø–µ—Ä–∞—Ç–æ—Ä.\n\n\n\n\n\n\n–ü—Ä–æ–µ–∫—Ü–∏—è –Ω–∞ –≤—ã–ø—É–∫–ª–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ S —è–≤–ª—è–µ—Ç—Å—è –Ω–µ—Ä–∞—Å—Ç—è–≥–∏–≤–∞—é—â–∏–º –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º: \n\\|\\text{proj}_S(x) - \\text{proj}_S(y)\\| \\leqslant \\|x - y\\| \\quad \\forall x, y.\n\n\n\n\n–ú–µ—Ç–æ–¥ –§—Ä–∞–Ω–∫-–í—É–ª—å—Ñ–∞.\n\n\n\n\n\n\n–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –∑–∞–¥–∞—á—É f(x) \\rightarrow \\min\\limits_{x \\in S}. –ú–µ—Ç–æ–¥ –§—Ä–∞–Ω–∫-–í—É–ª—å—Ñ–∞ –∏–º–µ–µ—Ç –≤–∏–¥: \n\\begin{aligned}\ny_k &= \\text{arg}\\min_{x \\in S} f^I_{x_k}(x) = \\text{arg}\\min_{x \\in S} \\langle\\nabla f(x_k), x \\rangle \\\\\nx_{k+1} &= \\gamma_k x_k + (1-\\gamma_k)y_k\n\\end{aligned}\n –≥–¥–µ \\gamma_k - –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä. \n\n\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞ –ø—Ä–æ–µ–∫—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –¥–ª—è –≥–ª–∞–¥–∫–∏—Ö –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞.\n\n\n\n\n\n\n–î–ª—è –≥–ª–∞–¥–∫–∏—Ö –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –º–µ—Ç–æ–¥ –ø—Ä–æ–µ–∫—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –∏–º–µ–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø–æ—Ä—è–¥–∫–∞ \\mathcal{O}\\left(\\frac{1}{k}\\right), –≥–¥–µ k ‚Äî —á–∏—Å–ª–æ –∏—Ç–µ—Ä–∞—Ü–∏–π. –¢–æ –µ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ç–∞–∫–∞—è –∂–µ, –∫–∞–∫ –∏ –¥–ª—è –±–µ–∑—É—Å–ª–æ–≤–Ω–æ–π –∑–∞–¥–∞—á–∏, –Ω–æ —Å—Ç–æ–∏–º–æ—Å—Ç—å –∏—Ç–µ—Ä–∞—Ü–∏–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã—à–µ –∏–∑-–∑–∞ –ø—Ä–æ–µ–∫—Ü–∏–∏.\n\n\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞ –ø—Ä–æ–µ–∫—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –¥–ª—è –≥–ª–∞–¥–∫–∏—Ö —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞.\n\n\n\n\n\n\n–î–ª—è –≥–ª–∞–¥–∫–∏—Ö —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –º–µ—Ç–æ–¥ –ø—Ä–æ–µ–∫—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –∏–º–µ–µ—Ç –ª–∏–Ω–µ–π–Ω—É—é —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø–æ—Ä—è–¥–∫–∞ \\mathcal{O}\\left(\\left(1 - \\frac{\\mu}{L}\\right)^k\\right), –≥–¥–µ k ‚Äî —á–∏—Å–ª–æ –∏—Ç–µ—Ä–∞—Ü–∏–π. –¢–æ –µ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ç–∞–∫–∞—è –∂–µ, –∫–∞–∫ –∏ –¥–ª—è –±–µ–∑—É—Å–ª–æ–≤–Ω–æ–π –∑–∞–¥–∞—á–∏, –Ω–æ —Å—Ç–æ–∏–º–æ—Å—Ç—å –∏—Ç–µ—Ä–∞—Ü–∏–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã—à–µ –∏–∑-–∑–∞ –ø—Ä–æ–µ–∫—Ü–∏–∏.\n\n\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞ –§—Ä–∞–Ω–∫-–í—É–ª—å—Ñ–∞ –¥–ª—è –≥–ª–∞–¥–∫–∏—Ö –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞.\n\n\n\n\n\n\n–ú–µ—Ç–æ–¥ –§—Ä–∞–Ω–∫-–í—É–ª—å—Ñ–∞ –¥–ª—è –≥–ª–∞–¥–∫–∏—Ö –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –∏–º–µ–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø–æ—Ä—è–¥–∫–∞ \\mathcal{O}\\left(\\frac{1}{k}\\right), –≥–¥–µ k ‚Äî —á–∏—Å–ª–æ –∏—Ç–µ—Ä–∞—Ü–∏–π.\n\n\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞ –§—Ä–∞–Ω–∫-–í—É–ª—å—Ñ–∞ –¥–ª—è –≥–ª–∞–¥–∫–∏—Ö —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞.\n\n\n\n\n\n\n–î–ª—è –≥–ª–∞–¥–∫–∏—Ö —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –º–µ—Ç–æ–¥ –§—Ä–∞–Ω–∫-–í—É–ª—å—Ñ–∞ –∏–º–µ–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø–æ—Ä—è–¥–∫–∞ \\mathcal{O}\\left(\\frac{1}{k}\\right), –≥–¥–µ k ‚Äî —á–∏—Å–ª–æ –∏—Ç–µ—Ä–∞—Ü–∏–π.\n\n\n\n–°—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç. –°—É–±–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª.\n\n\n\n\n\n\n–°—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ f –≤ —Ç–æ—á–∫–µ x ‚Äî —ç—Ç–æ –≤–µ–∫—Ç–æ—Ä g, —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—â–∏–π —É—Å–ª–æ–≤–∏—é: \nf(y) \\geqslant f(x) + g^T (y - x), \\quad \\forall y.\n –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –≤—Å–µ—Ö —Å—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –≤ —Ç–æ—á–∫–µ x –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è —Å—É–±–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª–æ–º –∏ –æ–±–æ–∑–Ω–∞—á–∞–µ—Ç—Å—è –∫–∞–∫ \\partial f(x).\n\n\n\n–°—É–±–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª —Ñ—É–Ω–∫—Ü–∏–∏ ReLU.\n\n\n\n\n\n–°—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –º–µ—Ç–æ–¥.\n\n\n\n\n\n\n–°—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ–≥–ª–∞–¥–∫–∏–º–∏. –ò—Ç–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ –º–µ—Ç–æ–¥–∞: \nx_{k+1} = x_k - \\alpha_k g_k,\n –≥–¥–µ g_k \\in \\partial f(x_k) ‚Äî —Å—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ f –≤ —Ç–æ—á–∫–µ x_k, \\alpha_k ‚Äî —à–∞–≥ –º–µ—Ç–æ–¥–∞ –Ω–∞ k-–π –∏—Ç–µ—Ä–∞—Ü–∏–∏.\n\n\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è –Ω–µ–≥–ª–∞–¥–∫–∏—Ö –≤—ã–ø—É–∫–ª—ã—Ö –õ–∏–ø—à–∏—Ü–µ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞.\n\n\n\n\n\n\nf^{\\text{best}}_k - f^* \\sim \\mathcal{O} \\left( \\frac{1}{\\sqrt{k}} \\right)\n\n\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è –Ω–µ–≥–ª–∞–¥–∫–∏—Ö —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã—Ö –õ–∏–ø—à–∏—Ü–µ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞. –°—Ç—Ä–∞—Ç–µ–≥–∏—è –≤—ã–±–æ—Ä–∞ —à–∞–≥–∞.\n\n\n\n\n\n\nf^{\\text{best}}_k - f^* \\sim \\mathcal{O} \\left( \\frac{1}{k} \\right), \\alpha_k = \\frac{2}{\\mu (k+1)}\n\n\n\n–ö–∞–∫–æ–º—É —É—Å–ª–æ–≤–∏—é –¥–æ–ª–∂–Ω–∞ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –≤—ã–±–æ—Ä–∞ —à–∞–≥–∞, —á—Ç–æ–±—ã —Å—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –º–µ—Ç–æ–¥ —Å—Ö–æ–¥–∏–ª—Å—è –¥–ª—è –≤—ã–ø—É–∫–ª—ã—Ö –õ–∏–ø—à–∏—Ü–µ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π?\n\n\n\n\n\n\n\\sum_{i=1}^k \\alpha_i = \\infty, \\quad \\sum_{i=1}^k \\alpha_i^2 &lt; \\infty\n\n\n\n–ù–∏–∂–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –Ω–µ–≥–ª–∞–¥–∫–æ–π –≤—ã–ø—É–∫–ª–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤ –ø–µ—Ä–≤–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞.\n\n\n\n\n\n\nf^{\\text{best}}_k - f^* \\sim \\mathcal{O} \\left( \\frac{1}{\\sqrt{k}} \\right)\n\n\n\n–ù–∏–∂–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –Ω–µ–≥–ª–∞–¥–∫–æ–π —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤ –ø–µ—Ä–≤–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞.\n\n\n\n\n\n\nf^{\\text{best}}_k - f^* \\sim \\mathcal{O} \\left( \\frac{1}{k} \\right)\n\n\n\n–ü—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä.\n\n\n\n\n\n\n\\text{prox}_{f} (x_k) = \\arg \\min\\limits_{x \\in \\mathbb{R}^n} \\left[f(x) + \\frac{1}{2} \\|x - x_k \\|_2^2\\right]\n\n\n\n–û–ø–µ—Ä–∞—Ç–æ—Ä –ø—Ä–æ–µ–∫—Ü–∏–∏ –∫–∞–∫ —á–∞—Å—Ç–Ω—ã–π —Å–ª—É—á–∞–π –ø—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞.\n\n\n\n\n\n\n\n\\text{proj}_S(y) : = \\arg \\min\\limits_{x \\in S} \\frac{1}{2}\\|x-y\\|_2^2\n –í–≤–µ–¥—ë–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é: \n\\mathbb{I}_S(x) =\n\\begin{cases}\n0, & \\text{if } x \\; \\in \\; S, \\\\\n\\infty, & \\text{else.}\n\\end{cases}\n –ü–µ—Ä–µ–ø–∏—à–µ–º –æ–ø–µ—Ä–∞—Ç–æ—Ä: \n\\text{proj}_S(y) = \\arg \\min\\limits_{x\\in S} \\left[ \\frac{1}{2}\\|x-y\\|_2^2 + \\mathbb{I}_S(x)\\right]\n –ò, –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, –≤—Å–ø–æ–º–Ω–∏–º \\text{prox}_{r} (x_k) = \\arg \\min\\limits_{x \\in \\mathbb{R}^n} \\left[\\frac{1}{2} \\|x - x_k \\|_2^2 + r(x)\\right].\n\n\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è –≥–ª–∞–¥–∫–∏—Ö –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π f –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞.\n\n\n\n\n\n\n–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∑–∞–¥–∞—á–∞: \\varphi(x) \\rightarrow \\min\\limits_{x \\in \\mathbb{R}^n}, –≥–¥–µ \\varphi(x)  = f(x) +r(x), f(x) - –≥–ª–∞–¥–∫–∞—è –≤—ã–ø—É–∫–ª–∞—è, r(x) - –Ω–µ–≥–ª–∞–¥–∫–∞—è –≤—ã–ø—É–∫–ª–∞—è, –ø—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω–æ –¥—Ä—É–∂–µ—Å—Ç–≤–µ–Ω–Ω–∞—è. \nx_{k+1} = \\text{prox}_{\\alpha r}\\left(x_k -\\alpha \\nabla f(x_k)\\right)\n –°—Ö–æ–¥–∏—Ç—Å—è –∑–∞ \\mathcal{O}\\left(\\frac{1}{k} \\right).\n\n\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è –≥–ª–∞–¥–∫–∏—Ö —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π f –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞.\n\n\n\n\n\n\n–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∑–∞–¥–∞—á–∞: \\varphi(x) \\rightarrow \\min\\limits_{x \\in \\mathbb{R}^n}, –≥–¥–µ \\varphi(x)  = f(x) +r(x), f(x) - –≥–ª–∞–¥–∫–∞—è –≤—ã–ø—É–∫–ª–∞—è, r(x) - –Ω–µ–≥–ª–∞–¥–∫–∞—è –≤—ã–ø—É–∫–ª–∞—è, –ø—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω–æ –¥—Ä—É–∂–µ—Å—Ç–≤–µ–Ω–Ω–∞—è. \n\\|x_k - x^*\\|^2 \\sim \\mathcal{O} \\left( \\left(1 - \\frac{\\mu}{L}\\right)^k \\right)\n –≥–¥–µ \\mu - –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞ —Å–∏–ª—å–Ω–æ–π –≤—ã–ø—É–∫–ª–æ—Å—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏ f, L - –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞ –≥–ª–∞–¥–∫–æ—Å—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏ f.\n\n\n\n–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è \\text{prox}_{\\lambda \\|x\\|_1}.\n\n\n\n\n\n\n\n\\begin{aligned}\nr(x) &= \\lambda \\|x\\|_1, \\quad \\lambda &gt; 0 \\\\\n[\\text{prox}_r(x)]_i &= [|x_i - \\lambda]_+ \\cdot \\text{sign}(x_i)\n\\end{aligned}\n\n\n\n\n–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è \\text{prox}_{\\frac{\\mu}{2} \\|x\\|_2^2}.\n\n\n\n\n\n\n\n\\begin{aligned}\nr(x) &= \\frac{\\mu}{2} \\|x\\|_2^2\\\\\n\\text{prox}_r(x) &= \\frac{x}{1 - \\mu}\n\\end{aligned}\n\n\n\n\n–ü—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä –∫–∞–∫ –Ω–µ—Ä–∞—Å—Ç—è–≥–∏–≤–∞—é—â–∏–π –æ–ø–µ—Ä–∞—Ç–æ—Ä.\n\n\n\n\n\n\n–ü—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä \\text{prox}_r(x) —Å—Ç—Ä–æ–≥–æ –Ω–µ—Ä–∞—Å—Ç—è–≥–∏–≤–∞—é—â–∏–π (FNE - firmly non-expansive): \n\\left\\|\\text{prox}_r(x)-\\text{prox}_r(y)\\right\\|_2^2 \\leqslant \\left\\langle\\text{prox}_r(x)-\\text{prox}_r(y), x-y\\right\\rangle\n –∏ –Ω–µ—Ä–∞—Å—Ç—è–≥–∏–≤–∞—é—â–∏–π: \n\\left\\|\\text{prox}_r(x)-\\text{prox}_r(y)\\right\\|_2 \\leqslant \\|x-y\\|_2\n\n\n\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è –≥–ª–∞–¥–∫–∏—Ö –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π f –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞.\n\n\n\n\n\n\n\\varphi(x) = f(x) + r(x), f(x) - –≤—ã–ø—É–∫–ª–∞—è, L-–≥–ª–∞–¥–∫–∞—è, r(x) - –≤—ã–ø—É–∫–ª–∞—è –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω \\text{prox}_{\\alpha r}(x_k) \\Rightarrow \\varphi(x_k) - \\varphi^* \\leqslant \\frac{L\\|x_0 - x^*\\|^2}{2k^2} \\sim \\mathcal{O}\\left(\\frac{1}{k^2}\\right)\n\n\n\n–ú–µ—Ç–æ–¥ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞.\n\n\n\n\n\n\n–†–µ—à–∞–µ–º–∞—è –∑–∞–¥–∞—á–∞: f(x) \\rightarrow \\min\\limits_{x \\in \\mathbb{R}^p}, –≥–¥–µ f(x) =\\frac{1}{n} \\sum\\limits_{i=1}^n f_i(x) \n\\text{SGD:}\\quad x_{k+1} = x_k - \\alpha_k \\nabla f_{i_k}(x),\n –≥–¥e i_k - —Å–ª—É—á–∞–π–Ω–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å. –ï—Å–ª–∏ \\mathbb{P}(i_k = i) = \\frac{1}{n}, —Ç–æ \\mathbb{E}[\\nabla f_{i_k}(x)] = \\nabla f(x)\n\n\n\n–ò–¥–µ—è –º–∏–Ω–∏-–±–∞—Ç—á–∞ –¥–ª—è –º–µ—Ç–æ–¥–∞ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞. –≠–ø–æ—Ö–∞.\n\n\n\n\n\n\n–†–∞–∑–¥–µ–ª–∏–º –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä–∞ N –Ω–∞ k –º–∏–Ω–∏-–±–∞—Ç—á–µ–π (–≤—ã–±–æ—Ä–æ–∫) —Ä–∞–∑–º–µ—Ä–∞ B_k, –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –ø–æ—Å—á–∏—Ç–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç –º–∏–Ω–∏-–±–∞—Ç—á–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞. –ó–∞ \\frac{N}{k} –∏—Ç–µ—Ä–∞—Ü–∏–π –ø—Ä–æ–π–¥—ë–º—Å—è –ø–æ –≤—Å–µ–π –≤—ã–±–æ—Ä–∫–µ. –≠–ø–æ—Ö–∞ - –Ω–∞–±–æ—Ä k –∏—Ç–µ—Ä–∞—Ü–∏–π —Å –±–∞—Ç—á–µ–º —Ä–∞–∑–º–µ—Ä–∞ B_k = \\frac{N}{k}. \nx_{k + 1} = x_k -  \\frac{1}{|B_k|} \\sum_{i \\in B_k} - \\text{—à–∞–≥ –º–∏–Ω–∏-–±–∞—Ç—á–∞}.\n C —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞ –º–∏–Ω–∏-–±–∞—Ç—á–∞ –≤—Ä–µ–º—è –Ω–∞ —ç–ø–æ—Ö—É —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –Ω–∞–º —Ö–≤–∞—Ç–∞–µ—Ç –ø–∞–º—è—Ç–∏ (–≤ —Å–ª—É—á–∞–µ –Ω–∞–ª–∏—á–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞).\n\n\n\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –¥–ª—è –≥–ª–∞–¥–∫–∏—Ö –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞.\n\n\n\n\n\n\nf - –≥–ª–∞–¥–∫–∞—è –∏ –≤—ã–ø—É–∫–ª–∞—è \\Rightarrow \\mathcal{O}\\left(\\frac{1}{\\varepsilon^2}\\right), \\mathcal{O}\\left(\\frac{1}{\\sqrt{k}}\\right)\n\n\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –¥–ª—è –≥–ª–∞–¥–∫–∏—Ö PL-—Ñ—É–Ω–∫—Ü–∏–π –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O} –æ—Ç —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –º–µ—Ç–æ–¥–∞.\n\n\n\n\n\n\nf \\in PL \\Rightarrow \\mathcal{O}\\left(\\frac{1}{k}\\right), \\mathcal{O}\\left(\\frac{1}{\\varepsilon}\\right)\n\n\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Ä–∞–±–æ—Ç—ã —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ —Å –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º —à–∞–≥–æ–º –¥–ª—è –≥–ª–∞–¥–∫–∏—Ö PL-—Ñ—É–Ω–∫—Ü–∏–π.\n\n\n\n\n\n\n–ü—É—Å—Ç—å \\min_{x \\in \\mathbb{R}^p} f(x) = \\min_{x \\in \\mathbb{R}^p}\\frac{1}{n} \\sum_{i=1}^n f_i(x) –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ —Å –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º —à–∞–≥–æ–º \\alpha \nx_{k+1} = x_k - \\alpha  \\nabla f_{i_k}(x_k)\n –∏–º–µ–µ–º —Å–ª–µ–¥—É—é—â—É—é –æ—Ü–µ–Ω–∫—É \\mathbb{E}[f(x_{k+1}) - f^*] \\leq (1 - 2\\alpha \\mu)^k[f(x_{0}) - f^*]  + \\frac{L \\sigma^2 \\alpha }{ 4 \\mu}. –•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ - –ª–∏–Ω–µ–π–Ω—ã–π –¥–æ –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ —à–∞—Ä–∞ –Ω–µ—Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏, –≤ –∫–æ—Ç–æ—Ä–æ–º –±—É–¥—É—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å –æ—Å—Ü–∏–ª–ª—è—Ü–∏–∏ –∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –Ω–µ –±—É–¥–µ—Ç.\n\n\n\n–û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –º–µ—Ç–æ–¥–æ–≤ —É–º–µ–Ω—å—à–µ–Ω–∏—è –¥–∏—Å–ø–µ—Ä—Å–∏–∏.\n\n\n\n\n\n\n–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º —Å–ª—É—á–∞–Ω—É—é –≤–µ–ª–∏—á–∏–Ω—É X. –•–æ—Ç–∏–º —É–º–µ–Ω—å—à–∏—Ç—å —É –Ω–µ—ë –¥–∏—Å–ø–µ—Ä—Å–∏—é. –ü—É—Å—Ç—å Y - —Ç–æ–∂–µ —Å–ª—É—á–∞–π–Ω–∞—è –≤–ª–∏—á–∏–Ω–∞ —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º –º–∞—Ç. –æ–∂–∏–¥–∞–Ω–∏–µ–º. –†–∞—Å—Å–º–æ—Ç—Ä–∏–º –Ω–æ–≤—É—é —Å.–≤ Z_{\\alpha} = \\alpha (X ‚àí Y) + \\mathbb{E}[Y]\n\n\\mathbb{E}[Z_\\alpha] = \\alpha \\mathbb{E}[X] + (1-\\alpha)\\mathbb{E}[Y]\n\\text{var}(Z_\\alpha) = \\alpha^2 \\left(\\text{var}(X) + \\text{var}(Y) - 2\\text{cov}(X, Y)\\right)\n\n\\alpha = 1: –Ω–µ—Ç —Å–º–µ—â–µ–Ω–∏—è –º–∞—Ç.–æ–∂–∏–¥–∞–Ω–∏—è\n\\alpha &lt; 1: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ (–Ω–æ —É–º–µ–Ω—å—à–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏).\n\n–ü–æ–ª–µ–∑–Ω–æ, –µ—Å–ª–∏ Y –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å X.\n\n\n\n\n–ú–µ—Ç–æ–¥ SVRG.\n\n\n\n\n\n\n\n–ü—É—Å—Ç—å X = \\nabla f_{i_k}(x_{m-1}) - —Å—Ç–æ—Ö. –≥—Ä–∞–¥–∏–µ–Ω—Ç, –∞ Y = \\nabla f_{i_k}(\\tilde{x}), —Å \\alpha = 1 –∏ \\tilde{x} —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ –ø–∞–º—è—Ç–∏.\n\\mathbb{E}[Y] = \\frac{1}{n} \\sum_{i=1}^n \\nabla f_i(\\tilde{x}) –ø–æ–ª–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç –≤ \\tilde{x};\nX - Y = \\nabla f_{i_k}(x^{(m-1)}) - \\nabla f_{i_k}(\\tilde{x})\n\n–ü–æ–ª—É—á–∞–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º:\n\nInitialize: \\tilde{x} \\in \\mathbb{R}^d\nFor i_{epoch} = 1 to # of epochs\nCompute all gradients \\nabla f_i(\\tilde{x}); store \\nabla f(\\tilde{x}) = \\frac{1}{n} \\sum_{i=1}^n \\nabla f_i(\\tilde{x})\nInitialize x_0 = \\tilde{x}\nFor t = 1 to length of epochs (m)\n\nPick i_t \\in \\{1, \\dots, n\\} uniformly at random\nx_t = x_{t-1} - \\alpha \\left[\\nabla f_{i_t}(x_{t-1}) - \\nabla f_{i_t}(\\tilde{x}) + \\nabla f(\\tilde{x})\\right]\n\nUpdate \\tilde{x} = x_m\n\n\n\n\n–ú–µ—Ç–æ–¥ SAG.\n\n\n\n\n\n\n–ó–∞–¥–∞—á–∞: f(x) = \\frac{1}{n} \\sum_{i=1}^n f_{i}(x)\n\nInitialize x^{(0)} and g_i^{(0)} = \\nabla f_i(x^{(0)})\nAt steps k = 1, 2, 3, \\dots pick random i_k \\in \\{1, \\dots n\\}\ng_{i_k}^{(k)} = \\nabla f_{i_k}(x^{(k-1)})\nSet all other g_i^{(k)} = g_i^{(k-1)}, i \\neq i_k\nUpdate: g^{(k)} = g^{(k-1)} + \\frac{1}{n} (g_{i_k}^{(k)} - g_{i_k}^{(k-1)}) = \\frac{1}{n}\\sum\\limits_{i=1}^n g_i^{(k)}\nx^{(k)} = x^{(k-1)} - \\alpha ^k g^{(k)}\n\nPS: —Å—Ç–æ–∏–º–æ—Å—Ç—å –∏—Ç–µ—Ä–∞—Ü–∏–∏ –∫–∞–∫ –≤ –æ–±—ã—á–Ω–æ–º SGD, –Ω–æ –ø–ª–∞—Ç–∏–º –∑–∞ —ç—Ç–æ –ø–∞–º—è—Ç—å—é.\n–°—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ –≤—ã–ø—É–∫–ª–æ–º —Å–ª—É—á–∞–µ: f(x_{\\text{mean}}^{(k)}) - f^* \\leq \\frac{48n|f(x^{(0)}) - f^*| + 128L\\|x^{(0)} - x^*\\|^2}{k} = O(\\frac{1}{k})\n–°–∫–æ—Ä–æ—Å—Ç—å –≤ —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–æ–º —Å–ª—É—á–∞–µ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º \\mu:\n\\mathbb{E}\\left[f(x^{(k)}) - f^*\\right] \\leq \\left(1 - \\min\\left(\\frac{\\mu}{16L}, \\frac{1}{8n}\\right)\\right)^k (\\frac{3}{2}(f(x^{(0)}) - f^*) + \\frac{4L}{n} \\|x^{(0)} - x^*\\|^2) = O(\\gamma ^k)\n\n\n\n–ú–µ—Ç–æ–¥ Adagrad.\n\n\n\n\n\n\n–ó–∞–¥–∞—á–∞: f(x) = \\frac{1}{n} \\sum_{i=1}^n f_{i}(x). –ü—É—Å—Ç—å g^{(k)} = \\nabla f_{i_k}(x^{(k-1)}), –∏ –æ–±–Ω–æ–≤–ª—è–µ–º for j = 1, \\dots, p: \n\\begin{aligned}\nv^{(k)}_j &= v^{k-1}_j + \\left(g_j^{(k)}\\right)^2 \\\\\nx_j^{(k)} &= x_j^{(k-1)} - \\alpha \\frac{g_j^{(k)}}{\\sqrt{v^{(k)}_j  + \\varepsilon}}\n\\end{aligned}\n\n–ü–æ—Å—Ç–æ—è–Ω–Ω–∞—è \\varepsilon –æ–±—ã—á–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è —Ä–∞–≤–Ω—ã–º 10^{-6} —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å, —á—Ç–æ –º—ã –Ω–µ –±—É–¥–µ–º –∏–º–µ—Ç—å –ø—Ä–æ–±–ª–µ–º—ã –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å –∏–ª–∏ —á—Ä–µ–∑–º–µ—Ä–Ω–æ –±–æ–ª—å—à–∏—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ —à–∞–≥–∞.\n\n\n\n–ú–µ—Ç–æ–¥ RMSProp.\n\n\n\n\n\n\n–ó–∞–¥–∞—á–∞: f(x) = \\frac{1}{n} \\sum\\limits_{i=1}^n f_{i}(x)\n–£—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ AdaGrad, —É—á–∏—Ç—ã–≤–∞—é—â–µ–µ –µ–≥–æ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é, –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ —Å–Ω–∏–∂–∞—é—â—É—é—Å—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–µ—Å–∞. –ü—É—Å—Ç—å g^{(k)} = \\nabla f_{i_k}(x^{(k-1)}) and update rule for j = 1, \\dots, p: \nv^{(k)}_j = \\gamma v^{(k-1)}_j + (1-\\gamma) (g_j^{(k)})^2\n \nx_j^{(k)} = x_j^{(k-1)} - \\alpha \\frac{g_j^{(k)}}{\\sqrt{v^{(k)}_j + \\varepsilon}}\n\n\n\n\n–ú–µ—Ç–æ–¥ Adadelta.\n\n\n\n\n\n\n–ó–∞–¥–∞—á–∞: f(x) = \\frac{1}{n} \\sum_{i=1}^n f_{i}(x)\n–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ RMSProp, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –Ω–∞ —Å–Ω–∏–∂–µ–Ω–∏–µ –µ–≥–æ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≥–ª–æ–±–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º–æ–π –≤—Ä—É—á–Ω—É—é. –í–º–µ—Å—Ç–æ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≤—Å–µ—Ö –ø—Ä–æ—à–ª—ã—Ö –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤, Adadelta –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –æ–∫–Ω–æ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –ø—Ä–æ—à–ª—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º $ w $. –ú–µ—Ö–∞–Ω–∏–∑–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è \\alpha: \n\\begin{aligned}\nv^{(k)}_j &= \\gamma v^{(k-1)}_j + (1-\\gamma) \\left(g_j^{(k)}\\right)^2 \\\\\n\\tilde{g}_j^{(k)} &= \\frac{\\sqrt{{\\Delta x_j^{(k-1)}} + \\varepsilon}}{\\sqrt{v^{(k)}_j+ \\varepsilon}} g_j^{(k)} \\\\\nx_j^{(k)} &= x_j^{(k-1)} - \\tilde{g}_j^{(k)} \\\\\n\\Delta x_j^{(k)} &= \\rho \\Delta x_j^{(k-1)} + (1-\\rho) \\left(\\tilde{g}_j^{(k)}\\right)^2\n\\end{aligned}\n\n\n\n\n–ú–µ—Ç–æ–¥ Adam.\n\n\n\n\n\n\n–ó–∞–¥–∞—á–∞: f(x) = \\frac{1}{n} \\sum\\limits_{i=1}^n f_{i}(x) \n\\begin{aligned}\nm_j^{(k)} &= \\beta_1 m_j^{(k-1)} + (1-\\beta_1) g_j^{(k)} \\\\\nv_j^{(k)} &= \\beta_2 v_j^{(k-1)} + (1-\\beta_2) (g_j^{(k)})^2 \\\\\n\\tilde{m}_j &= \\frac{m_j^{(k)}}{1-\\beta_1^k}, \\quad \\hat{v}_j = \\frac{v_j^{(k)} }{1-\\beta_2^k} \\\\\nx_j^{(k)} &= x_j^{(k-1)} - \\alpha \\frac{\\tilde{m}_j}{\\sqrt{\\hat{v}_j} + \\varepsilon}\n\\end{aligned}\n\n\n\n\n–ú–µ—Ç–æ–¥ AdamW.\n\n\n\n\n\n\n–†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å \\ell_2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π –≤ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ Adam. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è \\ell_2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –¥–æ–±–∞–≤–ª—è–µ—Ç \\lambda \\|x\\|^2 –∫ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–º—É —á–ª–µ–Ω—É \\lambda x. –í Adam —ç—Ç–æ—Ç —á–ª–µ–Ω –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –≤–º–µ—Å—Ç–µ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º —à–∞–≥–æ–º –æ–±—É—á–µ–Ω–∏—è \\left(\\sqrt{\\hat{v}_j} + \\epsilon\\right), —Å–≤—è–∑—ã–≤–∞—è –¥–µ–º–ø—Ñ–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏.\nAdamW –æ—Ç–¥–µ–ª—è–µ—Ç –¥–µ–º–ø—Ñ–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç —à–∞–≥–∞ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞. \nm_j^{(k)} = \\beta_1 m_j^{(k-1)} + (1-\\beta_1) g_j^{(k)}\n \nv_j^{(k)} = \\beta_2 v_j^{(k-1)} + (1-\\beta_2) (g_j^{(k)})^2\n \n\\hat{m}_j = \\frac{m_j^{(k)}}{1-\\beta_1^k}, \\quad \\hat{v}_j = \\frac{v_j^{(k)} }{1-\\beta_2^k}\n \nx_j^{(k)} = x_j^{(k-1)} - \\alpha \\left( \\frac{\\hat{m}_j}{\\sqrt{\\hat{v}_j} + \\epsilon} + \\lambda x_j^{(k-1)} \\right)\n\n\n\n\n–ú–µ—Ç–æ–¥ Shampoo.\n\n\n\n\n\n\n\n–í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç G_k.\n–û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ L_k = \\beta L_{k-1} + (1-\\beta) G_k G_k^T –∏ R_k = \\beta R_{k-1} + (1-\\beta) G_k^T G_k.\n–í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–µ–¥–æ–±—É—Å–ª–∞–≤–ª–∏–≤–∞—Ç–µ–ª–∏ P_L = L_k^{-1/4} –∏ P_R = R_k^{-1/4}. (–û–±—Ä–∞—Ç–Ω—ã–π –∫–æ—Ä–µ–Ω—å –º–∞—Ç—Ä–∏—Ü—ã)\n–û–±–Ω–æ–≤–ª—è–µ–º: W_{k+1} = W_k - \\alpha P_L G_k P_R.\n\n\n\n\n–ú–µ—Ç–æ–¥ Muon.\n\n\n\n\n\n\n\nW_{t+1} = W_t - \\eta UV^\\top,\n –≥–¥–µ G = U \\Sigma V^\\top - —Å–∏–Ω–≥—É–ª—è—Ä–Ω–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π —Ç–µ–∫—É—â–µ–º—É —Å–ª–æ—é.\n\n\n\n–ö–∞–∫ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è –º–µ—Ç–æ–¥—ã –≤ AlgoPerf Benchmark.\n\n\n\n\n\n\n–î–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∞ —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–∏–∫–ª–∞–¥–Ω—ã—Ö –∑–∞–¥–∞—á –∏ –º–æ–¥–µ–ª–µ–π. –î–ª—è –∫–∞–∂–¥–æ–π –∏–∑ –Ω–∏—Ö –∑–∞–¥–∞—ë—Ç—Å—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π —É—Ä–æ–≤–µ–Ω—å —Ü–µ–ª–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –∑–∞ –æ—Ç–≤–µ–¥—ë–Ω–Ω–æ–µ –≤—Ä–µ–º—è.\n–î–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –ª—É—á—à–∏–π –º–µ—Ç–æ–¥ –∏ –≤—Ä–µ–º—è –µ–≥–æ —Ä–∞–±–æ—Ç—ã –æ–±–æ–∑–Ω–∞—á–∞–µ—Ç—Å—è –∑–∞ t_{best}. –î–ª—è –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ —Å—Ç—Ä–æ—è—Ç –≥—Ä–∞—Ñ–∏–∫ –ø—Ä–æ—Ü–µ–Ω—Ç–∞ —Ä–µ—à–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–µ–Ω—Ç–∞ \\tau, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–æ —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Ç—Ä–∞—á–µ–Ω–æ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ª—É—á—à–∏–º –º–µ—Ç–æ–¥–æ–º –Ω–∞ –¥–∞–Ω–Ω–æ–π –∑–∞–¥–∞—á–µ t = \\tau t_{best}.\n\n\n\n\n–ò–¥–µ—è –ø—Ä–æ–µ–∫—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –Ω–∞ –ø—Ä—è–º—É—é, –ø–ª–æ—Å–∫–æ—Å—Ç—å.\n\n\n\n\n\n\n–ü—É—Å—Ç—å L(w) - —Ñ—É–Ω–∫—Ü–∏—è –æ—Ç w \\in \\mathbb{R}^n. –í–≤–µ–¥–µ–º –ø—Ä–æ–µ—Ü–∏—é –Ω–∞ –ª–∏–Ω–∏—é: \nL(\\alpha) = L(w_0 + \\alpha w_1)\n –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä–≥–æ w_1 \\in \\mathbb{R}^n. –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –º–æ–∂–Ω–æ –≤–≤–µ—Å—Ç–∏ –ø—Ä–æ–µ–∫—Ü–∏—é –Ω–∞ –ø–ª–æ—Å–∫–æ—Å—Ç—å \nL(\\alpha, \\beta) = L(w_0 + \\alpha w_1 + \\beta w_2)\n –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö w_1, w_2 \\in \\mathbb{R}^n.\n\n–î–≤–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–∞ –±–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å –≤—ã—Å–æ–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã –¥—Ä—É–≥ –¥—Ä—É–≥—É.\n–ï—Å–ª–∏ –ø—Ä–æ–µ–∫—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ–≤—ã–ø—É–∫–ª–∞, —Ç–æ –∏ –∏—Å—Ö–æ–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –Ω–µ–≤—ã–ø—É–∫–ª–∞. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –º–æ–∂–Ω–æ –∑–∞–≥–ª—è–Ω—É—Ç—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Ç –º–Ω–æ–≥–∏—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö.\n\n\n\n\nGrokking.\n\n\n\n\n\n\nGrokking –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π ‚Äî —ç—Ç–æ —è–≤–ª–µ–Ω–∏–µ, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å–Ω–∞—á–∞–ª–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–ª–æ—Ö—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –Ω–∞–±–æ—Ä–µ. –ó–∞—Ç–µ–º, –ø–æ—Å–ª–µ –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –º–æ–¥–µ–ª—å –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ –Ω–∞—á–∏–Ω–∞–µ—Ç –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–æ –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å –≤ –∫–æ–Ω–µ—á–Ω–æ–º –∏—Ç–æ–≥–µ –Ω–∞—Ö–æ–¥–∏—Ç –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏–µ –∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –µ–π –ª—É—á—à–µ –æ–±–æ–±—â–∞—Ç—å –Ω–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n\n\n\n–¢–æ—á–Ω–æ—Å—Ç—å —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω–æ–≥–æ —Å–ª–æ–∂–µ–Ω–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é.\n\n\n\n\n\nDouble Descent.\n\n\n\n\n\n\nDouble descent ‚Äî —ç—Ç–æ —è–≤–ª–µ–Ω–∏–µ, –Ω–∞–±–ª—é–¥–∞–µ–º–æ–µ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, –∫–æ–≥–¥–∞ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å–Ω–∏–∂–µ–Ω–∏—é –æ—à–∏–±–∫–∏ –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –∏ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–∞—Ö (–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ bias-variance tradeoff), –∑–∞—Ç–µ–º –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Ä–µ–∑–∫–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ (–ø–µ—Ä–≤–∞—è —Ç–æ—á–∫–∞ –ø–µ—Ä–µ–≥–∏–±–∞, —Å–≤—è–∑–∞–Ω–Ω–∞—è —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º), –ø–æ—Å–ª–µ —á–µ–≥–æ, —Å –¥–∞–ª—å–Ω–µ–π—à–∏–º —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ—à–∏–±–∫–∞ —Å–Ω–æ–≤–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç —É–º–µ–Ω—å—à–∞—Ç—å—Å—è, —Ñ–æ—Ä–º–∏—Ä—É—è –≤—Ç–æ—Ä—É—é ‚Äú–≤–æ–ª–Ω—É‚Äù —É–ª—É—á—à–µ–Ω–∏—è. –≠—Ç–æ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–π U-–æ–±—Ä–∞–∑–Ω–æ–π –∫—Ä–∏–≤–æ–π, –∏ –µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤–∞–∂–Ω–æ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –≤—ã–±–æ—Ä–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏.\n\n\n\n–ò–ª–ª—é—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞.\n\n\n\n\n\n–í–∑—Ä—ã–≤/–ó–∞—Ç—É—Ö–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π.\n\n\n\n\n\n\n–ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π —á–∞—Å—Ç–æ –≤–æ–∑–Ω–∏–∫–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã –≤–∑—Ä—ã–≤–∞ –∏ –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –º–µ–¥–ª–µ–Ω–Ω–æ–π –∏–ª–∏ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –≠—Ç–∏ —è–≤–ª–µ–Ω–∏—è –º–æ–∂–Ω–æ –æ–ø–∏—Å–∞—Ç—å —Å –ø–æ–º–æ—â—å—é –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –æ—à–∏–±–∫–∏ L –ø–æ –≤–µ—Å–∞–º —Å–µ—Ç–∏ W. –ü—É—Å—Ç—å L - —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å, –∞ \\frac{\\partial L}{\\partial W} - –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤. –ö–æ–≥–¥–∞ —Å–µ—Ç—å –∏–º–µ–µ—Ç –º–Ω–æ–≥–æ —Å–ª–æ–µ–≤, –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –∫–∞–∫ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü –Ø–∫–æ–±–∏ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è: \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial z^{(n)}} \\cdot \\frac{\\partial z^{(n)}}{\\partial z^{(n-1)}} \\cdots \\frac{\\partial z^{(2)}}{\\partial z^{(1)}} \\cdot \\frac{\\partial z^{(1)}}{\\partial W}, –≥–¥–µ z^{(i)} - –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ i-–≥–æ —Å–ª–æ—è. –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö \\frac{\\partial z^{(i+1)}}{\\partial z^{(i)}} –≤ —Å—Ä–µ–¥–Ω–µ–º –±–æ–ª—å—à–µ –µ–¥–∏–Ω–∏—Ü—ã, –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–∞—á–∏–Ω–∞—é—Ç —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å—Å—è –ø—Ä–∏ –æ–±—Ä–∞—Ç–Ω–æ–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–∏, –≤—ã–∑—ã–≤–∞—è –≤–∑—Ä—ã–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤. –ù–∞–ø—Ä–æ—Ç–∏–≤, –µ—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –º–µ–Ω—å—à–µ –µ–¥–∏–Ω–∏—Ü—ã, –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —É–º–µ–Ω—å—à–∞—é—Ç—Å—è, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∏—Ö –∑–∞—Ç—É—Ö–∞–Ω–∏—é.\n\n\n\n–ò–¥–µ—è gradient checkpointing.\n\n\n\n\n\n\nGradient checkpointing ‚Äî —ç—Ç–æ —Ç–µ—Ö–Ω–∏–∫–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∑–∏—Ç—å –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –∑–∞ —Å—á–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ—Å—á–µ—Ç–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –∞–∫—Ç–∏–≤–∞—Ü–∏–π –≤–æ –≤—Ä–µ–º—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–∫–∏. –í —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è –∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ –ø–∞–º—è—Ç–∏, —á—Ç–æ —Ç—Ä–µ–±—É–µ—Ç O(N) –ø–∞–º—è—Ç–∏, –≥–¥–µ N ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ –≤ —Å–µ—Ç–∏.\n–ü—Ä–∏ gradient checkpointing –≤–º–µ—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–µ–≤, –º—ã —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ö —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Å–ª–æ–µ–≤, –Ω–∞–∑—ã–≤–∞–µ–º—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏. –ê–∫—Ç–∏–≤–∞—Ü–∏–∏ –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–ª–æ–µ–≤ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è –Ω–∞ —ç—Ç–∞–ø–µ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è, —á—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç –æ–±—â–µ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏. –ï—Å–ª–∏ –º—ã —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –∫–∞–∂–¥—ã–µ k —Å–ª–æ–µ–≤, —Ç–æ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è –¥–æ O(\\frac{N}{k}). –û–¥–Ω–∞–∫–æ, —ç—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º –∑–∞—Ç—Ä–∞—Ç–∞–º, —Ç–∞–∫ –∫–∞–∫ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª–æ–µ–≤ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑.\n\n\n\n\n–ò–¥–µ—è –∞–∫–∫—É–º—É–ª—è—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.\n\n\n\n\n\n\n–ê–∫–∫—É–º—É–ª—è—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ–≥–¥–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω –æ–±—ä–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ–π –≤–∏–¥–µ–æ–ø–∞–º—è—Ç–∏. –í–º–µ—Å—Ç–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞ –¥–∞–Ω–Ω—ã—Ö, –∫–∞–∫ —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–º —Å–ø—É—Å–∫–µ (SGD), –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–∞–∫–∞–ø–ª–∏–≤–∞—é—Ç—Å—è –≤ —Ç–µ—á–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–∞—Ç—á–µ–π. –ó–∞—Ç–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–∞—Ç—á–µ–π, —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã—Ö –æ–¥–Ω–æ–º—É –±–æ–ª—å—à–æ–º—É –±–∞—Ç—á—É. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à–∏–π –æ–±—ä–µ–º –ø–∞–º—è—Ç–∏, —Ç–∞–∫ –∫–∞–∫ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ö—Ä–∞–Ω–∏—Ç—å –±–æ–ª—å—à–∏–µ –±–∞—Ç—á–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –≤–∏–¥–µ–æ–ø–∞–º—è—Ç–∏, –ø—Ä–∏ —ç—Ç–æ–º –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è —Å—Ö–æ–¥–Ω—ã–π —Å –±–æ–ª—å—à–∏–º –±–∞—Ç—á–µ–º —ç—Ñ—Ñ–µ–∫—Ç –Ω–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤, —á—Ç–æ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–º—É –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–∏.\n\n\n\n–ó–∞—á–µ–º —É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å –±–∞—Ç—á –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. Warmup.\n\n\n\n\n\n\n–ï—Å–ª–∏ —É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞, —Ç–æ, –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞, –≤—Ä–µ–º—è –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç–ø–æ—Ö–∏ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–µ –ø—Ä–∞–≤–∏–ª–æ: –∫–æ–≥–¥–∞ —Ä–∞–∑–º–µ—Ä –º–∏–Ω–∏–±–∞—Ç—á–∞ —É–≤–µ–ª–∏—á–∏–ª—Å—è –≤ k —Ä–∞–∑, learning rate —Ç–∞–∫–∂–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–≤–µ–ª–∏—á–∏—Ç—å –≤ k —Ä–∞–∑ (linear scaling rule). –î–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —à–∫–∞–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ learning rate –≤ \\sqrt{k} —Ä–∞–∑ (square root scaling rule).\nWarmup ‚Äî —ç—Ç–æ —Ç–µ—Ö–Ω–∏–∫–∞, –ø—Ä–∏–º–µ–Ω—è–µ–º–∞—è –∫ –ø—Ä–æ—Ü–µ—Å—Å—É –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π, —á—Ç–æ–±—ã —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ —É–ª—É—á—à–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö —ç—Ç–∞–ø–∞—Ö. –í –ø—Ä–æ—Ü–µ—Å—Å–µ Warmup –Ω–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –æ—Ç –Ω–∏–∑–∫–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –¥–æ —Ü–µ–ª–µ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Ç–µ—á–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–µ—Ä–≤—ã—Ö —ç–ø–æ—Ö –∏–ª–∏ —à–∞–≥–æ–≤. –≠—Ç–∞ —Ç–µ—Ö–Ω–∏–∫–∞ –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å—é –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∏ —Ä–µ–∑–∫–∏–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –≤ —Å–∞–º–æ–º –Ω–∞—á–∞–ª–µ –æ–±—É—á–µ–Ω–∏—è.\n\n\n\n–ü—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞ –≤—Ä–µ–º—è –Ω–∞ —ç–ø–æ—Ö—É —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –Ω–∞–º —Ö–≤–∞—Ç–∞–µ—Ç –ø–∞–º—è—Ç–∏ (–≤ —Å–ª—É—á–∞–µ –Ω–∞–ª–∏—á–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞).\n\n\n\n\n\n–ò–¥–µ—è cooldown —Ñ–∞–∑—ã –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è learning rate. –í —á—ë–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å cosine scheduler?\n\n\n\n\n\n\n–ü–æ–¥—Ö–æ–¥ –∫ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é learning rate, –∑–∞–∫–ª—é—á–∞—é—â–∏–π—Å—è –≤ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–º —É–º–µ–Ω—å—à–µ–Ω–∏–∏ learning rate –≤ –∫–æ–Ω—Ü–µ –æ–±—É—á–µ–Ω–∏—è. –ü–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –∑–Ω–∞–Ω–∏—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è, –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç cosine scheduler.\n\n\n\n\n–î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞.\n\n\n\n\n\n\n\n\\frac{dx}{dt} = -\\nabla f(x)\n\n\n\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –¥–ª—è –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O}\\left( t \\right).\n\n\n\n\n\n\n\nf(x(t)) - f^* \\leqslant \\frac{1}{2t}\\|x(0) - x^*\\|^2 \\quad \\Rightarrow \\quad \\mathcal{O}\\left(\\frac{1}{t}\\right).\n\n\n\n\n–•–∞—Ä–∞–∫—Ç–µ—Ä —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –¥–ª—è PL-—Ñ—É–Ω–∫—Ü–∏–π –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö \\mathcal{O}\\left( t \\right).\n\n\n\n\n\n\n\nf(x(t)) - f^* \\leqslant \\exp\\{-2\\mu t\\}(f(x(0)) - f^*) \\quad \\Rightarrow \\quad \\mathcal{O}\\left(\\exp\\{-2\\mu t\\}\\right).\n\n\n\n\n–î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ –ù–µ—Å—Ç–µ—Ä–æ–≤—Å–∫–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞.\n\n\n\n\n\n\n\n\\ddot{X}(t) + \\frac{3}{t}\\dot{X}(t) + \\nabla f(X(t)) = 0.\n\n\n\n\n–ú–µ—Ç–æ–¥ –¥–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–¥—ä–µ–º–∞.\n\n\n\n\n\n\n–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∑–∞–¥–∞—á–∞: \nf(x) \\rightarrow \\min\\limits_{Ax = b}.\n –î–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞: \n-f^*(-A^Tu) - b^Tu \\rightarrow \\max\\limits_{u},\n –≥–¥–µ f^*(y) = \\max\\limits_{x}\\left[y^T x - f(x)\\right] - —Å–æ–ø—Ä—è–∂–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è. –û–ø—Ä–µ–¥–µ–ª–∏–º g(u) = -f^*(-A^Tu) - b^Tu, —Ç–æ–≥–¥–∞ \\partial g(u) = A\\partial f^*(-A^Tu) - b. –ü–µ—Ä–µ–ø–∏—à–µ–º —ç—Ç–æ –≤ –≤–∏–¥–µ \\partial g(u) = Ax - b, –≥–¥–µ x \\in \\arg \\min\\limits_{z}\\left[f(z) + u^TAz\\right]. –¢–æ–≥–¥–∞ –æ–ø—Ä–µ–¥–µ–ª–∏–º –º–µ—Ç–æ–¥ –¥–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–¥—ä–µ–º–∞: \n\\begin{aligned}\nx_k &\\in \\arg \\min\\limits_{x}\\left[f(x) + (u_{k-1})^TAx\\right] \\\\\nu_k &= u_{k-1} + \\alpha_k(Ax_k - b).\n\\end{aligned}\n\n\n\n\n–°–≤—è–∑—å –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã —Å–∏–ª—å–Ω–æ–π –≤—ã–ø—É–∫–ª–æ—Å—Ç–∏ f –∏ –≥–ª–∞–¥–∫–æ—Å—Ç–∏ f^*.\n\n\n\n\n\n\n–ü—É—Å—Ç—å f - –∑–∞–º–∫–Ω—É—Ç–∞—è –∏ –≤—ã–ø—É–∫–ª–∞—è. –¢–æ–≥–¥–∞ f - —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–∞—è —Å –∫–æ–Ω—Å—Ç–∞–Ω—Ç–æ–π –≤—ã–ø—É–∫–ª–æ—Å—Ç–∏ \\mu \\Leftrightarrow \\nabla f^* - –ª–∏–ø—à–∏—Ü–µ–≤ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º \\frac{1}{\\mu}.\n\n\n\n–ò–¥–µ—è dual decomposition.\n\n\n\n\n\n\n–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –∑–∞–¥–∞—á—É \\sum\\limits_{i=1}^Bf_i(x_i) \\rightarrow \\min\\limits_{Ax = b}. –ó–¥–µ—Å—å x = (x_1, \\ldots, x_B)^T \\in \\mathbb{R}^n —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ B –±–ª–æ–∫–æ–≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö, –∫–∞–∂–¥—ã–π x_i \\in \\mathbb{R}^{n_i}. –†–∞–∑–¥–µ–ª–∏–º –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –º–∞—Ç—Ä–∏—Ü—É A: A = \\left[A_1, \\ldots, A_B\\right], –≥–¥–µ A_i \\in \\mathbb{R}^{m\\times n_i}. –¢–æ–≥–¥–∞ \nx^{\\text{new}} \\in \\arg\\min\\limits_{x}\\left(\\sum_{i=1}^B f_i(x_i) + u^TAx\\right) \\Rightarrow x^{\\text{new}}_i \\in \\arg\\min\\limits_{x_i}\\left(f_i(x_i) + u^TA_ix_i\\right), \\quad i = \\overline{1,B}\n –¢–æ–≥–¥–∞ –º–µ—Ç–æ–¥ –¥–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—ä–µ–º–∞ –∑–∞–ø–∏—à–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º: \n\\begin{aligned}\nx^{k}_i &\\in \\arg\\min\\limits_{x_i}\\left(f_i(x_i) + u^TA_ix_i\\right), \\quad i = \\overline{1,B} \\\\\nu^k &= u^{k-1} + \\alpha_k\\left(\\sum_{i=1}^B A_ix_i^k - b\\right).\n\\end{aligned}\n\n\n\n\n–ú–µ—Ç–æ–¥ –¥–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–¥—ä–µ–º–∞ –¥–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π-–Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤.\n\n\n\n\n\n\n–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –∑–∞–¥–∞—á—É \\sum\\limits_{i=1}^Bf_i(x_i) \\rightarrow \\min\\limits_{\\sum\\limits_{i=1}^BA_ix_i \\preccurlyeq b}.\n\n\\begin{aligned}\nx_i^k &\\in \\arg \\min\\limits_{x_i}\\left[f_i(x_i) + (u^{k-1})^TA_ix_i\\right], \\quad i = \\overline{1,B} \\\\\nu^k &= \\left(u^{k-1} + \\alpha_k\\left[\\sum\\limits_{i=1}^BA_ix_i^k - b\\right]\\right)_+,\n\\end{aligned}\n\n–≥–¥–µ (u)_+ –æ–±–æ–∑–Ω–∞—á–∞–µ—Ç (u_+)_i = \\max\\{0, u_i\\}, i=\\overline{0,m}.\n\n\n\n–ú–µ—Ç–æ–¥ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –õ–∞–≥—Ä–∞–Ω–∂–∞.\n\n\n\n\n\n\n–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –∑–∞–¥–∞—á—É f(x) + \\frac{\\rho}{2}\\|Ax - b\\|^2 \\rightarrow \\min\\limits_{Ax = b}, –≥–¥–µ \\rho &gt; 0 - –ø–∞—Ä–∞–º–µ—Ç—Ä. –¢–æ–≥–¥–∞ –º–µ—Ç–æ–¥ –¥–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–¥—ä–µ–º–∞ –∏–º–µ–µ—Ç –≤–∏–¥: \n\\begin{aligned}\nx_k &= \\arg \\min\\limits_{x}\\left[f(x) + (u_{k-1})^TAx + \\frac{\\rho}{2}\\|Ax - b\\|^2\\right] \\\\\nu_k &= u_{k-1} + \\rho(Ax_k - b).\n\\end{aligned}\n –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –∏–º–µ–µ—Ç –º–µ—Å—Ç–æ —Å–ª–µ–¥—É—é—â–µ–µ: \n\\begin{aligned}\nL &= f(x) + u^T(Ax - b) + \\frac{\\rho}{2}\\|Ax - b\\|^2 \\\\\nx_k &= \\arg \\min\\limits_{x}\\left[f(x) + (u_{k-1})^TAx + \\frac{\\rho}{2}\\|Ax - b\\|^2\\right]  \\\\\n0 &\\in \\partial f(x_k) + A^T(u_{k-1} + \\rho(Ax_k - b)) \\\\\n0 &\\in \\partial f(x_k) + A^Tu_k.\n\\end{aligned}\n\n\n\n\n–ú–µ—Ç–æ–¥ ADMM.\n\n\n\n\n\n\n–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –∑–∞–¥–∞—á—É \n\\begin{aligned}\n\\min_{x,z}& \\; f(x) + g(z) \\\\\n\\text{s.t. }& Ax + Bz = c\n\\end{aligned}\n –ü–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —à—Ç—Ä–∞—Ñ–∞ –∑–∞ –≤—ã—Ö–æ–¥ –∏–∑ –±—é–¥–∂–µ—Ç–Ω–æ–≥–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∏–º–µ–µ–º f(x) + g(z) + \\|Ax + Bz - c\\|^2 \\rightarrow \\min\\limits_{Ax + Bz = c}, –≥–¥–µ \\rho &gt; 0 - –ø–∞—Ä–∞–º–µ—Ç—Ä. –¢–æ–≥–¥–∞ —Ñ—É–Ω–∫—Ü–∏—è –õ–∞–≥—Ä–∞–Ω–∂–∞ –∏–º–µ–µ—Ç –≤–∏–¥: \nL_\\rho(x, z, u) = f(x) + g(z) + u^T(Ax + Bz - c) + \\frac{\\rho}{2}\\|Ax + Bz - c\\|^2.\n –ò —à–∞–≥ ADMM –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –∫–∞–∫: \n\\begin{aligned}\nx_k &= \\arg\\min\\limits_{x}L_\\rho(x, z_{k-1}, u_{k-1}) \\\\\nz_k &= \\arg\\min\\limits_{z}L_{\\rho}(x_k, z, u_{k-1}) \\\\\nu_k &= u_{k-1} + \\rho(Ax_k + Bz_k - c).\n\\end{aligned}\n\n\n\n\n–§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏ –ª–∏–Ω–µ–π–Ω—ã—Ö –Ω–∞–∏–º–µ–Ω—å—à–∏—Ö –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ —Å \\ell_1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π –≤ —Ñ–æ—Ä–º–µ ADMM.\n\n\n\n\n\n\n–ü—É—Å—Ç—å –∏–º–µ–µ—é—Ç—Å—è b\\in \\mathbb{R}^n, A \\in \\mathbb{R}^{n\\times p} –∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∑–∞–¥–∞—á–∞ lasso: \\frac{1}{2}\\|Ax - b\\|^2_2 + \\lambda \\|x\\|_1. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–æ–±–ª–µ–º—É –∫ ADMM –≤–∏–¥—É: \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda\\|z\\|_1 \\rightarrow \\min\\limits_{x - z = 0}.\n\n\n\n–§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏ –ø–æ–∏—Å–∫–∞ —Ç–æ—á–∫–∏ –Ω–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–∏ –¥–≤—É—Ö –≤—ã–ø—É–∫–ª—ã—Ö –º–Ω–æ–∂–µ—Å—Ç–≤ –≤ —Ñ–æ—Ä–º–µ ADMM.\n\n\n\n\n\n\n–ü—É—Å—Ç—å –∏–º–µ—é—Ç—Å—è –≤—ã–ø—É–∫–ª—ã–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ U, V \\subseteq \\mathbb{R}^n. –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –∑–∞–¥–∞—á—É \\mathbb{I}_U(x) + \\mathbb{I}_V(x) \\rightarrow \\min\\limits_{x}. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–æ–±–ª–µ–º—É –∫ ADMM –≤–∏–¥—É: \\mathbb{I}_U(x) + \\mathbb{I}_V(z) \\rightarrow \\min\\limits_{x - z = 0}\n\n\n\n\n\n\n–¢–µ–æ—Ä–µ–º—ã —Å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º–∏\n\n–¢–µ–æ—Ä–µ–º–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –¥–ª—è –≥–ª–∞–¥–∫–∏—Ö –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π.\n\n\n\n\n\n\n–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –∑–∞–¥–∞—á—É \nf(x) \\rightarrow \\min_{x \\in \\mathbb{R}^d}\n –∏ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ f - –≤—ã–ø—É–∫–ª–∞—è, L-–≥–ª–∞–¥–∫–∞—è, L &gt; 0.\n–ü—É—Å—Ç—å (x_k)_{k \\in \\mathbb{N}} —ç—Ç–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —Å–æ–∑–¥–∞–Ω–Ω–∞—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º —Å–ø—É—Å–∫–æ–º —Å –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º —à–∞–≥–æ–º \\alpha, 0&lt;\\alpha \\leqslant \\frac{1}{L}. –¢–æ–≥–¥–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ —Å—Ö–æ–¥–∏—Ç—Å—è —Å—É–±–ª–∏–Ω–µ–π–Ω–æ, —Ç–æ –µ—Å—Ç—å: \nf(x_k)-f^* \\leq \\frac{\\|x_0-x^*\\| ^2}{2 \\alpha k}.\n\n\n\n\n\n–§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –º–µ—Ç–æ–¥–∞: \nx_{k + 1} = x_k  - \\alpha \\nabla f(x_k) \\Rightarrow x_{k + 1} - x_k = -\\alpha \\nabla f(x_k)\n\nL-–≥–ª–∞–¥–∫–æ—Å—Ç—å: \\forall x, y: f(y) \\leqslant f(x) + \\langle \\nabla f(x), y - x \\rangle + \\frac{L}{2} \\| y - x \\|^2 \ny := x_{k+1}, x := x_k \\Rightarrow f(x_{k + 1}) \\leqslant f(x_k) + \\langle \\nabla f(x_k), - \\alpha \\nabla f(x_k) \\rangle + \\frac{L}{2} \\alpha^2 \\| \\nabla f(x_k) \\|^2\n \nf(x_{k + 1}) \\leqslant f(x_k) - \\alpha \\| \\nabla f(x_k) \\|^2 + \\frac{L}{2} \\alpha^2 \\| \\nabla f(x_k) \\|^2 \\qquad (1)\n\n–†–µ—à–∏–º –∑–∞–¥–∞—á—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ —à–∞–≥–∞ \\left(\\frac{L}{2} \\alpha^2 - \\alpha\\right) \\rightarrow \\min\\limits_{\\alpha}. –ü–æ–ª—É—á–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —à–∞–≥: \\alpha = \\frac{1}{L} –∏ f(x_k) - f(x_{k + 1}) \\geqslant \\frac{1}{2L} \\| \\nabla f(x_k) \\|^2\n–í—ã–ø—É–∫–ª–æ—Å—Ç—å: f(y) \\geqslant f(x) + \\nabla f(x)^T(y - x) \ny := x^*, x := x_k \\Rightarrow f(x^*) \\geqslant f(x_k) + \\nabla f(x_k)^T(x^* - x_k) \\Rightarrow\n \n\\Rightarrow f(x_k) \\leqslant f(x^*) + \\nabla f(x_k)^T(x_k - x^*) \\Rightarrow f(x_k) - f(x^*) \\leqslant \\nabla f(x_k)^T(x_k - x^*)\n\n–ü–æ–¥—Å—Ç–∞–≤–∏–º –≤—ã–ø—É–∫–ª–æ—Å—Ç—å –≤ (1): \n\\begin{split}\nf(x_{k+1}) &\\leq f(x_k) -\\frac{\\alpha}{2} \\Vert \\nabla f(x_k)\\Vert^2 \\leq f^* + \\langle \\nabla f(x_k), x_k-x^*\\rangle - \\frac{\\alpha}{2} \\Vert \\nabla f(x_k)\\Vert^2 \\\\\n&= f^* + \\langle \\nabla f(x_k), x_k-x^* - \\frac{\\alpha}{2} \\nabla f(x_k)\\rangle \\\\\n&= f^* + \\frac{1}{2 \\alpha}\\left\\langle \\alpha \\nabla f(x_k), 2\\left(x_k-x^* - \\frac{\\alpha}{2} \\nabla f(x_k)\\right)\\right\\rangle\n\\end{split}\n\n–ü—É—Å—Ç—å a = x_k-x^* –∏ b =x_k-x^* - \\alpha\\nabla f(x_k). –¢–æ–≥–¥–∞ a+b = \\alpha \\nabla f(x_k) –∏ a-b=2\\left(x_k-x^* - \\frac{\\alpha}{2} \\nabla f(x_k)\\right). \n\\begin{split}\nf(x_{k+1}) &\\leq f^* + \\frac{1}{2 \\alpha}\\left[ \\|x_k-x^*\\|_2^2 - \\|x_k-x^* - \\alpha\\nabla f(x_k)\\|_2^2\\right] \\\\\n&\\leq f^* + \\frac{1}{2 \\alpha}\\left[ \\|x_k-x^*\\|_2^2 - \\|x_{k+1}-x^*\\|_2^2\\right] \\\\\n2\\alpha \\left(f(x_{k+1}) - f^*\\right) &\\leq \\|x_k-x^*\\|_2^2 - \\|x_{k+1}-x^*\\|_2^2\n\\end{split}\n\n–ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–æ–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ i –∏ –º—ã —Å—É–º–º–∏—Ä—É–µ–º –ø–æ i \\in [0, k-1]. –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å–ª–∞–≥–∞–µ–º—ã—Ö –±—É–¥—É—Ç —Ä–∞–≤–Ω—ã –Ω—É–ª—é –∏–∑-–∑–∞ —Ç–µ–ª–µ—Å–∫–æ–ø–∏—á–µ—Å–∫–æ–π –ø—Ä–∏—Ä–æ–¥—ã —Å—É–º–º—ã: \n\\begin{split}\n2\\alpha \\sum\\limits_{i=0}^{k-1} \\left(f(x_{i+1}) - f^*\\right) &\\leq \\|x_0-x^*\\|_2^2 - \\|x_{k}-x^*\\|_2^2 \\leq \\|x_0-x^*\\|_2^2\n\\end{split}\n  \\tag{1}\n–ò–∑-–∑–∞ –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ–≥–æ —É–±—ã–≤–∞–Ω–∏—è –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ f(x_{i+1}) &lt; f(x_i): \nkf(x_k) \\leq \\sum\\limits_{i=0}^{k-1}f(x_{i+1})\n\n–ü–æ–¥—Å—Ç–∞–≤–∏–º –≤ (1): \n\\begin{split}\n2\\alpha kf(x_k) - 2\\alpha kf^* &\\leq 2\\alpha \\sum\\limits_{i=0}^{k-1} \\left(f(x_{i+1}) - f^*\\right)  \\leq \\|x_0-x^*\\|_2^2 \\\\\nf(x_k) - f^* &\\leq \\frac{\\|x_0-x^*\\|_2^2}{2 \\alpha k} \\leq  \\frac{L \\|x_0-x^*\\|_2^2}{2 k}\n\\end{split}\n –¢–æ –µ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å —Å—É–±–ª–∏–Ω–µ–π–Ω–∞—è.\n\n–¢–µ–æ—Ä–µ–º–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –¥–ª—è –≥–ª–∞–¥–∫–∏—Ö PL —Ñ—É–Ω–∫—Ü–∏–π.\n\n\n\n\n\n\n–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –∑–∞–¥–∞—á—É \nf(x) \\to \\min_{x \\in \\mathbb{R}^d}\n –∏ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ f —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è–µ—Ç —É—Å–ª–æ–≤–∏—é –ü–æ–ª—è–∫–∞-–õ–æ—è—Å–∏–µ–≤–∏—á–∞ —Å –∫–æ–Ω—Å—Ç–∞–Ω—Ç–æ–π \\mu –∏ L-–≥–ª–∞–¥–∫–æ—Å—Ç–∏, –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö L\\geq \\mu &gt;0. –ü—É—Å—Ç—å (x_k)_{k \\in \\mathbb{N}} - –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —Å–æ–∑–¥–∞–Ω–Ω–∞—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º —Å–ø—É—Å–∫–æ–º —Å –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º —à–∞–≥–æ–º \\alpha, 0&lt;\\alpha \\leq \\frac{1}{L}. –¢–æ–≥–¥–∞ –∏–º–µ–µ—Ç—Å—è –ª–∏–Ω–µ–π–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å: \nf(x_k)-f^* \\leq (1-\\alpha \\mu)^k (f(x_0)-f^*).\n\n\n\n\n\n–ò—Å–ø–æ–ª—å–∑—É—è L-–≥–ª–∞–¥–∫–æ—Å—Ç—å, –≤–º–µ—Å—Ç–µ —Å –ø—Ä–∞–≤–∏–ª–æ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞, –º–æ–∂–Ω–æ –∑–∞–ø–∏—Å–∞—Ç—å: \n\\begin{split}\nf(x_{k+1})& \\leq f(x_k) + \\langle \\nabla f(x_k), x_{k+1}-x_k \\rangle +\\frac{L}{2} \\| x_{k+1}-x_k\\|^2\\\\\n&= f(x_k)-\\alpha\\Vert \\nabla f(x_k) \\Vert^2 +\\frac{L \\alpha^2}{2} \\| \\nabla f(x_k)\\|^2 \\\\\n&= f(x_k) - \\frac{\\alpha}{2} \\left(2 - L \\alpha \\right)\\Vert \\nabla f(x_k) \\Vert^2 \\\\\n& \\leq f(x_k) - \\frac{\\alpha}{2}\\Vert \\nabla f(x_k)\\Vert^2,\n\\end{split}\n –í –ø–æ—Å–ª–µ–¥–Ω–µ–º –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ –æ —à–∞–≥–µ 0 &lt; \\alpha L \\leq 1.\n–ò—Å–ø–æ–ª—å–∑—É—è —Å–≤–æ–π—Å—Ç–≤–æ –ü–æ–ª—è–∫–∞-–õ–æ—è—Å–∏–µ–≤–∏—á–∞, –º–æ–∂–Ω–æ –∑–∞–ø–∏—Å–∞—Ç—å: \nf(x_{k+1}) \\leq f(x_k) - \\alpha \\mu (f(x_k) - f^*).\n\n–í—ã—á–∏—Ç–∞—è f^* —Å –æ–±–µ–∏—Ö —Å—Ç–æ—Ä–æ–Ω –∏ –∏—Å–ø–æ–ª—å–∑—É—è —Ä–µ–∫—É—Ä—Å–∏—é, –ø–æ–ª—É—á–∞–µ–º: \nf(x_k) - f^* \\leq (1-\\alpha \\mu)^k (f(x_0) - f^*).\n\n\n–¢–µ–æ—Ä–µ–º–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –¥–ª—è —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã—Ö –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π. –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã.\n\n\n\n\n\n\n f(x) \\rightarrow \\min\\limits_{x \\in \\mathbb{R}^d}   f(x) = \\frac{1}{2}x^TAx - b^Tx + c, \\ A \\in \\mathbb{S}_{++}  –¢–æ–≥–¥–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ —Å —à–∞–≥–æ–º \\alpha = \\frac{2}{\\mu + L} —Å—Ö–æ–¥–∏—Ç—Å—è –ª–∏–Ω–µ–π–Ω–æ —Å –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–º \\frac{L-\\mu}{L+\\mu} \nf(x_k) - f^* \\leqslant \\left(\\frac{L-\\mu}{L+\\mu}\\right)^k(f(x_0) - f^*).\n\n\n\n\n\n\\nabla f(x) = Ax - b \\overset{\\nabla f(x^*) = 0}{\\Rightarrow} Ax^* = b\n –¢–æ–≥–¥–∞ —à–∞–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –∏–º–µ–µ—Ç –≤–∏–¥ \nx_{k+1} = x_k - \\alpha (Ax - b)\n –ù–∞–π–¥–µ–º \\alpha^*. –í–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è A = Q\\Lambda Q^T, –≥–¥–µ \\Lambda = \\text{diag}\\{\\lambda_1, \\ldots, \\lambda_n\\}, \\ Q = \\|q_1, \\ldots, q_n\\|, \\lambda_i, q_i - —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ.  x_{k+1}  = (I - \\alpha A)x_k + \\alpha A x^* \\ | \\ -x^*  x_{k + 1} - x^* = (I - \\alpha A)(x_k - x^*)   x_{k + 1} - x^* = (I - \\alpha Q\\Lambda Q^T)(x_k - x^*) \\ | \\ \\cdot Q^T   Q^T(x_{k+1} - x^*) = (Q^T - \\alpha \\Lambda Q^T)(x_k - x^*) = (I - \\alpha \\Lambda)Q^T(x_k - x^*)   \\text{–ó–∞–º–µ–Ω–∞: } \\tilde{x} = Q^T(x - x^*) \\Rightarrow \\tilde{x}_{k+1} = (I - \\alpha \\Lambda)\\tilde{x}_{k} \\Leftrightarrow \\tilde{x}_i^{(k+1)} = (1 - \\alpha \\lambda_i)\\tilde{x}_i^{(k)} \\ i=\\overline{1,d}   \\lambda_{\\min} = \\mu, \\quad \\lambda_{\\max} = L  –°—Ö–æ–¥–∏–º–æ—Å—Ç—å –µ—Å—Ç—å \\Leftrightarrow \\max\\limits_{i}|1 - \\alpha \\lambda_i| &lt; 1 \n\\left\\{\n\\begin{array}{rl}\n    |1 - \\lambda \\mu| &lt; 1 \\Rightarrow &1 - \\lambda \\mu &lt; 1 \\Rightarrow \\alpha &gt; 0 \\\\\n    &\\alpha \\mu - 1 &lt; 1 \\Rightarrow \\alpha &lt; \\frac{2}{\\mu} \\\\\n    |1 - \\alpha L| &lt; 1 \\Rightarrow &1 - \\alpha L &lt; 1 \\Rightarrow \\alpha &gt; 0 \\\\\n    &\\alpha L - 1 &lt; 1 \\Rightarrow \\alpha &lt; \\frac{2}{L}\n\\end{array}\n\\right\\} \\Rightarrow \\alpha &lt; \\frac{2}{L}\n –†–∞–¥–∏—É—Å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ \\rho = \\max (|1 - \\alpha \\mu|, |1 - \\alpha L|) –∏ \\rho \\rightarrow \\min \\Leftrightarrow \\alpha^*L - 1 = 1 - \\alpha^*\\mu \\Rightarrow \\alpha^* = \\frac{2}{\\mu + L} –∏ \\rho^* = \\frac{L - \\mu}{L + \\mu}\n–ò—Ç–æ–≥–æ –ø–æ–ª—É—á–∞–µ–º, —á—Ç–æ –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è f(x_k) - f^* \\leqslant \\left(1 - \\frac{\\mu}{\\mu + L}\\right)^k(f(x_0) - f^*).\n–¢–µ–æ—Ä–µ–º–∞ –æ –Ω–∏–∂–Ω–µ–π –æ—Ü–µ–Ω–∫–µ –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ –≥–ª–∞–¥–∫–∏—Ö –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤ –ø–µ—Ä–≤–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞.\n\n\n\n\n\n\n–†–∞—Å—Å–º–æ—Ç—Ä–∏–º —Å–µ–º–µ–π—Å—Ç–≤–æ –º–µ—Ç–æ–¥–æ–≤ –ø–µ—Ä–≤–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞, –≥–¥–µ \n\\begin{aligned}\nx^{k+1} &\\in x^0 + \\text{span} \\left\\{\\nabla f(x^{0}), \\nabla f(x^{1}), \\ldots, \\nabla f(x^{k})\\right\\} \\; & f \\text{ - –≥–ª–∞–¥–∫–∞—è} \\\\\nx^{k+1} &\\in x^0 + \\text{span} \\left\\{g_{0}, g_{1}, \\ldots, g_{k}\\right\\} \\text{, –≥–¥–µ }\ng_{i} \\in \\partial f(x^{i}) \\; & f \\text{ - –Ω–µ–≥–ª–∞–¥–∫–∞—è}\n\\end{aligned}\n\\tag{2}\n–°—É—â–µ—Å—Ç–≤—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏—è f, –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è L-–≥–ª–∞–¥–∫–æ–π –∏ –≤—ã–ø—É–∫–ª–æ–π, —Ç–∞–∫ —á—Ç–æ –ª—é–±–æ–π –º–µ—Ç–æ–¥ –≤ —Ñ–æ—Ä–º–µ (2) —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è–µ—Ç –¥–ª—è –ª—é–±–æ–≥–æ k: 1 \\leq k \\leq \\frac{n-1}{2}: \nf(x^k) - f^* \\geq \\frac{3L \\|x^0 - x^*\\|_2^2}{32(k+1)^2}\n\n\n\n\n\n–ù–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ç–æ–≥–æ, –∫–∞–∫–æ–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≤–∏–¥–∞ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ, –≤—Å–µ–≥–¥–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏—è f, –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –Ω–∞ –Ω–µ–π –≤–∞—à–µ–≥–æ –º–µ—Ç–æ–¥–∞, –Ω–µ–≤—è–∑–∫–∞ –±—É–¥–µ—Ç —É–±—ã–≤–∞—Ç—å –Ω–µ –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º \\mathcal{O}\\left(\\frac{1}{k^2}\\right).\n–ö–ª—é—á–æ–º –∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤—É —è–≤–ª—è–µ—Ç—Å—è —è–≤–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ f.\n\n–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ:\n\n–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø—Ä–∏–º–µ—Ä –ø—Ä–∏ n=3: \nA = \\begin{bmatrix}\n     2 & -1 & 0 \\\\\n     -1 & 2 & -1 \\\\\n     0 & -1 & 2 \\\\\n     \\end{bmatrix}\n –û—Ü–µ–Ω–∫–∞ —Å–Ω–∏–∑—É: \n\\begin{aligned}\nx^T A x &= 2x_1^2 + 2x_2^2 + 2x_3^2 - 2x_1x_2 - 2x_2x_3 \\\\\n&= x_1^2 + x_1^2 - 2x_1x_2 + x_2^2 + x_2^2 - 2x_2x_3 + x_3^2 + x_3^2 \\\\\n&= x_1^2 + (x_1 - x_2)^2 + (x_2 - x_3)^2 + x_3^2 \\geq 0\n\\end{aligned}\n –û—Ü–µ–Ω–∫–∞ —Å–≤–µ—Ä—Ö—É: \n\\begin{aligned}\nx^T A x &= 2x_1^2 + 2x_2^2 + 2x_3^2 - 2x_1x_2 - 2x_2x_3 \\\\\n& \\leq 4(x_1^2 + x_2^2 + x_3^2) \\\\\n0 &\\leq 2x_1^2 + 2x_2^2 + 2x_3^2 + 2x_1x_2 + 2x_2x_3 \\\\\n0 &\\leq x_1^2 + x_1^2 + 2x_1x_2 + x_2^2 + x_2^2 + 2x_2x_3 + x_3^2 + x_3^2 \\\\\n0 &\\leq x_1^2 + (x_1 + x_2)^2 + (x_2 + x_3)^2 + x_3^2\n\\end{aligned}\n\n–û–ø—Ä–µ–¥–µ–ª–∏–º —Å–ª–µ–¥—É—é—â—É—é L-–≥–ª–∞–¥–∫—É—é –≤—ã–ø—É–∫–ª—É—é —Ñ—É–Ω–∫—Ü–∏—é: f(x) = \\frac{L}{4}\\left(\\frac{1}{2} x^T A x - e_1^T x \\right) = \\frac{L}{8} x^T A x - \\frac{L}{4} e_1^T x. –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ x^* —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è–µ—Ç Ax^* = e_1, –∏ —Ä–µ—à–µ–Ω–∏–µ —ç—Ç–æ–π —Å–∏—Å—Ç–µ–º—ã —É—Ä–∞–≤–Ω–µ–Ω–∏–π –¥–∞–µ—Ç: \n\\begin{bmatrix}\n     2 & -1 & 0 & 0 & \\cdots & 0 \\\\\n     -1 & 2 & -1 & 0 & \\cdots & 0 \\\\\n     0 & -1 & 2 & -1  & \\cdots & 0 \\\\\n     0 & 0 & -1 & 2  & \\cdots & 0 \\\\\n     \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n     0 & 0 & 0 & 0 & \\cdots & 2  \\\\\n\\end{bmatrix} \\begin{bmatrix}\n     x_1^* \\\\\n     x_2^* \\\\\n     x_3^* \\\\\n     \\vdots \\\\\n     x_{n}^* \\\\\n\\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\quad \\begin{cases} 2x_1^* - x_2^* = 1 \\\\ -x_i^* + 2x_{i+1}^* - x_{i+2}^* = 0, \\; i = 2, \\ldots, n-1 \\\\ -x_{n-1}^* + 2x_n^* = 0 \\end{cases}\n\n–ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ x_i^* = a+bi. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –≤—Ç–æ—Ä–æ–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è–µ—Ç—Å—è, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ a –∏ b –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —É—Ä–∞–≤–Ω–µ–Ω–∏–π. –†–µ—à–µ–Ω–∏–µ: \nx^*_i = 1 - \\frac{i}{n+1},\n –ó–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–∏ —ç—Ç–æ–º: \nf(x^*) =  \\frac{L}{8} {x^*}^T A x^* - \\frac{L}{4}\\langle x^*, e_1 \\rangle = -\\frac{L}{8} \\langle x^*, e_1 \\rangle = -\\frac{L}{8} \\left(1 - \\frac{1}{n+1}\\right).\n\n–ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ –º—ã –Ω–∞—á–∏–Ω–∞–µ–º —Å x^0 = 0. –ó–∞–ø—Ä–æ—Å–∏–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç —É –æ—Ä–∞–∫—É–ª–∞, –º—ã –ø–æ–ª—É—á–∞–µ–º g_0 = -e_1. –¢–æ–≥–¥–∞ x^1 –¥–æ–ª–∂–µ–Ω –ª–µ–∂–∞—Ç—å –Ω–∞ –ø—Ä—è–º–æ–π, –∑–∞–¥–∞–≤–∞–µ–º–æ–π e_1. –í —ç—Ç–æ–π —Ç–æ—á–∫–µ –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã x^1 —Ä–∞–≤–Ω—ã –Ω—É–ª—é, –∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–π, –ø–æ—ç—Ç–æ–º—É \nx^1 = \\begin{bmatrix} \\bullet \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}.\n\n–ù–∞ –≤—Ç–æ—Ä–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –º—ã —Å–Ω–æ–≤–∞ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç –∏ –ø–æ–ª—É—á–∞–µ–º g_1 = Ax^1 - e_1. –¢–æ–≥–¥–∞ x^2 –¥–æ–ª–∂–µ–Ω –ª–µ–∂–∞—Ç—å –Ω–∞ –ø—Ä—è–º–æ–π, –∑–∞–¥–∞–≤–∞–µ–º–æ–π e_1 –∏ Ax^1 - e_1. –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã x^2 —Ä–∞–≤–Ω—ã –Ω—É–ª—é, –∫—Ä–æ–º–µ –ø–µ—Ä–≤—ã—Ö –¥–≤—É—Ö, –ø–æ—ç—Ç–æ–º—É \n\\begin{bmatrix}\n     2 & -1 & 0  & \\cdots & 0 \\\\\n     -1 & 2 & -1 & \\cdots & 0 \\\\\n     0 & -1 & 2 & \\cdots & 0 \\\\\n     \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n     0 & 0 & 0 & \\cdots & 2  \\\\\n\\end{bmatrix} \\begin{bmatrix} \\bullet \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\Rightarrow x^2 = \\begin{bmatrix} \\bullet \\\\ \\bullet \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}.\n\n–ò–∑-–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–∞—Ç—Ä–∏—Ü—ã A –º–æ–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å, —á—Ç–æ –ø–æ—Å–ª–µ k –∏—Ç–µ—Ä–∞—Ü–∏–π –≤—Å–µ –ø–æ—Å–ª–µ–¥–Ω–∏–µ n-k –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã x^k —Ä–∞–≤–Ω—ã –Ω—É–ª—é. \nx^{(k)} =\n\\begin{bmatrix}\n     \\bullet \\\\\n     \\bullet \\\\\n     \\vdots \\\\\n     \\bullet \\\\\n     0 \\\\\n     \\vdots \\\\\n     0\n\\end{bmatrix}\n\\begin{array}{l}\n     1 \\\\\n     2 \\\\\n     \\vdots \\\\\n     k \\\\\n     k+1 \\\\\n     \\vdots \\\\\n     n\n\\end{array}\n\n–û–¥–Ω–∞–∫–æ, –ø–æ—Å–∫–æ–ª—å–∫—É –∫–∞–∂–¥–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è x^k, –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–Ω–∞—è –Ω–∞—à–∏–º –º–µ—Ç–æ–¥–æ–º, –ª–µ–∂–∏—Ç –≤ –ª–∏–Ω–µ–π–Ω–æ–π –æ–±–æ–ª–æ—á–∫–µ S_k = \\text{span}\\{e_1, e_2, \\ldots, e_{k}\\} (—Ç.–µ. –∏–º–µ–µ—Ç –Ω—É–ª–∏ –≤ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö k+1,\\dots,n), –æ–Ω–∞ –Ω–µ –º–æ–∂–µ—Ç ‚Äú–¥–æ—Å—Ç–∏—á—å‚Äù –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ x^*. –î—Ä—É–≥–∏–º–∏ —Å–ª–æ–≤–∞–º–∏, –¥–∞–∂–µ –µ—Å–ª–∏ –±—ã –º—ã –≤—ã–±—Ä–∞–ª–∏ –ª—É—á—à–∏–π –≤–æ–∑–º–æ–∂–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –∏–∑ S_k, –æ–±–æ–∑–Ω–∞—á–∞–µ–º—ã–π \n\\tilde{x}^k=\\arg\\min_{x\\in S_k} f(x),\n –∑–Ω–∞—á–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ f(\\tilde{x}^k) –±—É–¥–µ—Ç —Å—Ç—Ä–æ–≥–æ —Ö—É–∂–µ, —á–µ–º f(x^*).\n–ü–æ—Å–∫–æ–ª—å–∫—É x^k\\in S_k = \\text{span}\\{e_1, e_2, \\ldots, e_{k}\\} –∏ \\tilde{x}^k —è–≤–ª—è–µ—Ç—Å—è –ª—É—á—à–∏–º –≤–æ–∑–º–æ–∂–Ω—ã–º –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ–º –∫ x^* –≤ S_k, –º—ã –∏–º–µ–µ–º \nf(x^k)\\ge f(\\tilde{x}^k).\n –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∑–∞–∑–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç–∏: \nf(x^k)-f(x^*)\\ge f(\\tilde{x}^k)-f(x^*).\n\n–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ, –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –∏—Å—Ö–æ–¥–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏, –º—ã –∏–º–µ–µ–º \\tilde{x}^k_i = 1 - \\frac{i}{k+1} –∏ f(\\tilde{x}^k) = -\\frac{L}{8} \\left(1 - \\frac{1}{k+1}\\right). –¢–æ–≥–¥–∞, –º—ã –∏–º–µ–µ–º: \n\\begin{aligned}\nf(x^k)-f(x^*) &\\ge f(\\tilde{x}^k)-f(x^*) \\\\\n&= -\\frac{L}{8} \\left(1 - \\frac{1}{k+1}\\right) - \\left(-\\frac{L}{8} \\left(1 - \\frac{1}{n+1}\\right)\\right) \\\\\n&= \\frac{L}{8} \\left(\\frac{1}{k+1} - \\frac{1}{n+1}\\right) = \\frac{L}{8} \\left(\\frac{n-k}{(k+1)(n+1)}\\right) \\\\\n&\\overset{n = 2k+1}{=} \\frac{L }{16(k+1)}\n\\end{aligned}\n  \\tag{3}\n–¢–µ–ø–µ—Ä—å –º—ã –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º R = \\|x^0 - x^*\\|_2: \n\\begin{aligned}\n\\|x^0 - x^*\\|_2^2 &= \\|0 - x^*\\|_2^2 = \\|x^*\\|_2^2 = \\sum_{i=1}^n \\left( 1 - \\frac{i}{n+1} \\right)^2 \\\\\n&= n - \\frac{2}{n+1} \\sum_{i=1}^{n} i + \\frac{1}{(n+1)^2} \\sum_{i=1}^{n} i^2 \\\\\n&\\leq n - \\frac{2}{n+1} \\cdot \\frac{n(n+1)}{2} + \\frac{1}{(n+1)^2} \\cdot \\frac{(n+1)^3}{3} \\\\\n&= \\frac{n+1}{3} \\overset{n = 2k+1}{=} \\frac{2(k+1)}{3}.\n\\end{aligned}\n –¢–æ –µ—Å—Ç—å, \nk+1 \\geq \\frac{3}{2}\\|x^0 - x^*\\|_2^2. = \\frac32 R^2\n  \\tag{4}\n–ó–∞–º–µ—Ç–∏–º, —á—Ç–æ \n\\begin{aligned}\n\\sum_{i=1}^{n} i &= \\frac{n(n+1)}{2} \\\\\n\\sum_{i=1}^{n} i^2 &= \\frac{n(n+1)(2n+1)}{6} \\leq \\frac{(n+1)^3}{3}\n\\end{aligned}\n\n–ù–∞–∫–æ–Ω–µ—Ü, –∏—Å–ø–æ–ª—å–∑—É—è (3) –∏ (4), –º—ã –ø–æ–ª—É—á–∞–µ–º: \n\\begin{aligned}\nf(x^k) - f(x^*) &\\geq \\frac{L}{16(k+1)}  = \\frac{L}{16(k+1)^2}(k+1) \\\\\n&\\geq \\frac{L}{16(k+1)^2} \\frac{3}{2} R^2 = \\frac{3L R^2}{32 (k+1)^2}\n\\end{aligned}\n –ß—Ç–æ –∑–∞–≤–µ—Ä—à–∞–µ—Ç –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –Ω–∏–∂–Ω–µ–π –æ—Ü–µ–Ω–∫–∏ —Å –∂–µ–ª–∞–µ–º–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é \\mathcal{O}\\left( \\frac{1}{k^2}\\right).\n\n–í—ã–≤–æ–¥ —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é –ø–æ–ª–∏–Ω–æ–º–æ–≤ –ß–µ–±—ã—à—ë–≤–∞.\n\n\n\n\n\n\n–†–µ—à–∞–µ–º –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—É—é –∑–∞–¥–∞—á—É: \nf(x) = \\frac{1}{2} x^T A x - b^T x \\qquad x_{k+1} = x_k - \\alpha_k \\nabla f(x_k)\n –ú–æ–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å, —á—Ç–æ –º–µ—Ç–æ–¥, –ø–æ–ª—É—á–µ–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é –ø–æ–ª–∏–Ω–æ–º–æ–≤ –ß–µ–±—ã—à—ë–≤–∞, –∏–º–µ–µ—Ç –≤–∏–¥: \nx_{k+1} = x_k - \\alpha_k \\nabla f(x_k) + \\beta_k \\left(x_k - x_{k-1}\\right)\n –æ–±–ª–∞–¥–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–π –ª–∏–Ω–µ–π–Ω–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å—é.\n\n\n\n\n–ü—É—Å—Ç—å x^* - –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –ª–∏–Ω–µ–π–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã Ax=b –∏ –ø—É—Å—Ç—å e_k = x_k-x^*, –≥–¥–µ x_{k+1}=x_k - \\alpha_k (Ax_k-b) –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ, –Ω–∞—á–∏–Ω–∞—è —Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ x_0, –∏ \\alpha_k - —à–∞–≥, –∫–æ—Ç–æ—Ä—ã–π –º—ã –æ–ø—Ä–µ–¥–µ–ª–∏–º –ø–æ–∑–∂–µ. \ne_{k+1} = (I-\\alpha_k A)e_k.\n 1.–í—ã—à–µ—É–∫–∞–∑–∞–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç –¥–∞–µ—Ç –Ω–∞–º e_k = p_k(A)e_0, –≥–¥–µ p_k - –ø–æ–ª–∏–Ω–æ–º \np_k(a) = \\prod_{i=1}^k (1-\\alpha_ia).\n –ú—ã –º–æ–∂–µ–º –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å —Å–≤–µ—Ä—Ö—É –Ω–æ—Ä–º—É –æ—à–∏–±–∫–∏ –∫–∞–∫ \n\\|e_k\\|\\le \\|p_k(A)\\|\\cdot\\|e_0\\|\\,.\n –ü–æ—Å–∫–æ–ª—å–∫—É A - —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —Å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –≤ [\\mu,L],: \n\\|p_k(A)\\|\\le \\max_{\\mu\\le a\\le L} \\left|p_k(a)\\right|\\,.\n –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–π –ø—Ä–æ–±–ª–µ–º–µ: —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –ø–æ–ª–∏–Ω–æ–º–æ–≤, —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—â–∏—Ö p_k(0)=1, –º—ã –∏—â–µ–º –ø–æ–ª–∏–Ω–æ–º, –≤–µ–ª–∏—á–∏–Ω–∞ –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–∞–∏–º–µ–Ω—å—à–∞—è –≤ –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ [\\mu,L].\n–ù–∞–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å–æ—Å—Ç–æ–∏—Ç –≤ –≤—ã–±–æ—Ä–µ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —à–∞–≥–∞ \\alpha_k=\\frac{2}{\\mu+L} –≤ –≤—ã—Ä–∞–∂–µ–Ω–∏–∏. –≠—Ç–æ—Ç –≤—ã–±–æ—Ä –¥–µ–ª–∞–µ—Ç |p_k(\\mu)| = |p_k(L)|. \n\\|e_k\\|\\le \\left(\\dfrac{L - \\mu}{L + \\mu}\\right)^k\\|e_0\\|\n –≠—Ç–æ —Ç–æ—á–Ω–æ —Ç–∞–∫–æ–π –∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –∫–æ—Ç–æ—Ä—ã–π –º—ã –¥–æ–∫–∞–∑–∞–ª–∏ –¥–ª—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –≤ —Å–ª—É—á–∞–µ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏.\n–î–∞–≤–∞–π—Ç–µ –≤–∑–≥–ª—è–Ω–µ–º –Ω–∞ —ç—Ç–æ—Ç –ø–æ–ª–∏–Ω–æ–º –ø–æ–±–ª–∏–∂–µ. –ù–∞ –ø—Ä–∞–≤–æ–º —Ä–∏—Å—É–Ω–∫–µ –º—ã –≤—ã–±—Ä–∞–ª–∏ \\alpha=1 –∏ \\beta=10 —Ç–∞–∫, —á—Ç–æ \\kappa=10. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª, —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —Ä–∞–≤–µ–Ω [1,10].\n–ú–æ–∂–µ–º –ª–∏ –º—ã —Å–¥–µ–ª–∞—Ç—å –ª—É—á—à–µ? –û—Ç–≤–µ—Ç - –¥–∞.\n\n\n\n\n\n–ü–æ–ª–∏–Ω–æ–º—ã –ß–µ–±—ã—à—ë–≤–∞ –æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∫–æ—Ç–æ—Ä—ã–π –º—ã –∑–∞–¥–∞–≤–∞–ª–∏. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, –æ–Ω–∏ –º–∏–Ω–∏–º–∏–∑–∏—Ä—É—é—Ç –∞–±—Å–æ–ª—é—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∂–µ–ª–∞–µ–º–æ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ [\\mu,L] –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏, —á—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ —Ä–∞–≤–Ω–æ 1 –≤ –Ω–∞—á–∞–ª–µ.\n\n\\begin{aligned}\nT_0(x) &= 1\\\\\nT_1(x) &= x\\\\\nT_k(x) &=2xT_{k-1}(x)-T_{k-2}(x),\\qquad k\\ge 2.\\\\\n\\end{aligned}\n\n–î–∞–≤–∞–π—Ç–µ –ø–æ—Å—Ç—Ä–æ–∏–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–æ–ª–∏–Ω–æ–º—ã –ß–µ–±—ã—à—ë–≤–∞ (–±–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è):\n\n\n\n\n\n–ò—Å—Ö–æ–¥–Ω—ã–µ –ø–æ–ª–∏–Ω–æ–º—ã –ß–µ–±—ã—à—ë–≤–∞ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –Ω–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ [-1,1]. –ß—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö –¥–ª—è –Ω–∞—à–∏—Ö —Ü–µ–ª–µ–π, –Ω–∞–º –Ω—É–∂–Ω–æ –∏—Ö –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª [\\mu,L].\n–ú—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â—É—é –∞—Ñ—Ñ–∏–Ω–Ω—É—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é: \nx = \\frac{L + \\mu - 2a}{L - \\mu}, \\quad a \\in [\\mu,L], \\quad x \\in [-1,1].\n\n–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ x=1 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç a=\\mu, x=-1 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç a=L –∏ x=0 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç a=\\frac{\\mu+L}{2}. –≠—Ç–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –ø–æ–ª–∏–Ω–æ–º–∞ –ß–µ–±—ã—à—ë–≤–∞ –Ω–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ [-1,1] –æ—Ç—Ä–∞–∂–∞–µ—Ç—Å—è –≤ –∏–Ω—Ç–µ—Ä–≤–∞–ª [\\mu, L]\n–í –Ω–∞—à–µ–º –∞–Ω–∞–ª–∏–∑–µ –æ—à–∏–±–æ–∫ –º—ã —Ç—Ä–µ–±—É–µ–º, —á—Ç–æ–±—ã –ø–æ–ª–∏–Ω–æ–º –±—ã–ª —Ä–∞–≤–µ–Ω 1 –≤ 0 (—Ç.–µ., p_k(0)=1). –ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∑–Ω–∞—á–µ–Ω–∏–µ T_k –≤ —Ç–æ—á–∫–µ, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π a=0, –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å 1. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –º—ã –Ω–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–∏–Ω–æ–º T_k, –¥–µ–ª—è –µ–≥–æ –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏–µ T_k\\left(\\frac{L+\\mu}{L-\\mu}\\right): \n\\frac{L+\\mu}{L-\\mu}, \\qquad \\text{–≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—è, —á—Ç–æ} \\qquad P_k(0)= T_k\\left(\\frac{L+\\mu-0}{L-\\mu}\\right) \\cdot T_k\\left(\\frac{L+\\mu}{L-\\mu}\\right)^{-1} = 1.\n\n–î–∞–≤–∞–π—Ç–µ –ø–æ—Å—Ç—Ä–æ–∏–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–ª–∏–Ω–æ–º—ã –ß–µ–±—ã—à—ë–≤–∞ \nP_k(a) = T_k\\left(\\frac{L+\\mu-2a}{L-\\mu}\\right) \\cdot T_k\\left(\\frac{L+\\mu}{L-\\mu}\\right)^{-1}\n –∏ –Ω–∞–±–ª—é–¥–∞–µ–º, —á—Ç–æ –æ–Ω–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ª—É—á—à–µ –≤–µ–¥—É—Ç —Å–µ–±—è –≤ –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ [\\mu,L] –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –Ω–∞–∏–≤–Ω—ã–º–∏ –ø–æ–ª–∏–Ω–æ–º–∞–º–∏.\n     \n–ú—ã –≤–∏–¥–∏–º, —á—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª–∏–Ω–æ–º–∞ –ß–µ–±—ã—à—ë–≤–∞ –Ω–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ [\\mu,L] –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –≤ —Ç–æ—á–∫–µ a=\\mu. –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â—É—é –≤–µ—Ä—Ö–Ω—é—é –≥—Ä–∞–Ω–∏—Ü—É: \n\\|P_k(A)\\|_2 \\le P_k(\\mu) = T_k\\left(\\frac{L+\\mu-2\\mu}{L-\\mu}\\right) \\cdot T_k\\left(\\frac{L+\\mu}{L-\\mu}\\right)^{-1} = T_k\\left(1\\right) \\cdot T_k\\left(\\frac{L+\\mu}{L-\\mu}\\right)^{-1} = T_k\\left(\\frac{L+\\mu}{L-\\mu}\\right)^{-1}\n\n–ò—Å–ø–æ–ª—å–∑—É—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∏—Å–ª–∞ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–Ω–æ—Å—Ç–∏ \\varkappa = \\frac{L}{\\mu}, –º—ã –ø–æ–ª—É—á–∞–µ–º: \n\\|P_k(A)\\|_2 \\le T_k\\left(\\frac{\\varkappa+1}{\\varkappa-1}\\right)^{-1} = T_k\\left(1 + \\frac{2}{\\varkappa-1}\\right)^{-1} = T_k\\left(1 + \\epsilon\\right)^{-1}, \\quad \\epsilon = \\frac{2}{\\varkappa-1}.\n\n–°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –Ω–∞–º –Ω—É–∂–Ω–æ —Ç–æ–ª—å–∫–æ –ø–æ–Ω—è—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ T_k –≤ 1+\\epsilon. –≠—Ç–æ —Ç–æ, –æ—Ç–∫—É–¥–∞ –±–µ—Ä–µ—Ç—Å—è —É—Å–∫–æ—Ä–µ–Ω–∏–µ. –ú—ã –±—É–¥–µ–º –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—Ç—å —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ —Å–≤–µ—Ä—Ö—É –≤–µ–ª–∏—á–∏–Ω–æ–π \\mathcal{O}\\left(\\frac{1}{\\sqrt{\\epsilon}}\\right).\n–ß—Ç–æ–±—ã –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å |P_k| —Å–≤–µ—Ä—Ö—É, –Ω–∞–º –Ω—É–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å |T_k(1 + \\epsilon)| —Å–Ω–∏–∑—É.\n–î–ª—è –ª—é–±–æ–≥–æ x\\ge 1, –ø–æ–ª–∏–Ω–æ–º –ß–µ–±—ã—à—ë–≤–∞ –ø–µ—Ä–≤–æ–≥–æ —Ä–æ–¥–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–ø–∏—Å–∞–Ω –∫–∞–∫ \n\\begin{aligned}\nT_k(x)&=\\cosh\\left(k\\,\\mathrm{arccosh}(x)\\right)\\\\\nT_k(1+\\epsilon)&=\\cosh\\left(k\\,\\mathrm{arccosh}(1+\\epsilon)\\right).\n\\end{aligned}\n\n–î–ª—è –ª—é–±–æ–≥–æ x\\ge 1, –ø–æ–ª–∏–Ω–æ–º –ß–µ–±—ã—à—ë–≤–∞ –ø–µ—Ä–≤–æ–≥–æ —Ä–æ–¥–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–ø–∏—Å–∞–Ω –∫–∞–∫ \n\\begin{aligned}\nT_k(x)&=\\cosh\\left(k\\,\\mathrm{arccosh}(x)\\right)\\\\\nT_k(1+\\epsilon)&=\\cosh\\left(k\\,\\mathrm{arccosh}(1+\\epsilon)\\right).\n\\end{aligned}\n\n–ü–æ–º–Ω–∏–º, —á—Ç–æ: \n\\cosh(x)=\\frac{e^x+e^{-x}}{2} \\quad \\mathrm{arccosh}(x) = \\ln(x + \\sqrt{x^2-1}).\n\n–ü—É—Å—Ç—å \\phi=\\mathrm{arccosh}(1+\\epsilon), \ne^{\\phi}=1+\\epsilon + \\sqrt{2\\epsilon+\\epsilon^2} \\geq 1+\\sqrt{\\epsilon}.\n\n–°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, \n\\begin{aligned}\nT_k(1+\\epsilon)&=\\cosh\\left(k\\,\\mathrm{arccosh}(1+\\epsilon)\\right) \\\\\n&= \\cosh\\left(k\\phi\\right) \\\\\n&= \\frac{e^{k\\phi} + e^{-k\\phi}}{2} \\geq\\frac{e^{k\\phi}}{2} \\\\\n&= \\frac{\\left(1+\\sqrt{\\epsilon}\\right)^k}{2}.\n\\end{aligned}\n\n–ù–∞–∫–æ–Ω–µ—Ü, –º—ã –ø–æ–ª—É—á–∞–µ–º: \n\\begin{aligned}\n\\|e_k\\| &\\leq \\|P_k(A)\\| \\|e_0\\| \\leq \\frac{2}{\\left(1 + \\sqrt{\\epsilon}\\right)^k} \\|e_0\\| \\\\\n&\\leq 2 \\left(1 + \\sqrt{\\frac{2}{\\varkappa-1}}\\right)^{-k} \\|e_0\\| \\\\\n&\\leq 2 \\exp\\left( - \\sqrt{\\frac{2}{\\varkappa-1}} k\\right) \\|e_0\\|\n\\end{aligned}\n\n–ò–∑-–∑–∞ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–ª–∏–Ω–æ–º–æ–≤ –ß–µ–±—ã—à—ë–≤–∞, –º—ã –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –ø–æ–ª—É—á–∞–µ–º –∏—Ç–µ—Ä–∞—Ü–∏–æ–Ω–Ω—É—é —Å—Ö–µ–º—É —É—Å–∫–æ—Ä–µ–Ω–∏—è. –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É—è —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö –Ω–∞—à–∏—Ö –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–ª–∏–Ω–æ–º–æ–≤ –ß–µ–±—ã—à—ë–≤–∞, –º—ã –ø–æ–ª—É—á–∞–µ–º: \nT_{k+1}(x) =2xT_{k}(x)-T_{k-1}(x)\n –ü–æ—Å–∫–æ–ª—å–∫—É x = \\frac{L+\\mu-2a}{L-\\mu}, –∏: \n\\begin{aligned}\nP_k(a) &= T_k\\left(\\frac{L+\\mu-2a}{L-\\mu}\\right) T_k\\left(\\frac{L+\\mu}{L-\\mu}\\right)^{-1}\\\\\nT_k\\left(\\frac{L+\\mu-2a}{L-\\mu}\\right) &= P_k(a) T_k\\left(\\frac{L+\\mu}{L-\\mu}\\right)\n\\end{aligned}\n \n\\begin{aligned}\nT_{k-1}\\left(\\frac{L+\\mu-2a}{L-\\mu}\\right) &= P_{k-1}(a) T_{k-1}\\left(\\frac{L+\\mu}{L-\\mu}\\right) \\\\\nT_{k+1}\\left(\\frac{L+\\mu-2a}{L-\\mu}\\right) &= P_{k+1}(a) T_{k+1}\\left(\\frac{L+\\mu}{L-\\mu}\\right)\n\\end{aligned}\n \n\\begin{aligned}\nP_{k+1}(a) t_{k+1} &= 2 \\frac{L+\\mu-2a}{L-\\mu} P_{k}(a) t_{k} - P_{k-1}(a) t_{k-1} \\text{, where } t_{k} = T_{k}\\left(\\frac{L+\\mu}{L-\\mu}\\right) \\\\\nP_{k+1}(a) &= 2 \\frac{L+\\mu-2a}{L-\\mu} P_{k}(a) \\frac{t_{k}}{t_{k+1}} - P_{k-1}(a) \\frac{t_{k-1}}{t_{k+1}}\n\\end{aligned}\n\n–ü–æ—Å–∫–æ–ª—å–∫—É –º—ã –∏–º–µ–µ–º P_{k+1}(0) = P_{k}(0) = P_{k-1}(0) = 1, –º—ã –º–æ–∂–µ–º –∑–∞–ø–∏—Å–∞—Ç—å –º–µ—Ç–æ–¥ –≤ —Å–ª–µ–¥—É—é—â–µ–π —Ñ–æ—Ä–º–µ: \nP_{k+1}(a) = (1 - \\alpha_k a) P_k(a) + \\beta_k \\left(P_{k}(a) - P_{k-1}(a) \\right).\n\n–ü–µ—Ä–µ–≥—Ä—É–ø–ø–∏—Ä—É—è —á–ª–µ–Ω—ã, –º—ã –ø–æ–ª—É—á–∞–µ–º: \n\\begin{aligned}\nP_{k+1}(a) &= (1 + \\beta_k) P_k(a) - \\alpha_k a P_k(a) - \\beta_k P_{k-1}(a),\\\\\nP_{k+1}(a) &= 2 \\frac{L+\\mu}{L-\\mu}  \\frac{t_{k}}{t_{k+1}} P_{k}(a) - \\frac{4a}{L-\\mu}  \\frac{t_{k}}{t_{k+1}}P_{k}(a) - \\frac{t_{k-1}}{t_{k+1}} P_{k-1}(a)\n\\end{aligned}\n \n\\begin{cases}\n\\beta_k = \\dfrac{t_{k-1}}{t_{k+1}}, \\\\[6pt]\n\\alpha_k = \\dfrac{4}{L-\\mu} \\dfrac{t_k}{t_{k+1}}, \\\\[6pt]\n1 + \\beta_k = 2 \\dfrac{L + \\mu}{L - \\mu} \\dfrac{t_k}{t_{k+1}}\n\\end{cases}\n\n–ú—ã –ø–æ—á—Ç–∏ –∑–∞–∫–æ–Ω—á–∏–ª–∏ . –ü–æ–º–Ω–∏–º, —á—Ç–æ e_{k+1} = P_{k+1}(A) e_0. –¢–∞–∫–∂–µ –æ—Ç–º–µ—Ç–∏–º, —á—Ç–æ –º—ã —Ä–∞–±–æ—Ç–∞–µ–º —Å –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π –∑–∞–¥–∞—á–µ–π, –ø–æ—ç—Ç–æ–º—É –º—ã –º–æ–∂–µ–º –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç—å x^* = 0 –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –æ–±—â–Ω–æ—Å—Ç–∏. –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ e_0 = x_0 –∏ e_{k+1} = x_{k+1}. \n\\begin{aligned}\nx_{k+1} &= P_{k+1}(A) x_0 =  (I - \\alpha_k A) P_k(A) x_0 + \\beta_k \\left(P_{k}(A) - P_{k-1}(A) \\right) x_0 \\\\\n&= (I - \\alpha_k A) x_k + \\beta_k \\left(x_k - x_{k-1}\\right)\n\\end{aligned}\n\n–î–ª—è –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π –∑–∞–¥–∞—á–∏ –º—ã –∏–º–µ–µ–º \\nabla f(x_k) = A x_k, –ø–æ—ç—Ç–æ–º—É –º—ã –º–æ–∂–µ–º –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞–∫: \n\\boxed{\nx_{k+1} = x_k - \\alpha_k \\nabla f(x_k) + \\beta_k \\left(x_k - x_{k-1}\\right)\n}\n\n\n\n–¢–µ–æ—Ä–µ–º–∞ –æ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞ —Ç—è–∂–µ–ª–æ–≥–æ —à–∞—Ä–∏–∫–∞ –¥–ª—è —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–æ–π –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π –∑–∞–¥–∞—á–∏.\n\n\n\n\n\n\n–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∑–∞–¥–∞—á–∞ \nf(x) \\rightarrow \\min_{x \\in \\mathbb{R}^d}\n \nf(x) = \\frac{1}{2}x^TAx - b^Tx + c, \\quad A \\in \\mathbb{S}_{++} \\Rightarrow \\nabla f(x) = Ax - b \\overset{\\nabla f(x^*) = 0}{\\Rightarrow} Ax^* = b.\n –ù–µ —É–º–∞–ª—è—è –æ–±—â–Ω–æ—Å—Ç–∏, c = 0, —Ç–∞–∫ –∫–∞–∫ —Ä–µ—à–µ–Ω–∏–µ –æ—Ç –Ω–µ–≥–æ –Ω–µ –∑–∞–≤–∏—Å–∏—Ç.\n–ú–µ—Ç–æ–¥ —Ç—è–∂–µ–ª–æ–≥–æ —à–∞—Ä–∏–∫–∞ –∏–º–µ–µ—Ç –≤–∏–¥: \nx_{k+1} = x_k - \\alpha \\nabla f(x_k) + \\beta (x_k - x_{k-1})\n –¢–æ–≥–¥–∞ —Å–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ (\\rho) –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —à–∞–≥–∞ (–ø—Ä–∏ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è—Ö), \\rho \\sim \\sqrt{\\beta^*}, –≥–¥–µ \\beta^* - –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è \n\\|x_k-x^*\\|\\leqslant\\left(\\dfrac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^k\\|x_0 - x^*\\|.\n\n\n\n\n\n–î–ª—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–æ–π –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –º—ã –º–æ–∂–µ–º –≤–≤–µ—Å—Ç–∏ —Ç–∞–∫–∏–µ –Ω–æ–≤—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã, —á—Ç–æ–±—ã –º–∞—Ç—Ä–∏—Ü–∞ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ —Å—Ç–∞–ª–∞ –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ–π. –î–ª—è —ç—Ç–æ–≥–æ –æ–±—Ä–∞—Ç–∏–º—Å—è –∫ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–º—É —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—é –º–∞—Ç—Ä–∏—Ü—ã A = Q \\Lambda Q^T. –ü—É—Å—Ç—å \\hat{x} = Q^T(x - x^*), –≥–¥–µ x^* - —Ç–æ—á–∫–∞ –º–∏–Ω–∏–º—É–º–∞ –∏—Å—Ö–æ–¥–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏, –æ–ø—Ä–µ–¥–µ–ª—è–µ–º–∞—è –∫–∞–∫ Ax^* = b. –ü—Ä–∏ —ç—Ç–æ–º x = Q\\hat{x} + x^*. \n\\begin{split}\nf(\\hat{x}) &= \\frac12  (Q\\hat{x} + x^*)^\\top  A (Q\\hat{x} + x^*) - b^\\top  (Q\\hat{x} + x^*) \\\\\n&= \\frac12 \\hat{x}^T Q^TAQ\\hat{x} + \\frac12 (x^*)^T A (x^*) + (x^*)^TAQ\\hat{x} - b^T Q\\hat{x} - b^T x^*\\\\\n&= \\frac12 \\hat{x}^T \\Lambda \\hat{x} + \\frac12 (x^*)^T A (x^*) + (x^*)^TAQ\\hat{x} - (x^*)^T A^TQ\\hat{x} - (x^*)^T A x^*\\\\\n&= \\frac12 \\hat{x}^T \\Lambda \\hat{x} \\underbrace{- \\frac12 (x^*)^T A x^*}_{f(x^*)} \\simeq \\frac12 \\hat{x}^T \\Lambda \\hat{x}\n\\end{split}\n –í –ø–æ—Å–ª–µ–¥–Ω–µ–º –ø–µ—Ä–µ—Ö–æ–¥–µ –º—ã –æ—Ç–±—Ä–æ—Å–∏–ª–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–æ–µ —Å–ª–∞–≥–∞–µ–º–æ–µ, –∫–æ—Ç–æ—Ä–æ–µ –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞–≤–Ω–æ f(x^*). –û–Ω–æ –Ω—É–∂–Ω–æ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –∑–Ω–∞—á–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –≤ –Ω–æ–≤—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö –±—ã–ª–∏ —Ä–∞–≤–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è–º —Ñ—É–Ω–∫—Ü–∏–∏ –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö, –Ω–æ –≤–ª–∏—è–Ω–∏—è –Ω–∞ –º–µ—Ç–æ–¥ –æ–Ω–æ –Ω–µ –æ–∫–∞–∑—ã–≤–∞–µ—Ç.\n\n\n\n\n\n–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –º–µ—Ç–æ–¥ —Ç—è–∂–µ–ª–æ–≥–æ —à–∞—Ä–∏–∫–∞: \nx^{k+1} = x^k - \\alpha \\nabla f(x^k) + \\beta (x^k - x^{k-1}).\n –í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ (–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π –∑–∞–¥–∞—á–∏) –≤ –Ω–æ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏–º–µ–µ—Ç –≤–∏–¥: \n\\hat{x}_{k+1} = \\hat{x}_k - \\alpha \\Lambda \\hat{x}_k + \\beta (\\hat{x}_k - \\hat{x}_{k-1}) = (I - \\alpha \\Lambda + \\beta I) \\hat{x}_k - \\beta \\hat{x}_{k-1}\n –≠—Ç–æ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å –∫–∞–∫: \n\\begin{split}\n&\\hat{x}_{k+1} = (I - \\alpha \\Lambda + \\beta I) \\hat{x}_k - \\beta \\hat{x}_{k-1}, \\\\\n&\\hat{x}_{k} = \\hat{x}_k.\n\\end{split}\n –†–∞—Å—Å–º–æ—Ç—Ä–∏–º —Å–ª–µ–¥—É—é—â–∏–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è: \n\\hat{z}_k = \\begin{bmatrix}\n\\hat{x}_{k+1} \\\\\n\\hat{x}_{k}\n\\end{bmatrix}\n –¢–æ–≥–¥–∞ \\hat{z}_{k+1} = M \\hat{z}_k, –≥–¥–µ –º–∞—Ç—Ä–∏—Ü–∞ M –∏–º–µ–µ—Ç –≤–∏–¥: \nM = \\begin{bmatrix}\nI - \\alpha \\Lambda + \\beta I & - \\beta I \\\\\nI & 0_{d}\n\\end{bmatrix}.\n –ó–∞–º–µ—Ç–∏–º, —á—Ç–æ M —è–≤–ª—è–µ—Ç—Å—è –º–∞—Ç—Ä–∏—Ü–µ–π —Ä–∞–∑–º–µ—Ä–∞ 2d \\times 2d —Å 4 –±–ª–æ—á–Ω–æ-–¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—ã–º–∏ –º–∞—Ç—Ä–∏—Ü–∞–º–∏ —Ä–∞–∑–º–µ—Ä–∞ d \\times d –≤–Ω—É—Ç—Ä–∏. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º—ã –º–æ–∂–µ–º –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏—Ç—å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã, —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å M –±–ª–æ—á–Ω–æ-–¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ–π –≤ —Å–ª–µ–¥—É—é—â–µ–º –≤–∏–¥–µ. –ó–∞–º–µ—Ç–∏–º, —á—Ç–æ –≤ —É—Ä–∞–≤–Ω–µ–Ω–∏–∏ –Ω–∏–∂–µ –º–∞—Ç—Ä–∏—Ü–∞ M –æ–±–æ–∑–Ω–∞—á–∞–µ—Ç —Ç–æ –∂–µ —Å–∞–º–æ–µ, —á—Ç–æ –∏ –≤ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è—Ö –≤—ã—à–µ, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º –æ–ø–∏—Å–∞–Ω–Ω–æ–π –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∏ —Å—Ç—Ä–æ–∫ –∏ —Å—Ç–æ–ª–±—Ü–æ–≤. –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç—Ç—É –Ω–µ–±–æ–ª—å—à—É—é –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç—å –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏.  \n\\begin{aligned}\n\\begin{bmatrix}\n\\hat{x}_{k}^{(1)} \\\\\n\\vdots \\\\\n\\hat{x}_{k}^{(d)} \\\\\n\\addlinespace\n\\hat{x}_{k-1}^{(1)} \\\\\n\\vdots \\\\\n\\hat{x}_{k-1}^{(d)}\n\\end{bmatrix} \\to\n\\begin{bmatrix}\n\\hat{x}_{k}^{(1)} \\\\\n\\addlinespace\n\\hat{x}_{k-1}^{(1)} \\\\\n\\vdots \\\\\n\\hat{x}_{k}^{(d)} \\\\\n\\addlinespace\n\\hat{x}_{k-1}^{(d)}\n\\end{bmatrix} \\quad M = \\begin{bmatrix}\nM_1\\\\\n&M_2\\\\\n&&\\ldots\\\\\n&&&M_d\n\\end{bmatrix}\n\\end{aligned}\n –≥–¥–µ \\hat{x}_{k}^{(i)} —è–≤–ª—è–µ—Ç—Å—è i-–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ–π –≤–µ–∫—Ç–æ—Ä–∞ \\hat{x}_{k} \\in \\mathbb{R}^d, –∞ M_i –æ–±–æ–∑–Ω–∞—á–∞–µ—Ç 2 \\times 2 –º–∞—Ç—Ä–∏—Ü—É. –≠—Ç–∞ –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–º –∏–∑—É—á–∞—Ç—å –¥–∏–Ω–∞–º–∏–∫—É –º–µ—Ç–æ–¥–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã. –ú–æ–∂–Ω–æ –∑–∞–º–µ—Ç–∏—Ç—å, —á—Ç–æ –∞—Å–∏–º–ø—Ç–æ—Ç–∏—á–µ—Å–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ 2d-–º–µ—Ä–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤ \\hat{z}_k –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –Ω–∞–∏—Ö—É–¥—à–µ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å—Ä–µ–¥–∏ –µ–≥–æ –±–ª–æ–∫–æ–≤ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–∑—É—á–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –≤ –æ–¥–Ω–æ–º–µ—Ä–Ω–æ–º —Å–ª—É—á–∞–µ.\n–î–ª—è i-–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Å \\lambda_i –∫–∞–∫ i-–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –º–∞—Ç—Ä–∏—Ü—ã \nM_i = \\begin{bmatrix}\n1 - \\alpha \\lambda_i + \\beta & -\\beta \\\\\n1 & 0\n\\end{bmatrix}.\n –ú–µ—Ç–æ–¥ –±—É–¥–µ—Ç —Å—Ö–æ–¥–∏—Ç—å—Å—è, –µ—Å–ª–∏ \\rho(M) &lt; 1, –∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç—Å—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–µ–π —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–∞–¥–∏—É—Å–∞ \n\\alpha^*, \\beta^* = \\arg \\min_{\\alpha, \\beta} \\max_{i} \\rho(M_i)\n –ü–æ–∫–∞–∂–µ–º, —á—Ç–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ä–∞–≤–Ω—ã: \n\\alpha^* = \\dfrac{4}{(\\sqrt{L} + \\sqrt{\\mu})^2} \\qquad \\beta^* = \\left(\\dfrac{\\sqrt{L} - \\sqrt{\\mu}}{\\sqrt{L} + \\sqrt{\\mu}}\\right)^2.\n –ú–æ–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å, —á—Ç–æ –¥–ª—è —Ç–∞–∫–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–∞—Ç—Ä–∏—Ü–∞ M –∏–º–µ–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ-—Å–æ–ø—Ä—è–∂–µ–Ω–Ω—É—é –ø–∞—Ä—É —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π, –ø–æ—ç—Ç–æ–º—É —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –æ–ø—Ç–∏–º—É–º–∞ (–≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ, \\Vert z_k \\Vert) –≤ –æ–±—â–µ–º —Å–ª—É—á–∞–µ –Ω–µ –±—É–¥–µ—Ç –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ —É–º–µ–Ω—å—à–∞—Ç—å—Å—è.\n–ú–æ–∂–Ω–æ —è–≤–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è M_i: \n\\lambda^{M_i}_1, \\lambda^{M_i}_2 = \\lambda \\left( \\begin{bmatrix}\n1 - \\alpha \\lambda_i + \\beta & -\\beta \\\\\n1 & 0\n\\end{bmatrix}\\right) = \\dfrac{1+\\beta - \\alpha \\lambda_i \\pm \\sqrt{(1+\\beta - \\alpha\\lambda_i)^2 - 4\\beta}}{2}.\n –ö–æ–≥–¥–∞ \\alpha –∏ \\beta –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã (\\alpha^*, \\beta^*), —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —è–≤–ª—è—é—Ç—Å—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ-—Å–æ–ø—Ä—è–∂–µ–Ω–Ω–æ–π –ø–∞—Ä–æ–π (1+\\beta - \\alpha\\lambda_i)^2 - 4\\beta \\leq 0, —Ç.–µ. \\beta \\geq (1 - \\sqrt{\\alpha \\lambda_i})^2. \n\\text{Re}(\\lambda^M) = \\dfrac{L + \\mu - 2\\lambda_i}{(\\sqrt{L} + \\sqrt{\\mu})^2}; \\quad \\text{Im}(\\lambda^M) = \\dfrac{\\pm 2\\sqrt{(L - \\lambda_i)(\\lambda_i - \\mu)}}{(\\sqrt{L} + \\sqrt{\\mu})^2}; \\quad \\vert \\lambda^M \\vert = \\dfrac{L - \\mu}{(\\sqrt{L} + \\sqrt{\\mu})^2}.\n –°–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —à–∞–≥–∞ –∏ —Ä–∞–≤–Ω–∞ \\sqrt{\\beta^*}.\n\n\n–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∏ –≤—ã–≤–æ–¥ —Ñ–æ—Ä–º—É–ª –º–µ—Ç–æ–¥–∞ (–í —ç—Ç–æ–º –≤–æ–ø—Ä–æ—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–æ–∫–∞–∑–∞—Ç—å –∑–∞ –∫–∞–∫–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ —Å—Ö–æ–¥–∏—Ç—Å—è –º–µ—Ç–æ–¥, –∫–∞–∫ –≤—ã–±–∏—Ä–∞—é—Ç—Å—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ –ø–æ—á–µ–º—É –≤ A-–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –ª–∏—à—å –ø—Ä–µ–¥—ã–¥—É—â–∏–π —à–∞–≥ –º–µ—Ç–æ–¥–∞, –∞ –Ω–µ –≤—Å–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ).\n\n\n\n\n\n\n\n\\begin{aligned}\n& \\mathbf{r}_0 := \\mathbf{b} - \\mathbf{A x}_0 \\\\\n& \\hbox{–ï—Å–ª–∏ } \\mathbf{r}_{0} \\text{ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–∞–ª–∞, —Ç–æ –≤–µ—Ä–Ω—É—Ç—å } \\mathbf{x}_{0} \\text{ –∫–∞–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç}\\\\\n& \\mathbf{d}_0 := \\mathbf{r}_0 \\\\\n& k := 0 \\\\\n& \\text{–ø–æ–≤—Ç–æ—Ä–∏—Ç—å} \\\\\n& \\qquad \\alpha_k := \\frac{\\mathbf{r}_k^\\mathsf{T} \\mathbf{r}_k}{\\mathbf{d}_k^\\mathsf{T} \\mathbf{A d}_k}  \\\\\n& \\qquad \\mathbf{x}_{k+1} := \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k \\\\\n& \\qquad \\mathbf{r}_{k+1} := \\mathbf{r}_k - \\alpha_k \\mathbf{A d}_k \\\\\n& \\qquad \\hbox{–ï—Å–ª–∏ } \\mathbf{r}_{k+1} \\text{ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–∞–ª–∞, —Ç–æ –≤—ã–π—Ç–∏ –∏–∑ —Ü–∏–∫–ª–∞} \\\\\n& \\qquad \\beta_k := \\frac{\\mathbf{r}_{k+1}^\\mathsf{T} \\mathbf{r}_{k+1}}{\\mathbf{r}_k^\\mathsf{T} \\mathbf{r}_k} \\\\\n& \\qquad \\mathbf{d}_{k+1} := \\mathbf{r}_{k+1} + \\beta_k \\mathbf{d}_k \\\\\n& \\qquad k := k + 1 \\\\\n& \\text{–∫–æ–Ω–µ—Ü –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è} \\\\\n& \\text{–≤–µ—Ä–Ω—É—Ç—å } \\mathbf{x}_{k+1} \\text{ –∫–∞–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç}\n\\end{aligned}\n\n\n\n\n–†–∞—Å—Å–º–æ—Ç—Ä–∏–º —Å–ª–µ–¥—É—é—â—É—é –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—É—é –∑–∞–¥–∞—á—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: \n\\min\\limits_{x \\in \\mathbb{R}^n} f(x) =  \\min\\limits_{x \\in \\mathbb{R}^n} \\dfrac{1}{2} x^\\top  A x - b^\\top  x + c, \\text{ –≥–¥–µ }A \\in \\mathbb{S}^n_{++}.\n\\tag{5}\n–ë–µ—Ä—ë–º –≤ –∞—Ä—Å–µ–Ω–∞–ª –ø—Ä–æ—Ü–µ—Å—Å –ì—Ä–∞–º-–®–º–∏–¥—Ç–∞: \nd_k = u_k + \\sum\\limits_{i=0}^{k-1}\\beta_{ik} d_i \\qquad \\beta_{ik} = - \\dfrac{\\langle d_i, u_k \\rangle}{\\langle d_i, d_i \\rangle}\n\\tag{6}\n–õ–µ–º–º–∞ 1. –õ–∏–Ω–µ–π–Ω–∞—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å A-–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤.\n–ï—Å–ª–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ d_1, \\ldots, d_n - –ø–æ–ø–∞—Ä–Ω–æ A-–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã (–∫–∞–∂–¥–∞—è –ø–∞—Ä–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ A-–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–∞), —Ç–æ —ç—Ç–∏ –≤–µ–∫—Ç–æ—Ä—ã –ª–∏–Ω–µ–π–Ω–æ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã. A \\in \\mathbb{S}^n_{++}.\n–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ\n–ü–æ–∫–∞–∂–µ–º, —á—Ç–æ –µ—Å–ª–∏ \\sum\\limits_{i=1}^n\\alpha_i d_i = 0, —Ç–æ –≤—Å–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–∞–≤–Ω—ã –Ω—É–ª—é: \n\\begin{aligned}\n0 &= \\sum\\limits_{i=1}^n\\alpha_i d_i \\\\\n\\text{–£–º–Ω–æ–∂–∞–µ–º –Ω–∞ } d_j^T A \\cdot \\qquad &= d_j^\\top A \\left( \\sum\\limits_{i=1}^n\\alpha_i d_i \\right)\n=  \\sum\\limits_{i=1}^n \\alpha_i d_j^\\top A d_i  \\\\\n&=  \\alpha_j d_j^\\top A d_j  + 0 + \\ldots + 0\n\\end{aligned}\n –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, \\alpha_j = 0, –¥–ª—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –Ω—É–∂–Ω–æ –ø—Ä–æ–¥–µ–ª–∞—Ç—å —Ç–æ—Ç –∂–µ –ø—Ä–æ—Ü–µ—Å—Å\n–í–≤–µ–¥–µ–º —Å–ª–µ–¥—É—é—â–∏–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è:\n\nr_k = b - Ax_k - –Ω–µ–≤—è–∑–∫–∞\ne_k = x_k - x^* - –æ—à–∏–±–∫–∞\n–ü–æ—Å–∫–æ–ª—å–∫—É Ax^* = b, –∏–º–µ–µ–º r_k = b - Ax_k = Ax^* - Ax_k = -A (x_k - x^*) \n  r_k = -Ae_k.\n   \\tag{7}\n–¢–∞–∫–∂–µ –∑–∞–º–µ—Ç–∏–º, —á—Ç–æ –ø–æ—Å–∫–æ–ª—å–∫—É x_{k+1} = x_0 + \\sum\\limits_{i=1}^k\\alpha_i d_i, –∏–º–µ–µ–º \n  e_{k+1} = e_0 + \\sum\\limits_{i=1}^k\\alpha_i d_i.\n   \\tag{8}\n\n–õ–µ–º–º–∞ 2. –°—Ö–æ–¥–∏–º–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π.\n–ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, –º—ã —Ä–µ—à–∞–µ–º n-–º–µ—Ä–Ω—É—é –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—É—é —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—É—é –∑–∞–¥–∞—á—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (5). –ú–µ—Ç–æ–¥ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π \n    x_{k+1} = x_0 + \\sum\\limits_{i=0}^k\\alpha_i d_i\n     —Å \\alpha_i = \\frac{\\langle d_i, r_i \\rangle}{\\langle d_i, Ad_i \\rangle} –≤–∑—è—Ç—ã–º –∏–∑ —Ç–æ—á–Ω–æ–≥–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, —Å—Ö–æ–¥–∏—Ç—Å—è –∑–∞ –Ω–µ –±–æ–ª–µ–µ n —à–∞–≥–æ–≤ –∞–ª–≥–æ—Ä–∏—Ç–º–∞.\n–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ\n\n–ù—É–∂–Ω–æ –¥–æ–∫–∞–∑–∞—Ç—å, —á—Ç–æ \\delta_i = - \\alpha_i: \ne_0 = x_0 - x^* =  \\sum\\limits_{i=0}^{n-1}\\delta_i d_i\n\n–£–º–Ω–æ–∂–∞–µ–º –æ–±–µ —á–∞—Å—Ç–∏ —Å–ª–µ–≤–∞ –Ω–∞ d_k^T A: \n\\begin{aligned}\nd_k^T Ae_0 &= \\sum\\limits_{i=0}^{n-1}\\delta_i d_k^T A d_i = \\delta_k d_k^T A d_k \\\\\nd_k^T A\\left(e_0 + \\sum\\limits_{i=0}^{k-1}\\alpha_i d_i \\right) = d_k^T A e_k &= \\delta_k d_k^T A d_k \\quad \\left(A-\\text{ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å}\\right)\\\\\n\\delta_k = \\frac{ d_k^T A e_k}{d_k^T A d_k } &= -\\frac{ d_k^T r_k}{d_k^T A d_k } \\Leftrightarrow \\delta_k = - \\alpha_k\n\\end{aligned}\n\n\n–õ–µ–º–º–∞ 3. –†–∞–∑–ª–æ–∂–µ–Ω–∏–µ –æ—à–∏–±–∫–∏.\n\ne_i = \\sum\\limits_{j=i}^{n-1}-\\alpha_j d_j\n\\tag{9}\n–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ\n–ü–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é \ne_{i} = e_0 + \\sum\\limits_{j=0}^{i-1}\\alpha_j d_j = x_0 - x^* + \\sum\\limits_{j=0}^{i-1}\\alpha_j d_j = -\\sum\\limits_{j=0}^{n-1}\\alpha_j d_j + \\sum\\limits_{j=0}^{i-1}\\alpha_j d_j = \\sum\\limits_{j=i}^{n-1}-\\alpha_j d_j\n\n–õ–µ–º–º–∞ 4. –ù–µ–≤—è–∑–∫–∞ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–∞ –≤—Å–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º –¥–ª—è CD.\n–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –Ω–µ–≤—è–∑–∫—É –º–µ—Ç–æ–¥–∞ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –Ω–∞ k –∏—Ç–µ—Ä–∞—Ü–∏–∏ r_k, —Ç–æ–≥–¥–∞ –¥–ª—è –ª—é–±–æ–≥–æ i &lt; k: \nd_i^T r_k = 0\n\\tag{10}\n–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ\n–ó–∞–ø–∏—à–µ–º (9) –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ k: \ne_k = \\sum\\limits_{j=k}^{n-1}-\\alpha_j d_j\n –£–º–Ω–æ–∂–∞–µ–º –æ–±–µ —á–∞—Å—Ç–∏ –Ω–∞ -d_i^TA \\cdot \n-d_i^TA e_k = \\sum\\limits_{j=k}^{n-1}\\alpha_j d_i^TA d_j  = 0\n\n\n\n\n\n\n–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, d_i^T r_k = 0 –∏ –Ω–µ–≤—è–∑–∫–∞ r_k –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–∞ –≤—Å–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º d_i –¥–ª—è –º–µ—Ç–æ–¥–∞ CD.\n–ò–¥–µ—è –º–µ—Ç–æ–¥–∞ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (CG)\n\n–≠—Ç–æ –±—É–∫–≤–∞–ª—å–Ω–æ –º–µ—Ç–æ–¥ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π, –≤ –∫–æ—Ç–æ—Ä–æ–º –º—ã –≤—ã–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä d_0, \\ldots, d_{n-1}, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –ì—Ä–∞–º–∞-–®–º–∏–¥—Ç–∞.\n–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å –ì—Ä–∞–º–∞-–®–º–∏–¥—Ç–∞ —Å A-–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –≤–º–µ—Å—Ç–æ –ï–≤–∫–ª–∏–¥–æ–≤–æ–π –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –∏—Ö –∏–∑ –Ω–∞–±–æ—Ä–∞ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤.\n–ù–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ r_0, \\ldots, r_{n-1} –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –ì—Ä–∞–º–∞-–®–º–∏–¥—Ç–∞.\n–û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ CD –ø—Ä–æ—Ü–µ—Å—Å –ì—Ä–∞–º–∞-–®–º–∏–¥—Ç–∞ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ –¥–æ—Ä–æ–≥–æ–π –∏ —Ç—Ä–µ–±—É–µ—Ç –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–≥–æ —á–∏—Å–ª–∞ –æ–ø–µ—Ä–∞—Ü–∏–π —Å–ª–æ–∂–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏ —Å–∫–∞–ª—è—Ä–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π \\mathcal{O}\\left( n^2\\right), –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –≤ —Å–ª—É—á–∞–µ CG –º—ã –ø–æ–∫–∞–∂–µ–º, —á—Ç–æ —Å–ª–æ–∂–Ω–æ—Å—Ç—å —ç—Ç–æ–π –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –º–æ–∂–µ—Ç –±—ã—Ç—å —É–º–µ–Ω—å—à–µ–Ω–∞ –¥–æ –ª–∏–Ω–µ–π–Ω–æ–π \\mathcal{O}\\left( n\\right).\n\n\n\\text{CG} = \\text{CD} + r_0, \\ldots, r_{n-1} \\text{ –∫–∞–∫ –Ω–∞—á–∞–ª—å–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –ì—Ä–∞–º–∞-–®–º–∏–¥—Ç–∞} + A\\text{-–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å.}\n\n–õ–µ–º–º–∞ 5. –ù–µ–≤—è–∑–∫–∏ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã –¥—Ä—É–≥ –¥—Ä—É–≥—É –≤ –º–µ—Ç–æ–¥–µ CG\n–í—Å–µ –Ω–µ–≤—è–∑–∫–∏ –≤ –º–µ—Ç–æ–¥–µ CG –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã –¥—Ä—É–≥ –¥—Ä—É–≥—É: \nr_i^T r_k = 0 \\qquad \\forall i \\neq k\n\\tag{11}\n–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ\n–ó–∞–ø–∏—à–µ–º –ø—Ä–æ—Ü–µ—Å—Å –ì—Ä–∞–º–∞-–®–º–∏–¥—Ç–∞ (6) —Å \\langle \\cdot, \\cdot \\rangle –∑–∞–º–µ–Ω–µ–Ω–Ω—ã–º –Ω–∞ \\langle \\cdot, \\cdot \\rangle_A = x^T A y \nd_i = u_i + \\sum\\limits_{j=0}^{i-1}\\beta_{ji} d_j \\;\\; \\beta_{ji} = - \\dfrac{\\langle d_j, u_i \\rangle_A}{\\langle d_j, d_j \\rangle_A}\n\\tag{12}\n–¢–æ–≥–¥–∞, –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ–≤—è–∑–∫–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏ u_i = r_i.\n\nd_i = r_i + \\sum\\limits_{j=0}^{i-1}\\beta_{ji} d_j \\;\\; \\beta_{ji} = - \\dfrac{\\langle d_j, r_i \\rangle_A}{\\langle d_j, d_j \\rangle_A}\n\\tag{13}\n\n\n\n\n\n–£–º–Ω–æ–∂–∞–µ–º –æ–±–µ —á–∞—Å—Ç–∏ (12) –Ω–∞ r_k^T \\cdot –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ k: \nr_k^Td_i = r_k^Tu_i + \\sum\\limits_{j=0}^{i-1}\\beta_{ji} r_k^Td_j\n\n–ï—Å–ª–∏ j &lt; i &lt; k, —Ç–æ –∏–º–µ–µ–º –ª–µ–º–º—É 4 —Å d_i^T r_k = 0 –∏ d_j^T r_k = 0. –ò–º–µ–µ–º: \nr_k^Tu_i= 0 \\;\\text{ –¥–ª—è CD} \\;\\; r_k^Tr_i = 0 \\;\\text{ –¥–ª—è CG}\n –ë–æ–ª–µ–µ —Ç–æ–≥–æ, –µ—Å–ª–∏ k=i: \nr_k^Td_k = r_k^Tu_k + \\sum\\limits_{j=0}^{k-1}\\beta_{jk} r_k^Td_j = r_k^Tu_k + 0,\n –∏ –º—ã –∏–º–µ–µ–º –¥–ª—è –ª—é–±–æ–≥–æ k (–∏–∑-–∑–∞ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ i): \nr_k^Td_k = r_k^Tu_k.\n\\tag{14}\n–õ–µ–º–º–∞ 6. –ü–µ—Ä–µ—Å—á–µ—Ç –Ω–µ–≤—è–∑–∫–∏ \nr_{k+1} = r_k - \\alpha_k A d_k\n\\tag{15} \nr_{k+1} = -A e_{k+1} = -A \\left( e_{k} + \\alpha_k d_k \\right) = -A e_{k} - \\alpha_k A d_k = r_k - \\alpha_k A d_k\n –ù–∞–∫–æ–Ω–µ—Ü, –≤—Å–µ —ç—Ç–∏ –≤—ã—à–µ—É–∫–∞–∑–∞–Ω–Ω—ã–µ –ª–µ–º–º—ã –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã –¥–ª—è –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞, —á—Ç–æ \\beta_{ji} = 0 –¥–ª—è –≤—Å–µ—Ö i,j, –∫—Ä–æ–º–µ —Å–æ—Å–µ–¥–Ω–∏—Ö.\n–ì—Ä–∞–º-–®–º–∏–¥—Ç –≤ –º–µ—Ç–æ–¥–µ CG\n–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø—Ä–æ—Ü–µ—Å—Å –ì—Ä–∞–º-–®–º–∏–¥—Ç–∞ –≤ –º–µ—Ç–æ–¥–µ CG: \n\\beta_{ji} = - \\dfrac{\\langle d_j, u_i \\rangle_A}{\\langle d_j, d_j \\rangle_A} = - \\dfrac{ d_j^T A u_i }{ d_j^T A d_j } = - \\dfrac{ d_j^T A r_i }{ d_j^T A d_j } = - \\dfrac{r_i^T A d_j}{ d_j^T A d_j }.\n –†–∞—Å—Å–º–æ—Ç—Ä–∏–º —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ \\langle r_i, r_{j+1} \\rangle –∏—Å–ø–æ–ª—å–∑—É—è (15): \n\\begin{aligned}\n\\langle r_i, r_{j+1} \\rangle &= \\langle r_i, r_j - \\alpha_j A d_j  \\rangle = \\langle r_i, r_j \\rangle - \\alpha_j\\langle r_i, A d_j  \\rangle \\\\\n\\alpha_j\\langle r_i, A d_j  \\rangle &= \\langle r_i, r_j \\rangle - \\langle r_i, r_{j+1} \\rangle\n\\end{aligned}\n\n\n–ï—Å–ª–∏ i=j: \\alpha_i\\langle r_i, A d_i  \\rangle = \\langle r_i, r_i \\rangle - \\langle r_i, r_{i+1} \\rangle = \\langle r_i, r_i \\rangle. –≠—Ç–æ—Ç —Å–ª—É—á–∞–π –Ω–µ –∏–Ω—Ç–µ—Ä–µ—Å–µ–Ω –ø–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—é –ø—Ä–æ—Ü–µ—Å—Å–∞ –ì—Ä–∞–º-–®–º–∏–¥—Ç–∞.\n–°–æ—Å–µ–¥–Ω–∏–π —Å–ª—É—á–∞–π i=j + 1: \\alpha_j\\langle r_i, A d_j \\rangle = \\langle r_i, r_{i-1} \\rangle - \\langle r_i, r_{i} \\rangle = - \\langle r_i, r_i \\rangle\n–î–ª—è –ª—é–±–æ–≥–æ –¥—Ä—É–≥–æ–≥–æ —Å–ª—É—á–∞—è: \\alpha_j\\langle r_i, A d_j \\rangle = 0, –ø–æ—Ç–æ–º—É —á—Ç–æ –≤—Å–µ –Ω–µ–≤—è–∑–∫–∏ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã –¥—Ä—É–≥ –¥—Ä—É–≥—É.\n\n–ù–∞–∫–æ–Ω–µ—Ü, –º—ã –∏–º–µ–µ–º —Ñ–æ—Ä–º—É–ª—É –¥–ª—è i=j + 1: \n\\beta_{ji} = - \\dfrac{r_i^T A d_j}{ d_j^T A d_j} = \\dfrac{1}{\\alpha_j}\\dfrac{\\langle r_i, r_i \\rangle}{ d_j^T A d_j} =  \\dfrac{d_j^T A d_j}{d_j^T r_j}\\dfrac{\\langle r_i, r_i \\rangle}{ d_j^T A d_j} = \\dfrac{\\langle r_i, r_i \\rangle}{\\langle r_j, r_j \\rangle} = \\dfrac{\\langle r_i, r_i \\rangle}{\\langle r_{i-1}, r_{i-1} \\rangle}\n\n–ò –¥–ª—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è d_{k+1} = r_{k+1} + \\beta_{k,k+1} d_k, \\qquad  \\beta_{k,k+1} = \\beta_k = \\dfrac{\\langle r_{k+1}, r_{k+1} \\rangle}{\\langle r_{k}, r_{k} \\rangle}.\n–¢–µ–æ—Ä–µ–º–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞ –ù—å—é—Ç–æ–Ω–∞ –¥–ª—è —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π —Å –õ–∏–ø—à–∏—Ü–µ–≤—ã–º –≥–µ—Å—Å–∏–∞–Ω–æ–º.\n\n\n\n\n\n\n–ü—É—Å—Ç—å f(x) ‚Äî —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–∞—è –¥–≤–∞–∂–¥—ã –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞—è —Ñ—É–Ω–∫—Ü–∏—è –Ω–∞ \\mathbb{R}^n, –¥–ª—è –≤—Ç–æ—Ä–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–æ–π –∫–æ—Ç–æ—Ä–æ–π –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞: \\mu I_n\\preceq \\nabla^2 f(x) \\preceq L I_n. –ü—É—Å—Ç—å —Ç–∞–∫–∂–µ –≥–µ—Å—Å–∏–∞–Ω —Ñ—É–Ω–∫—Ü–∏–∏ M-–ª–∏–ø—à–∏—Ü–µ–≤. –¢–æ–≥–¥–∞ –º–µ—Ç–æ–¥ –ù—å—é—Ç–æ–Ω–∞ —Å—Ö–æ–¥–∏—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–æ –∫ —Ä–µ—à–µ–Ω–∏—é —Å –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é, —Ç.–µ. –ø—Ä–∏ \\| x_0 - x^* \\| &lt; \\frac{2 \\mu}{3M}: \n\\|x_{k+1} - x^*\\| \\leq \\frac{3 M}{2\\mu} \\|x_k - x^*\\|^2\n\n\n\n\n–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ\n\n–ú—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ–æ—Ä–º—É–ª—É –ù—å—é—Ç–æ–Ω–∞-–õ–µ–π–±–Ω–∏—Ü–∞ \n\\nabla f(x_{k}) - \\nabla f(x^*) = \\int_0^1 \\nabla^2 f(x^* + \\tau (x_k - x^*))  (x_k - x^*) d\\tau\n\n–ú—ã –±—É–¥–µ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Ä–µ—à–µ–Ω–∏—è \n\\begin{aligned}\nx_{k+1} - x^* = x_k -\\left[ \\nabla^2 f(x_k)\\right]^{-1} \\nabla f(x_{k}) - x^* = x_k - x^* -\\left[ \\nabla^2 f(x_k)\\right]^{-1} \\nabla f(x_{k}) = \\\\\n= x_k - x^* - \\left[ \\nabla^2 f(x_k)\\right]^{-1}  \\int_0^1 \\nabla^2 f(x^* + \\tau (x_k - x^*))  (x_k - x^*) d\\tau\n\\end{aligned}\n\n\n\\begin{aligned}\n= \\left( I - \\left[ \\nabla^2 f(x_k)\\right]^{-1} \\int_0^1 \\nabla^2 f(x^* + \\tau (x_k - x^*)) d \\tau\\right) (x_k - x^*)= \\\\\n= \\left[ \\nabla^2 f(x_k)\\right]^{-1} \\left( \\nabla^2 f(x_k) - \\int_0^1 \\nabla^2 f(x^* + \\tau (x_k - x^*)) d \\tau\\right) (x_k - x^*) = \\\\\n= \\left[ \\nabla^2 f(x_k)\\right]^{-1} \\left( \\int_0^1 \\left( \\nabla^2 f(x_k) - \\nabla^2 f(x^* + \\tau (x_k - x^*)) d \\tau\\right)\\right) (x_k - x^*)= \\\\\n= \\left[ \\nabla^2 f(x_k)\\right]^{-1} G_k (x_k - x^*)\n\\end{aligned}\n\n–í–≤–µ–¥—ë–º: \nG_k = \\int_0^1 \\left( \\nabla^2 f(x_k) - \\nabla^2 f(x^* + \\tau (x_k - x^*)) d \\tau\\right).\n\n–ü–æ–ø—Ä–æ–±—É–µ–º –æ—Ü–µ–Ω–∏—Ç—å —Ä–∞–∑–º–µ—Ä G_k —Å –ø–æ–º–æ—â—å—é r_k = \\| x_k - x^* \\|: \n\\begin{aligned}\n\\| G_k\\| = \\left\\| \\int_0^1 \\left( \\nabla^2 f(x_k) - \\nabla^2 f(x^* + \\tau (x_k - x^*)) d \\tau\\right)\\right\\| \\leq \\\\\n\\leq \\int_0^1 \\left\\| \\nabla^2 f(x_k) - \\nabla^2 f(x^* + \\tau (x_k - x^*))   \\right\\|d\\tau \\leq \\qquad \\text{(–õ–∏–ø—à–∏—Ü–µ–≤–æ—Å—Ç—å –≥–µ—Å—Å–∏–∞–Ω–∞)}\\\\\n\\leq \\int_0^1 M\\|x_k - x^* - \\tau (x_k - x^*)\\| d \\tau = \\int_0^1 M\\|x_k - x^*\\|(1- \\tau)d \\tau = \\frac{r_k}{2}M,\n\\end{aligned}\n\n–ü–æ–ª—É—á–∞–µ–º: \nr_{k+1}  \\leq \\left\\|\\left[ \\nabla^2 f(x_k)\\right]^{-1}\\right\\| \\cdot \\frac{r_k}{2}M \\cdot r_k\n –∏ –Ω–∞–º –Ω—É–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å –Ω–æ—Ä–º—É –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –≥–µ—Å—Å–∏–∞–Ω–∞\n–ò–∑ –ª–∏–ø—à–∏—Ü–µ–≤–æ—Å—Ç–∏ –∏ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ—Å—Ç–∏ –≥–µ—Å—Å–∏–∞–Ω–∞: \n\\begin{aligned}\n\\nabla^2 f(x_k) - \\nabla^2 f(x^*) \\succeq - Mr_k I_n \\\\\n\\nabla^2 f(x_k) \\succeq \\nabla^2 f(x^*) - Mr_k I_n \\\\\n\\nabla^2 f(x_k) \\succeq \\mu I_n - Mr_k I_n \\\\\n\\nabla^2 f(x_k) \\succeq (\\mu- Mr_k )I_n \\\\\n\\end{aligned}\n\n–ò–∑ —Å–∏–ª—å–Ω–æ–π –≤—ã–ø—É–∫–ª–æ—Å—Ç–∏ —Å–ª–µ–¥—É–µ—Ç, —á—Ç–æ \\nabla^2 f(x_k) \\succ 0, i.e.¬†r_k &lt; \\frac{\\mu}{M}. \n\\begin{aligned}\n\\left\\|\\left[ \\nabla^2 f(x_k)\\right]^{-1}\\right\\| \\leq (\\mu - Mr_k)^{-1} \\\\\nr_{k+1}  \\leq \\dfrac{r_k^2 M}{2(\\mu - Mr_k)}\n\\end{aligned}\n\n–ü–æ—Ç—Ä–µ–±—É–µ–º, —á—Ç–æ–±—ã –≤–µ—Ä—Ö–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ r_{k+1} –±—ã–ª–∞ –º–µ–Ω—å—à–µ r_k, —É—á–∏—Ç—ã–≤–∞—è, —á—Ç–æ 0 &lt;r_k &lt; \\frac{\\mu}{M}: \n\\begin{aligned}\n\\dfrac{r_k^2 M}{2(\\mu - Mr_k)} &&lt; r_k \\\\\n\\frac{M}{2(\\mu - Mr_k)}\\, r_k &&lt; 1 \\\\\nM r_k &&lt; 2(\\mu - Mr_k) \\\\\n3 M r_k &&lt; 2\\mu \\\\\nr_k &&lt; \\frac{2\\mu}{3M}\n\\end{aligned}\n\n–í–æ–∑–≤—Ä–∞—â–∞—è—Å—å –∫ –æ—Ü–µ–Ω–∫–µ –Ω–µ–≤—è–∑–∫–∏ –Ω–∞ k+1-–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏, –ø–æ–ª—É—á–∞–µ–º: \nr_{k+1}  \\leq \\dfrac{r_k^2 M}{2(\\mu - Mr_k)} &lt; \\dfrac{3 M r_k^2 }{2\\mu}\n\n–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –º—ã –ø–æ–ª—É—á–∏–ª–∏ –≤–∞–∂–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –º–µ—Ç–æ–¥ –ù—å—é—Ç–æ–Ω–∞ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ —Å –ª–∏–ø—à–∏—Ü–µ–≤—ã–º –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º –≥–µ—Å—Å–∏–∞–Ω–æ–º —Å—Ö–æ–¥–∏—Ç—Å—è –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ –≤–±–ª–∏–∑–∏ —Ä–µ—à–µ–Ω–∏—è.\n\n–í—ã–≤–æ–¥ —Ñ–æ—Ä–º—É–ª –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –≥–µ—Å—Å–∏–∞–Ω–∞ –∏ –≥–µ—Å—Å–∏–∞–Ω–∞ –∫–≤–∞–∑–∏–Ω—å—é—Ç–æ–Ω–æ–≤—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤ SR-1, DFP, BFGS.\n x_{k+1} = x_k + \\alpha_k d_k, \\quad   B_k d_k = -\\nabla f(x_k)   B_k = \\nabla^2 f(x_k) \n–¢–æ –µ—Å—Ç—å –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –≤—ã—á–∏—Å–ª—è—Ç—å –≥–µ—Å—Å–∏–∞–Ω –∏ —Ä–µ—à–∞—Ç—å —Å–∏—Å—Ç–µ–º—É –ª–∏–Ω–µ–π–Ω—ã—Ö —É—Ä–∞–≤–Ω–µ–Ω–∏–π.\n–í –∫–≤–∞–∑–∏-–Ω—å—é—Ç–æ–Ω–æ–≤—Å–∫–∏—Ö –º–µ—Ç–æ–¥–∞—Ö –º—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü B_k, —Å—Ö–æ–¥—è—â–∏—Ö—Å—è –≤ –∫–∞–∫–æ–º-—Ç–æ —Å–º—ã—Å–ª–µ –∫ –Ω–∞—Å—Ç–æ—è—â–µ–º—É –∑–Ω–∞—á–µ–Ω–∏—é –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ì–µ—Å—Å–∏–∞–Ω–∞ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º –æ–ø—Ç–∏–º—É–º–µ: \\left[\\nabla^2 f(x^*)\\right]^{-1}.\n–û–±—â–∞—è —Å—Ö–µ–º–∞:\n\n–†–µ—à–∏—Ç—å B_{k} d_{k} = -\\nabla f(x_{k})\n–û–±–Ω–æ–≤–∏—Ç—å x_{k+1} = x_{k} + \\alpha_k d_{k} (—É—Ä–∞–≤–Ω–µ–Ω–∏—è —Å–µ–∫—É—â–∏—Ö)\n–í—ã—á–∏—Å–ª–∏—Ç—å B_{k+1} –∏–∑ B_{k}\n\n–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ B_{k+1} –∏–∑ —É—Ä-—è —Å–µ–∫—É—â–∏—Ö:\n\n\\begin{aligned}\n\\nabla f(x_{k+1}) - \\nabla f(x_{k}) &= B_{k+1} (x_{k+1} - x_k) =  B_{k+1} d_{k} \\\\\n\\Delta y_k &= B_{k+1} d_k\n\\end{aligned}\n –¢–∞–∫–∂–µ —Ç—Ä–µ–±—É–µ–º:\n\nB_{k+1} - —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞\nB_{k+1} ‚Äú–±–ª–∏–∑–∫–∞‚Äù –∫ B_k\nB_k \\succ 0 \\Rightarrow B_{k+1} \\succ 0\n\n\nSymmetric Rank-One (Broyden) Update\n–ü–æ—Ä–æ–±—É–µ–º —Ç–∞–∫–æ–π –≤–∏–¥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è:  B_{k+1} = B_k + a u u^T \n—É—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–µ–∫—É—â–∏—Ö B_{k+1} d_k = \\Delta y_k –ø—Ä–∏–≤–æ–¥–∏—Ç –∫:  (a u^T d_k) u = \\Delta y_k - B_k d_k \n–≠—Ç–æ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ —Ç–æ–ª—å–∫–æ –≤ —Ç–æ–º —Å–ª—É—á–∞–µ, –µ—Å–ª–∏ u –∫—Ä–∞—Ç–Ω–æ $ y_k - B_k d_k$. –ü–æ–ª–∞–≥–∞—è u = \\Delta y_k - B_k d_k, –º—ã —Ä–µ—à–∞–µ–º –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—É—é –≤—ã—à–µ –∑–∞–¥–∞—á—É,\n a = \\frac{1}{(\\Delta y_k - B_k d_k)^T d_k}, \n–ß—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫:  B_{k+1} = B_k +  \\frac{(\\Delta y_k - B_k d_k)(\\Delta y_k - B_k d_k)^T}{(\\Delta y_k - B_k d_k)^T d_k} \n–ù–∞–∑—ã–≤–∞–µ—Ç—Å—è —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–π –æ–¥–Ω–æ—Ä–∞–Ω–≥–æ–≤—ã–π –∞–ø–¥–µ–π—Ç (SR1) –∏–ª–∏ –º–µ—Ç–æ–¥ –ë—Ä–æ–π–¥–µ–Ω–∞.\nDavidon-Fletcher-Powell Update (DFP)\n–ö–∞–∫ –º—ã –º–æ–∂–µ–º —Ä–µ—à–∏—Ç—å  B_{k+1} d_{k+1} = -\\nabla f(x_{k+1}),  –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥? –í –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—é B_k –∫ B_{k+1}, –¥–∞–≤–∞–π—Ç–µ –±—É–¥–µ–º –ø—Ä–∏–≤–æ–¥–∏—Ç—å –æ–±—Ä–∞—Ç–Ω—ã–µ, —Ç.–µ, C_k = B_k^{-1} to C_{k+1} = (B_{k+1})^{-1}.\nSherman-Morrison Formula: –§–æ—Ä–º—É–ª–∞ –®–µ—Ä–º–∞–Ω–∞-–ú–æ—Ä—Ä–∏—Å–æ–Ω–∞ —É—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç:\n (A + uv^T)^{-1} = A^{-1} - \\frac{A^{-1}uv^T A^{-1}}{1 + v^T A^{-1}u} \n C_{k+1} = C_k + a u u^T + b v v^T. \n–£–º–Ω–æ–∂–∞—è –Ω–∞ \\Delta y_k –∏ –∏–ø–æ–ª—å–∑—É—è —É—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–µ–∫—É—â–∏—Ö d_k = C_{k+1} \\Delta y_k –∏–º–µ–µ—Ç:\n d_k = C_k\\Delta y_k + (a u^T \\Delta y_k)u + (bv^T\\Delta y_k)v \n–ü–æ–ª–∞–≥–∞—è u = C_k\\Delta y_k, v = d_k –∏ —Ä–µ—à–∞—è –¥–ª—è a, b –ø–æ–ª—É—á–∞–µ–º:\n (1 + a \\Delta y_k ^T C \\Delta y_k) C_k \\Delta y_k + (bd_k^T\\Delta y_k - 1)d_k \\Leftrightarrow a = -\\frac{1}{\\Delta y_k ^T C \\Delta y_k}, b = \\frac{1}{\\Delta y_k^T d_k}\n C_{k+1} = C_k - \\frac{C_k \\Delta y_k \\Delta y_k^T C_k}{\\Delta y_k^T C_k \\Delta y_k} + \\frac{d_k d_k^T}{\\Delta y_k^T d_k} \nWoodbury Formula Application –§–æ—Ä–º—É–ª–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç:\n\nB_{k+1} = \\left(I - \\frac{\\Delta y_k d_k^T}{\\Delta y_k^T d_k}\\right)B_k\\left(I - \\frac{d_k \\Delta y_k^T}{\\Delta y_k^T d_k}\\right) + \\frac{\\Delta y_k \\Delta y_k^T}{\\Delta y_k^T d_k}\n\n–≠—Ç–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ Davidon-Fletcher-Powell (DFP). –¢–∞–∫–∂–µ –¥–µ—à–µ–≤–æ: \\mathcal{O}(n^2), —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—É—é –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å. –ù–µ —Ç–∞–∫ –ø–æ–ø—É–ª—è—Ä–Ω–æ, –∫–∞–∫ BFGS.\nBroyden-Fletcher-Goldfarb-Shanno update\n–î–∞–≤–∞–π—Ç–µ —Ç–µ–ø–µ—Ä—å –ø–æ–ø—Ä–æ–±—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Ç–æ—Ä–æ–≥–æ —Ä–∞–Ω–≥–∞:  B_{k+1} = B_k + a u u^T + b v v^T. \n–£–º–Ω–æ–∂–∞—è –Ω–∞ \\Delta y_k –∏ –∏—Å–ø–æ–ª—å–∑—É—è —É—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–µ–∫—É—â–∏—Ö \\Delta y_k = B_{k+1} d_k –∏–º–µ–µ–º:  \\Delta y_k - B_k d_k = (a u^T d_k) u + (b v^T d_k) v \n–ü–æ–ª–∞–≥–∞—è u = \\Delta y_k, v = B_k d_k, –∏ —Ä–µ—à–∞—è –¥–ª—è a, b –º—ã –ø–æ–ª—É—á–∞–µ–º:  (1 - a \\Delta y_k^T d_k)\\Delta y_k - (1 + bd_k^TB_k d_k)B_kd_k \\Leftrightarrow a = \\frac{1}{y_k^T d_k}, b = -\\frac{1}{d_k^TB_k d_k}   B_{k+1} = B_k - \\frac{B_k d_k d_k^T B_k}{d_k^T B_k d_k} + \\frac{\\Delta y_k \\Delta y_k^T}{d_k^T \\Delta y_k}  –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –ë—Ä–æ–π–¥–µ–Ω–∞-–§–ª–µ—Ç—á–µ—Ä–∞-–ì–æ–ª—å–¥—Ñ–∞—Ä–±–∞-–®–∞–Ω–Ω–æ (BFGS).\n\n–¢–µ–æ—Ä–µ–º–∞ –æ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞ –ø—Ä–æ–µ–∫—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –¥–ª—è –≤—ã–ø—É–∫–ª–æ–π –≥–ª–∞–¥–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏.\n\n\n\n\n\n\n–ü—É—Å—Ç—å f: \\mathbb{R}^n \\to \\mathbb{R} –≤—ã–ø—É–∫–ª–∞—è –∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞—è. –ü—É—Å—Ç—å S \\subseteq  \\mathbb{R}^n –∑–∞–º–∫–Ω—É—Ç–æ–µ –≤—ã–ø—É–∫–ª–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ, –∏ –ø—É—Å—Ç—å x^* - –º–∏–Ω–∏–º–∏–∑–∞—Ç–æ—Ä f –Ω–∞ S; –∫—Ä–æ–º–µ —Ç–æ–≥–æ, –ø—É—Å—Ç—å f –≥–ª–∞–¥–∫–∞—è –Ω–∞ S —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º L. –ú–µ—Ç–æ–¥ –ø—Ä–æ–µ–∫—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ —Å —à–∞–≥–æ–º \\frac1L –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–ª–µ–¥—É—é—â–µ–π –æ—Ü–µ–Ω–∫–∏ –ø–æ—Å–ª–µ –∏—Ç–µ—Ä–∞—Ü–∏–∏ k &gt; 0: \nf(x_k) - f^* \\leq \\frac{L\\|x_0 - x^*\\|_2^2}{2k}\n\n\n\n\n\n–î–æ–∫–∞–∂–µ–º –ª–µ–º–º—É –æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–º —É–±—ã–≤–∞–Ω–∏–∏, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—è, —á—Ç–æ y_{k} = x_k - \\frac1L\\nabla f(x_k) –∏ —Ç–µ–æ—Ä–µ–º—É –∫–æ—Å–∏–Ω—É—Å–æ–≤ 2x^Ty = \\|x\\|^2 + \\|y\\|^2 - \\|x-y\\|^2: \n\\begin{aligned}\n&\\text{–ì–ª–∞–¥–∫–æ—Å—Ç—å:} &f(x_{k+1})& \\leq f(x_{k}) + \\langle \\nabla f(x_{k}), x_{k+1}-x_{k} \\rangle +\\frac{L}{2} \\| x_{k+1}-x_{k}\\|^2\\\\\n&\\text{–ú–µ—Ç–æ–¥:} & &= f(x_{k})-L\\langle y_{k} - x_k , x_{k+1}-x_{k} \\rangle +\\frac{L}{2} \\| x_{k+1}-x_{k}\\|^2\\\\\n&\\cos: & &= f(x_{k})-\\frac{L}{2}\\left( \\|y_{k} - x_k\\|^2 + \\|x_{k+1}-x_{k}\\|^2 - \\|y_{k} - x_{k+1}\\|^2\\right) +\\frac{L}{2} \\| x_{k+1}-x_{k}\\|^2\\\\\n& & &= f(x_{k})-\\frac{1}{2L}\\|\\nabla f(x_k)\\|^2 + \\frac{L}{2} \\|y_{k} - x_{k+1}\\|^2 \\\\\n\\end{aligned}\n  \\tag{16}\n–°–Ω–æ–≤–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–æ—Ä–µ–º—É –∫–æ—Å–∏–Ω—É—Å–æ–≤: \n\\begin{aligned}\n\\left\\langle\\frac1L \\nabla f(x_k), x_k - x^* \\right\\rangle &=  \\frac12\\left(\\frac{1}{L^2}\\|\\nabla f(x_k)\\|^2 + \\|x_k - x^*\\|^2 -  \\|x_k - x^* - \\frac1L \\nabla f(x_k)\\|^2 \\right) \\\\\n\\langle \\nabla f(x_k), x_k - x^* \\rangle &=  \\frac{L}{2}\\left(\\frac{1}{L^2}\\|\\nabla f(x_k)\\|^2 + \\|x_k - x^*\\|^2 -  \\|y_k - x^*\\|^2 \\right) \\\\\n\\end{aligned}\n\n–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–≤–æ–π—Å—Ç–≤–æ –ø—Ä–æ–µ–∫—Ü–∏–∏: \\|x - \\text{proj}_S(y)\\|^2 + \\|y - \\text{proj}_S(y)\\|^2 \\leq \\|x-y\\|^2 —Å x = x^*, y = y_k: \n\\begin{aligned}\n\\|x^* - \\text{proj}_S(y_k)\\|^2 + \\|y_k - \\text{proj}_S(y_k)\\|^2 \\leq \\|x^*-y_k\\|^2 \\\\\n\\|y_k - x^*\\|^2 \\geq \\|x^* - x_{k+1}\\|^2 + \\|y_k - x_{k+1}\\|^2\n\\end{aligned}\n\n–ò—Å–ø–æ–ª—å–∑—É—è –≤—ã–ø—É–∫–ª–æ—Å—Ç—å –∏ –ø—Ä–µ–¥—ã–¥—É—â—É—é —á–∞—Å—Ç—å: \n\\begin{aligned}\n&\\text{–í—ã–ø—É–∫–ª–æ—Å—Ç—å:} &f(x_k) - f^* &\\leq  \\langle \\nabla f(x_k), x_k - x^* \\rangle \\\\\n& & &\\leq  \\frac{L}{2}\\left(\\frac{1}{L^2}\\|\\nabla f(x_k)\\|^2 + \\|x_k - x^*\\|^2 -  \\|x_{k+1} - x^*\\|^2 - \\|y_k - x_{k+1}\\|^2 \\right)\n\\end{aligned}\n –°—É–º–º–∏—Ä—É–µ–º –ø–æ i=0,k-1 \n\\begin{aligned}\n\\sum\\limits_{i=0}^{k-1} \\left[f(x_i) - f^*\\right]&\\leq\\sum\\limits_{i=0}^{k-1} \\frac{1}{2L}\\|\\nabla f(x_i)\\|^2 + \\frac{L}{2}\\|x_0 - x^*\\|^2  - \\frac{L}{2} \\sum\\limits_{i=0}^{i-1} \\|y_i - x_{i+1}\\|^2\n\\end{aligned}\n\n–û—Ü–µ–Ω–∏–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å –ø–æ–º–æ—â—å—é –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ –æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–º —É–±—ã–≤–∞–Ω–∏–∏ (16): \n\\begin{aligned}\n\\sum\\limits_{i=0}^{k-1} \\left[f(x_i) - f^*\\right]&\\leq \\sum\\limits_{i=0}^{k-1}\\left[ f(x_{i}) - f(x_{i+1}) + \\frac{L}{2} \\|y_{i} - x_{i+1}\\|^2 \\right] + \\frac{L}{2}\\|x_0 - x^*\\|^2  - \\frac{L}{2} \\sum\\limits_{i=0}^{i-1} \\|y_i - x_{i+1}\\|^2  \\\\\n&\\leq f(x_0) - f(x_k) + \\frac{L}{2} \\sum\\limits_{i=0}^{i-1} \\|y_i - x_{i+1}\\|^2 + \\frac{L}{2}\\|x_0 - x^*\\|^2  - \\frac{L}{2} \\sum\\limits_{i=0}^{i-1} \\|y_i - x_{i+1}\\|^2 \\\\\n&\\leq f(x_0) - f(x_k) + \\frac{L}{2}\\|x_0 - x^*\\|^2 \\\\\n\\sum\\limits_{i=0}^{k-1} f(x_i) - k f^* &\\leq f(x_0) - f(x\n\\end{aligned}\n\n–ò–∑ –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ –æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–º —É–±—ã–≤–∞–Ω–∏–∏: \nf(x_{k+1}) \\le f(x_k) - \\frac{1}{2L}\\|\\nabla f(x_k)\\|^2 + \\frac{L}{2}\\|y_k - x_{k+1}\\|^2,\n\n–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç —Ñ–∞–∫—Ç, —á—Ç–æ x_{k+1} = \\mathrm{proj}_S(y_k). –ü–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –ø—Ä–æ–µ–∫—Ü–∏–∏: \n\\|y_k - x_{k+1}\\| \\le \\|y_k - x_k\\|,\n\n–ò—Ç–∞–∫, y_k = x_k - \\tfrac{1}{L}\\nabla f(x_k) –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç, —á—Ç–æ \\|y_k - x_k\\| = \\tfrac{1}{L}\\|\\nabla f(x_k)\\|. –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ: \n\\frac{L}{2}\\,\\|y_k - x_{k+1}\\|^2 \\le \\frac{L}{2}\\,\\|y_k - x_k\\|^2 = \\frac{L}{2}\\,\\frac{1}{L^2}\\,\\|\\nabla f(x_k)\\|^2 = \\frac{1}{2L}\\,\\|\\nabla f(x_k)\\|^2.\n\n–ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ (*): \nf(x_{k+1}) \\le f(x_k) - \\frac{1}{2L}\\|\\nabla f(x_k)\\|^2 + \\frac{1}{2L}\\|\\nabla f(x_k)\\|^2 = f(x_k).\n\n–°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ: \nf(x_{k+1}) \\le f(x_k)\\quad\\text{for each }k,\n –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, \\{f(x_k)\\} —è–≤–ª—è–µ—Ç—Å—è –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ –Ω–µ–≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é.\n–û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏: –ò–∑ —à–∞–≥–∞ 5 –º—ã —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–∏–ª–∏: \n\\sum_{i=0}^{k-1}\\bigl[f(x_i) - f^*\\bigr] \\le \\frac{L}{2}\\|x_0 - x^*\\|_2^2.\n –ü–æ—Å–∫–æ–ª—å–∫—É f(x_i) —É–±—ã–≤–∞–µ—Ç –≤ i, –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏ f(x_k) \\le f(x_i) –¥–ª—è –≤—Å–µ—Ö i \\le k. –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ: \nk\\,\\bigl[f(x_k) - f^*\\bigr] \\le \\sum_{i=0}^{k-1}\\bigl[f(x_i) - f^*\\bigr] \\le \\frac{L}{2}\\|x_0 - x^*\\|_2^2,\n –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ: \nf(x_k) - f^* \\le \\frac{L\\|x_0 - x^*\\|_2^2}{2k}.\n \n\n–¢–µ–æ—Ä–µ–º–∞ –æ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞ –§—Ä–∞–Ω–∫-–í—É–ª—å—Ñ–∞ –¥–ª—è –≤—ã–ø—É–∫–ª—ã—Ö –≥–ª–∞–¥–∫–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π.\n\n\n\n\n\n\n–ü—É—Å—Ç—å f: \\mathbb{R}^n \\to \\mathbb{R} –≤—ã–ø—É–∫–ª–∞—è –∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞—è. –ü—É—Å—Ç—å S \\subseteq \\mathbb{R}^n –∑–∞–º–∫–Ω—É—Ç–æ–µ –≤—ã–ø—É–∫–ª–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ, –∏ –ø—É—Å—Ç—å x^* - –º–∏–Ω–∏–º–∏–∑–∞—Ç–æ—Ä f –Ω–∞ S; –∫—Ä–æ–º–µ —Ç–æ–≥–æ, –ø—É—Å—Ç—å f –≥–ª–∞–¥–∫–∞—è –Ω–∞ S —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º L. –ú–µ—Ç–æ–¥ –§—Ä–∞–Ω–∫-–í—É–ª—å—Ñ–∞ —Å —à–∞–≥–æ–º \\gamma_k = \\frac{k-1}{k+1} –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–ª–µ–¥—É—é—â–µ–π –æ—Ü–µ–Ω–∫–∏ –ø–æ—Å–ª–µ –∏—Ç–µ—Ä–∞—Ü–∏–∏ k &gt; 0: \nf(x_k) - f^* \\leq \\frac{2LR^2}{k+1}\n –≥–¥–µ R = \\max\\limits_{x, y \\in S} \\|x - y\\| - –¥–∏–∞–º–µ—Ç—Ä –º–Ω–æ–∂–µ—Å—Ç–≤–∞ S.\n\n\n\n–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ\n\n–ò–∑ –≥–ª–∞–¥–∫–æ—Å—Ç–∏ f: \n\\begin{aligned}\nf\\left(x_{k+1}\\right) - f\\left(x_k\\right) &\\leq \\left\\langle \\nabla f\\left(x_k\\right), x_{k+1} - x_k \\right\\rangle + \\frac{L}{2} \\left\\|x_{k+1} - x_k\\right\\|^2 \\\\\n&= (1 - \\gamma_k) \\left\\langle \\nabla f\\left(x_k\\right), y_k - x_k \\right\\rangle + \\frac{L (1 - \\gamma_k)^2}{2} \\left\\|y_k - x_k\\right\\|^2\n\\end{aligned}\n\n–ò–∑ –≤—ã–ø—É–∫–ª–æ—Å—Ç–∏ f, –¥–ª—è –ª—é–±–æ–≥–æ x \\in S, –≤–∫–ª—é—á–∞—è x^*: \n\\langle \\nabla f(x_k), x - x_k \\rangle \\leq f(x) - f(x_k)\n –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –¥–ª—è x = x^*: \n\\langle \\nabla f(x_k), x^* - x_k \\rangle \\leq f(x^*) - f(x_k)\n\n–ü–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é y_k, –º—ã –∏–º–µ–µ–º \\langle \\nabla f(x_k), y_k \\rangle \\leq \\langle \\nabla f(x_k), x^* \\rangle, —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ: \n\\langle \\nabla f(x_k), y_k - x_k \\rangle \\leq \\langle \\nabla f(x_k), x^* - x_k \\rangle \\leq f(x^*) - f(x_k)\n\n–û–±—ä–µ–¥–∏–Ω—è—è –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞: \n\\begin{aligned}\nf\\left(x_{k+1}\\right) - f\\left(x_k\\right) &\\leq (1 - \\gamma_k) \\left\\langle \\nabla f\\left(x_k\\right), y_k - x_k \\right\\rangle + \\frac{L (1 - \\gamma_k)^2}{2} \\left\\|y_k - x_k\\right\\|^2 \\\\\n&\\leq (1 - \\gamma_k) \\left( f(x^*) - f(x_k) \\right) + \\frac{L (1 - \\gamma_k)^2}{2} R^2\n\\end{aligned}\n\n–ü–µ—Ä–µ–≥—Ä—É–ø–ø–∏—Ä—É–µ–º: \n\\begin{aligned}\nf\\left(x_{k+1}\\right) - f(x^*) &\\leq \\gamma_k \\left( f(x_k) - f(x^*) \\right) + (1 - \\gamma_k)^2 \\frac{L R^2}{2}\n\\end{aligned}\n\n–û–±–æ–∑–Ω–∞—á–∏–≤ \\delta_k = \\frac{f\\left(x_k\\right) - f\\left(x^*\\right)}{L R^2}, –º—ã –ø–æ–ª—É—á–∞–µ–º: \n\\delta_{k+1} \\leq \\gamma_k \\delta_k + \\frac{(1 - \\gamma_k)^2}{2} = \\frac{k - 1}{k + 1} \\delta_k + \\frac{2}{(k + 1)^2}\n\n–î–æ–∫–∞–∂–µ–º, —á—Ç–æ \\delta_k \\leq \\frac{2}{k+1} –ø–æ –∏–Ω–¥—É–∫—Ü–∏–∏.\n\n–ë–∞–∑–∞: \\delta_2 \\leq \\frac{1}{2} &lt; \\frac23\n–ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ \\delta_k \\leq \\frac{2}{k+1}\n–¢–æ–≥–¥–∞ \\delta_{k+1} \\leq \\frac{k-1}{k+1} \\cdot \\frac{2}{k+1} + \\frac{2}{(k+1)^2} = \\frac{2k}{k^2 + 2k + 1} &lt; \\frac{2}{k+2} \n\n–ü–æ–ª—É—á–∞–µ–º –∂–µ–ª–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: \nf(x_k) - f^* \\leq \\frac{2LR^2}{k+1}\n\n\n–¢–µ–æ—Ä–µ–º–∞ –æ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è –≤—ã–ø—É–∫–ª—ã—Ö –õ–∏–ø—à–∏—Ü–µ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π. –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤—ã–±–æ—Ä–∞ —à–∞–≥–∞ –¥–ª—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏. –ö–∞–∫ –æ–±–µ—Å–ø–µ—á–∏—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å —Å –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º —à–∞–≥–æ–º, –∑–∞–¥–∞–≤–∞–µ–º—ã–º –∑–∞—Ä–∞–Ω–µ–µ? –ö–∞–∫ –æ–±–µ—Å–ø–µ—á–∏—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å —Å —É–±—ã–≤–∞—é—â–∏–º —à–∞–≥–æ–º?\n\n\n\n\n\n\n–ü—É—Å—Ç—å f –≤—ã–ø—É–∫–ª–∞—è –∏ G-–ª–∏–ø—à–∏—Ü–µ–≤–∞. –ü—É—Å—Ç—å R = \\|x_0 - x^*\\|_2. –î–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ —à–∞–≥–∞ \\alpha = \\frac{R}{G}\\sqrt{\\frac{1}{k}}, —Å—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –º–µ—Ç–æ–¥ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è–µ—Ç \nf_k^{\\text{best}} - f(x^*) \\leq \\frac{G R}{\\sqrt{k}}\n\n\n\n\n\n\\begin{split}\n\\| x_{k+1} - x^* \\|^2 & = \\|x_k - x^* - \\alpha_k g_k\\|^2 = \\\\\n& =   \\| x_k - x^* \\|^2 + \\alpha_k^2 \\|g_k\\|^2 - 2 \\alpha_k \\langle g_k, x_k - x^* \\rangle \\\\\n&\\leq \\| x_k - x^* \\|^2 + \\alpha_k^2 \\|g_k\\|^2 - 2 \\alpha_k (f(x_k) - f(x^*)) \\\\\n2 \\alpha_k (f(x_k) - f(x^*)) &\\leq \\| x_k - x^* \\|^2 - \\| x_{k+1} - x^* \\|^2 + \\alpha_k^2 \\|g_k\\|^2\n\\end{split}\n\n–°—É–º–º–∏—Ä—É–µ–º –ø–æ–ª—É—á–µ–Ω–Ω–æ–µ –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –¥–ª—è k = 0, \\ldots, T-1: \n\\begin{split}\n\\sum\\limits_{k = 0}^{T-1}2\\alpha_k (f(x_k) - f(x^*)) &\\leq  \\| x_0 - x^* \\|^2 - \\| x_{T} - x^* \\|^2 + \\sum\\limits_{k=0}^{T-1}\\alpha_k^2 \\|g_k\\|^2 \\\\\n&\\leq \\| x_0 - x^* \\|^2 + \\sum\\limits_{k=0}^{T-1}\\alpha_k^2 \\|g_k\\|^2 \\\\\n&\\leq R^2 + G^2\\sum\\limits_{k=0}^{T-1}\\alpha_k^2\n\\end{split}\n\n\n–ó–∞–ø–∏—à–µ–º, –Ω–∞—Å–∫–æ–ª—å–∫–æ –±–ª–∏–∑–∫–æ –º—ã –ø–æ–¥–æ—à–ª–∏ –∫ –æ–ø—Ç–∏–º—É–º—É x^* = \\text{arg}\\min\\limits_{x \\in \\mathbb{R}^n} f(x) = \\text{arg} f^* –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏:\n–î–ª—è —Å—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–∞: \\langle g_k, x^* - x_k \\rangle \\leq f(x^*) - f(x_k).\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ \\|g_k\\|^2 \\leq G^2\n–ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ R = \\|x_0 - x^*\\|_2\n\n–ó–∞–º–µ—Ç–∏–º, —á—Ç–æ: \n\\sum\\limits_{k = 0}^{T-1}2\\alpha_k (f(x_k) - f(x^*)) \\geq\n\\sum\\limits_{k = 0}^{T-1}2\\alpha_k (f_T^{\\text{best}} - f(x^*)) =\n(f_T^{\\text{best}} - f(x^*))\\sum\\limits_{k = 0}^{T-1}2\\alpha_k\n\n–ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–µ –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ: \n\\boxed{\nf_T^{\\text{best}} - f(x^*) \\leq \\frac{R^2 + G^2\\sum\\limits_{k=0}^{T-1}\\alpha_k^2}{2\\sum\\limits_{k = 0}^{T-1}\\alpha_k}}\n\n–ò–∑ —ç—Ç–æ–≥–æ —Å–ª–µ–¥—É–µ—Ç, —á—Ç–æ –µ—Å–ª–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —à–∞–≥–∞ —Ç–∞–∫–∞—è, —á—Ç–æ \n\\sum\\limits_{k = 0}^{T-1}\\alpha_k^2 &lt; \\infty, \\quad \\sum\\limits_{k = 0}^{T-1}\\alpha_k = \\infty,\n —Ç–æ —Å—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –º–µ—Ç–æ–¥ —Å—Ö–æ–¥–∏—Ç—Å—è (—à–∞–≥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–±—ã–≤–∞—é—â–∏–º, –Ω–æ –Ω–µ —Å–ª–∏—à–∫–æ–º –±—ã—Å—Ç—Ä–æ).\n\n–ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ \\alpha = \\frac{R}{G}\\sqrt{\\frac{1}{T}}, —Ç–æ: \n  f_T^{\\text{best}} - f(x^*) \\leq \\frac{R^2 + G^2 T\\frac{R^2}{G^2}\\frac{1}{T}}{2 T \\frac{R}{G}\\sqrt{\\frac{1}{T}}} = \\frac{GR}{\\sqrt{T}}\n  \n–ï—Å–ª–∏ \\alpha_k = \\frac{R}{G\\sqrt{k+1}}, —Ç–æ:\n\n–û–≥—Ä–∞–Ω–∏—á–∏–º —Å—É–º–º—ã: \n\\sum_{k=0}^{T-1}\\alpha_k^2 = \\frac{R^2}{G^2}\\sum_{k=1}^{T}\\frac{1}{k} \\le \\frac{R^2}{G^2}\\bigl(1+\\ln T\\bigr); \\qquad \\sum_{k=0}^{T-1}\\alpha_k = \\frac{R}{G}\\sum_{k=1}^{T}\\frac{1}{\\sqrt{k}} \\ge \\frac{R}{G}\\int_{1}^{T+1}\\frac{1}{\\sqrt{t}}\\,dt = \\frac{2R}{G}\\bigl(\\sqrt{T+1}-1\\bigr).\n\n–£–±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π -1 –≤ –≤–µ—Ä—Ö–Ω–µ–π –æ—Ü–µ–Ω–∫–µ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–µ –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ: \nf_T^{\\text{best}} - f(x^*) \\leq \\frac{R^2 + G^2\\sum\\limits_{k=0}^{T-1}\\alpha_k^2}{2\\sum\\limits_{k = 0}^{T-1}\\alpha_k} \\leq \\frac{R^2 + R^2 (1 + \\ln T)}{4\\frac{R}{G}\\bigl(\\sqrt{T+1}\\bigr)} = \\frac{GR(2 + \\ln T)}{4\\sqrt{T+1}}\n \n  f_k^{\\mathrm{best}} - f(x^*) \\le \\frac{GR(2+\\ln k)}{4\\sqrt{k+1}}\n  \n\n\n–¢–µ–æ—Ä–µ–º–∞ –æ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã—Ö –õ–∏–ø—à–∏—Ü–µ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π.\n\n\n\n\n\n\n–ü—É—Å—Ç—å f - \\mu-—Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–∞—è —Ñ—É–Ω–∫—Ü–∏—è (–≤–æ–∑–º–æ–∂–Ω–æ, –Ω–µ–≥–ª–∞–¥–∫–∞—è) —Å –º–∏–Ω–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º x^* –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Å—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏ \\|g_k\\| \\le G. –ò—Å–ø–æ–ª—å–∑—É—è —à–∞–≥ \\alpha_k = \\frac{2}{\\mu (k+1)}, —Å—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –¥–ª—è k &gt; 0 —Å–ª–µ–¥—É—é—â–µ–µ: \nf_k^{\\text{best}} - f(x^*) \\leq \\frac{2G^2}{\\mu k}\n\n\n\n\n–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π —Ñ–∞–∫—Ç\n–ü—É—Å—Ç—å f - \\mu-—Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–∞—è —Ñ—É–Ω–∫—Ü–∏—è –Ω–∞ –≤—ã–ø—É–∫–ª–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ –∏ x, y - –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏. –¢–æ–≥–¥–∞ –¥–ª—è –ª—é–±–æ–≥–æ g\\in\\partial f(x), \n\\langle g,x-y\\rangle \\ge f(x)-f(y)+\\frac{\\mu}{2}\\|x-y\\|^2.\n\n\n–î–ª—è –ª—é–±–æ–≥–æ \\lambda\\in[0,1), –∏–∑ \\mu-—Å–∏–ª—å–Ω–æ–π –≤—ã–ø—É–∫–ª–æ—Å—Ç–∏: \nf(\\lambda x+(1-\\lambda)y) \\le \\lambda f(x)+(1-\\lambda)f(y)-\\frac{\\mu}{2}\\lambda(1-\\lambda)\\|x-y\\|^2.\n\n–ò–∑ –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ —Å—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –≤ x, –º—ã –∏–º–µ–µ–º: \nf(\\lambda x+(1-\\lambda)y) \\ge f(x) + \\langle g,\\lambda x+(1-\\lambda)y-x\\rangle \\quad \\to \\quad f(\\lambda x+(1-\\lambda)y) \\ge f(x)-(1-\\lambda)\\langle g,x-y\\rangle.\n\n–°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, \n\\begin{aligned}\nf(x)-(1-\\lambda)\\langle g,x-y\\rangle &\\le \\lambda f(x)+(1-\\lambda)f(y)-\\frac{\\mu}{2}\\lambda(1-\\lambda)\\|x-y\\|^2 \\\\\n(1 - \\lambda) f(x) &\\le (1 - \\lambda) f(y) + (1 - \\lambda) \\langle g,x-y\\rangle - \\frac{\\mu}{2}\\lambda(1-\\lambda)\\|x-y\\|^2 \\\\\nf(x) &\\le f(y) + \\langle g,x-y\\rangle - \\frac{\\mu}{2}\\lambda\\|x-y\\|^2 \\\\\n\\end{aligned}\n\n–ü—É—Å—Ç—å \\lambda\\to 1^- –ø–æ–ª—É—á–∞–µ–º f(x) \\le f(y) + \\langle g,x-y\\rangle - \\frac{\\mu}{2}\\|x-y\\|^2 \\to \\langle g,x-y\\rangle \\ge f(x)-f(y)+\\frac{\\mu}{2}\\|x-y\\|^2.\n\n–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ\n\n–ù–∞—á–Ω–µ–º —Å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –º–µ—Ç–æ–¥–∞ –∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ: \n\\begin{aligned}\n\\| x_{k+1} - x^* \\|^2 & = \\|x_k - x^* - \\alpha_k g_k\\|^2 = \\\\\n& =   \\| x_k - x^* \\|^2 + \\alpha_k^2 \\|g_k\\|^2 - 2 \\alpha_k \\langle g_k, x_k - x^* \\rangle \\\\\n&\\leq \\| x_k - x^* \\|^2 + \\alpha_k^2 \\|g_k\\|^2 - 2 \\alpha_k (f(x_k) - f(x^*)) - \\alpha_k \\mu  \\|x_k - x^*\\|^2 \\\\\n&= (1 - \\mu \\alpha_k)\\| x_k - x^* \\|^2 + \\alpha_k^2 \\|g_k\\|^2 - 2 \\alpha_k \\left(f(x_k) - f(x^*)\\right) \\\\\n2\\alpha_k\\left(f(x_k)-f(x^*)\\right) &\\le (1-\\mu\\alpha_k)\\|x_k-x^*\\|^2-\\|x_{k+1}-x^*\\|^2+\\alpha_k^2\\|g_k\\|^2 \\\\\nf(x_k)-f(x^*) &\\le \\frac{1-\\mu\\alpha_k}{2\\alpha_k}\\|x_k-x^*\\|^2-\\frac{1}{2\\alpha_k}\\|x_{k+1}-x^*\\|^2+\\frac{\\alpha_k}{2}\\|g_k\\|^2\n\\end{aligned}\n\n–ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º —à–∞–≥ \\alpha_k = \\frac{2}{\\mu (k+1)} –≤ –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ: \n\\begin{aligned}\nf(x_k)-f(x^*) &\\le \\frac{\\mu (k-1)}{4}\\|x_k-x^*\\|^2-\\frac{\\mu(k+1)}{4}\\|x_{k+1}-x^*\\|^2+\\frac{1}{\\mu(k+1)}\\|g_k\\|^2 \\\\\nf(x_k)-f(x^*) &\\le \\frac{\\mu (k-1)}{4}\\|x_k-x^*\\|^2-\\frac{\\mu(k+1)}{4}\\|x_{k+1}-x^*\\|^2+\\frac{1}{\\mu k}\\|g_k\\|^2 \\\\\nk \\left(f(x_k)-f(x^*)\\right) &\\le \\frac{\\mu k(k-1)}{4}\\|x_k-x^*\\|^2-\\frac{\\mu k(k+1)}{4}\\|x_{k+1}-x^*\\|^2+\\frac{1}{\\mu}\\|g_k\\|^2\n\\end{aligned}\n\n–°—É–º–º–∏—Ä—É–µ–º –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ –¥–ª—è –≤—Å–µ—Ö k = 0, 1, \\ldots, T-1, –ø–æ–ª—É—á–∞–µ–º: \n\\begin{aligned}\n\\sum_{k=0}^{T-1} k \\left(f(x_k)-f(x^*)\\right) &\\le 0 -\\frac{\\mu (T-1)T}{4}\\|x_{T}-x^*\\|^2+\\frac{1}{\\mu}\\sum_{k=0}^{T-1}\\|g_k\\|^2 \\leq \\frac{G^2 T}{\\mu} \\\\\n\\left(f^{\\text{best}}_{T-1}-f(x^*)\\right) \\sum_{k=0}^{T-1} k &= \\sum_{k=0}^{T-1} k \\left(f^{\\text{best}}_{T-1}-f(x^*)\\right) \\le \\sum_{k=0}^{T-1} k \\left(f(x_k)-f(x^*)\\right) \\leq \\frac{G^2 T}{\\mu} \\\\\nf^{\\text{best}}_{T-1}-f(x^*) &\\leq \\frac{G^2 T}{\\mu \\sum_{k=0}^{T-1} k } = \\frac{2G^2 T}{\\mu T (T-1) } \\qquad  f_k^{\\text{best}} - f(x^*) \\leq \\frac{2G^2}{\\mu k}.\n\\end{aligned}\n\n\n–¢–µ–æ—Ä–µ–º–∞ –æ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è –≤—ã–ø—É–∫–ª–æ–π –≥–ª–∞–¥–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ f.\n\n\n\n\n\n\n–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –∑–∞–¥–∞—á—É  \\varphi(x) \\rightarrow \\min\\limits_{x \\in \\mathbb{R}^d}.  –ü—Ä–∏—á–µ–º \\varphi(x) = f(x) + r(x), –∏\n\nf-–≤—ã–ø—É–∫–ª–∞—è –∏ L-–≥–ª–∞–¥–∫–∞—è, \\text{dom}f = \\mathbb{R}^n\nr - –≤—ã–ø—É–∫–ª–∞—è –∏ \\text{prox}_{\\alpha r}(x_k) = \\arg \\min\\limits_{x \\in \\mathbb{R}^n}\\left[\\alpha r(x) + \\frac{1}{2}\\|x - x_k\\|^2\\right] –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω\n\n–¢–æ–≥–¥–∞ –¥–ª—è –ø—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —à–∞–≥–æ–º \\alpha = \\frac{1}{L}  x_{k+1} = \\text{prox}_{\\alpha, r}\\left(x_k - \\frac{1}{L} \\nabla f(x_k)\\right)  –≤—ã–ø–æ–ª–Ω–∞—è–µ—Ç—Å—è  \\varphi(x_k) - \\varphi^* \\leqslant \\frac{L \\|x_0 - x^*\\|^2}{2 k},  –¢–æ –µ—Å—Ç—å –∏–º–µ–µ—Ç –º–µ—Å—Ç–æ —Å—É–±–ª–∏–Ω–µ–π–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å.\n\n\n\n\n–ü—Ä–µ–¥—Å—Ç–∞–≤–∏–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –æ–±–æ–∑–Ω–∞—á–∞–µ–º–æ–µ –∫–∞–∫ G_{\\alpha}(x):  x_{k+1} = \\text{prox}_{\\alpha r}(x_k - \\alpha \\nabla f(x_k))   x_{k+1} = x_k - \\alpha G_{\\alpha}(x_k).  –≥–¥–µ G_{\\alpha}(x) –∏–º–µ–µ—Ç –≤–∏–¥:  G_{\\alpha}(x) = \\frac{1}{\\alpha} \\left(x - \\text{prox}_{\\alpha r}\\left(x - \\alpha \\nabla f(x)\\right)\\right) G_{\\alpha}(x) = 0 \\Leftrightarrow x = x^* \\Rightarrow G_{\\alpha} –∞–Ω–∞–ª–æ–≥–∏—á–µ–Ω \\nabla f.\nL-–≥–ª–∞–¥–∫–æ—Å—Ç—å:  f(x_{k+1}) \\leqslant  f(x_k) + \\langle \\nabla f(x_k), x_{k+1}-x_k \\rangle + \\frac{L}{2}\\|x_{k+1}-x_k\\|_2^2  –í—ã–ø—É–∫–ª–æ—Å—Ç—å:  f(x) \\geqslant f(x_k) + \\langle \\nabla f(x_k), x-x_k \\rangle    f(x_{k+1}) \\leqslant f(x) - \\langle \\nabla f(x_k), x-x_k \\rangle + \\langle \\nabla f(x_k), x_{k+1}-x_k \\rangle + \\frac{\\alpha^2 L}{2}\\|G_{\\alpha}(x_k)\\|_2^2  \\leqslant   \\leqslant f(x) + \\langle \\nabla f(x_k), x_{k+1}-x \\rangle + \\frac{\\alpha^2 L}{2}\\|G_{\\alpha}(x_k)\\|_2^2 \\qquad (1)\n–í–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è —Å–≤–æ–π—Å—Ç–≤–æ–º –ø—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞: \nx_{k+1} = \\text{prox}_{\\alpha r}\\left(x_k - \\alpha \\nabla f(x_k)\\right) \\quad  \\Leftrightarrow \\quad x_k - \\alpha \\nabla f(x_k) - x_{k+1} \\in \\partial \\alpha r (x_{k+1})   x_k - x_{k+1} = \\alpha G_{\\alpha}(x_k) \\qquad \\Rightarrow \\qquad  \\alpha G_{\\alpha}(x_k) - \\alpha \\nabla f(x_k) \\in \\partial \\alpha r (x_{k+1})  G_{\\alpha}(x_k) - \\nabla f(x_k) \\in \\partial r (x_{k+1}) \n–ü–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é —Å—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–∞:  r(x) \\geqslant r(x_{k+1}) + \\langle g, x - x_{k+1} \\rangle, \\quad g \\in \\partial r (x_{k+1})   \\qquad  r(x) \\geqslant r(x_{k+1}) + \\langle G_{\\alpha}(x_k) - \\nabla f(x), x - x_{k+1} \\rangle   r(x) \\geqslant r(x_{k+1}) + \\langle G_{\\alpha}(x_k), x - x_{k+1} \\rangle - \\langle \\nabla f(x), x - x_{k+1} \\rangle   \\langle \\nabla f(x),x_{k+1} - x \\rangle \\leqslant r(x) - r(x_{k+1}) - \\langle G_{\\alpha}(x_k), x - x_{k+1} \\rangle \n–ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ (1):  f(x_{k+1}) \\leqslant f(x) + \\langle \\nabla f(x_k), x_{k+1}-x \\rangle + \\frac{\\alpha^2 L}{2}\\|G_{\\alpha}(x_k)\\|_2^2   f(x_{k+1}) \\leqslant f(x) + r(x) - r(x_{k+1}) - \\langle G_{\\alpha}(x_k), x - x_{k+1} \\rangle + \\frac{\\alpha^2 L}{2}\\|G_{\\alpha}(x_k)\\|_2^2   f(x_{k+1}) + r(x_{k+1}) \\leqslant f(x) + r(x) - \\langle G_{\\alpha}(x_k), x - x_k + \\alpha G_{\\alpha}(x_k) \\rangle + \\frac{\\alpha^2 L}{2}\\|G_{\\alpha}(x_k)\\|_2^2 \n–ò—Å–ø–æ–ª—å–∑—É—è $(x) = f(x) + r(x) $ –¥–æ–∫–∞–∑—ã–≤–∞–µ–º –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ –∏—Ç–µ—Ä–∞—Ü–∏–∏:  \\varphi(x_{k+1}) \\leqslant \\varphi(x) - \\langle G_{\\alpha}(x_k), x - x_k \\rangle - \\langle G_{\\alpha}(x_k), \\alpha G_{\\alpha}(x_k) \\rangle + \\frac{\\alpha^2 L}{2}\\|G_{\\alpha}(x_k)\\|_2^2   \\varphi(x_{k+1}) \\leqslant \\varphi(x) + \\langle G_{\\alpha}(x_k), x_k - x \\rangle + \\frac{\\alpha}{2} \\left( \\alpha L - 2 \\right) \\|G_{\\alpha}(x_k) \\|_2^2  \\left(\\alpha \\leqslant \\frac{1}{L} \\Rightarrow \\frac{\\alpha}{2} \\left( \\alpha L - 2 \\right) \\leqslant  -\\frac{\\alpha}{2}\\right) \\Rightarrow \\quad \\varphi(x_{k+1}) \\leqslant \\varphi(x) + \\langle G_{\\alpha}(x_k), x_k - x \\rangle - \\frac{\\alpha}{2} \\|G_{\\alpha}(x_k) \\|_2^2   x := x_k \\Rightarrow \\qquad \\varphi(x_{k+1}) \\leqslant \\varphi(x_k) - \\frac{\\alpha}{2} \\|G_{\\alpha}(x_k) \\|_2^2\n–†–∞—Å—Å–º–æ—Ç—Ä–∏–º —Ç–µ–ø–µ—Ä—å x = x^*:  \\varphi(x_{k+1}) \\leqslant \\varphi(x^*) + \\langle G_{\\alpha}(x_k), x_k - x^* \\rangle - \\frac{\\alpha}{2} \\|G_{\\alpha}(x_k) \\|_2^2   \\varphi(x_{k+1}) - \\varphi(x^*) \\leqslant \\langle G_{\\alpha}(x_k), x_k - x^* \\rangle - \\frac{\\alpha}{2} \\|G_{\\alpha}(x_k) \\|_2^2 \\leqslant \\frac{1}{2\\alpha}\\left[2 \\langle \\alpha G_{\\alpha}(x_k), x_k - x^* \\rangle - \\|\\alpha G_{\\alpha}(x_k) \\|_2^2\\right] \\leqslant   \\leqslant \\frac{1}{2\\alpha}\\left[2 \\langle \\alpha G_{\\alpha}(x_k), x_k - x^* \\rangle - \\|\\alpha G_{\\alpha}(x_k) \\|_2^2 - \\|x_k - x^* \\|_2^2 + \\|x_k - x^* \\|_2^2\\right] \\leqslant   \\leqslant \\frac{1}{2\\alpha}\\left[- \\|x_k - x^* -  \\alpha G_{\\alpha}(x_k)\\|_2^2 + \\|x_k - x^* \\|_2^2\\right] \\leqslant \\frac{1}{2\\alpha}\\left[\\|x_k - x^* \\|_2^2 - \\|x_{k+1} - x^* \\|_2^2\\right] \n–°—É–º–º–∏—Ä—É–µ–º i = \\overline{0, k-1} –∏ —Å—É–º–º–∏—Ä—É–µ–º –∏—Ö:\n \\sum\\limits_{i=0}^{k-1}\\left[ \\varphi(x_{i+1}) - \\varphi(x^*) \\right] \\leqslant \\frac{1}{2\\alpha}\\left[\\|x_0 - x^* \\|_2^2 - \\|x_{k} - x^* \\|_2^2\\right] \\leqslant \\frac{1}{2\\alpha} \\|x_0 - x^* \\|_2^2 \n–ü–æ—Å–∫–æ–ª—å–∫—É $ (x_{k}) $ —è–≤–ª—è–µ—Ç—Å—è —É–±—ã–≤–∞—é—â–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é, –∏–∑ —ç—Ç–æ–≥–æ —Å–ª–µ–¥—É–µ—Ç, —á—Ç–æ:\n k \\varphi(x_{k}) \\leqslant \\sum\\limits_{i=0}^{k-1} \\varphi(x_{i+1}) \\Rightarrow \\varphi(x_{k}) \\leqslant \\frac1k \\sum\\limits_{i=0}^{k-1} \\varphi(x_{i+1})   \\varphi(x_{k})  - \\varphi(x^*) \\leqslant \\frac1k \\sum\\limits_{i=0}^{k-1}\\left[ \\varphi(x_{i+1}) - \\varphi(x^*)\\right] \\leqslant \\frac{\\|x_0 - x^* \\|_2^2}{2\\alpha k} =  \\frac{L\\|x_0 - x^* \\|_2^2}{2 k} \n–¢–æ –µ—Å—Ç—å –∏–º–µ–µ—Ç –º–µ—Å—Ç–æ —Å—É–±–ª–∏–Ω–µ–π–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å.\n\n–¢–µ–æ—Ä–µ–º–∞ –æ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–æ–π –≥–ª–∞–¥–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏.\n\n\n\n\n\n\n–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –∑–∞–¥–∞—á—É \n\\varphi(x) \\rightarrow \\min\\limits_{x \\in \\mathbb{R}^d}.\n –ü—Ä–∏—á–µ–º \\varphi(x) = f(x) + r(x), –∏\n\nf - \\mu-—Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–∞—è –∏ L-–≥–ª–∞–¥–∫–∞—è, \\text{dom}f = \\mathbb{R}^n\nr - –≤—ã–ø—É–∫–ª–∞—è –∏ \\text{prox}_{\\alpha r}(x_k) = \\arg \\min\\limits_{x \\in \\mathbb{R}^n}\\left[\\alpha r(x) + \\frac{1}{2}\\|x - x_k\\|^2\\right] –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω\n\n–¢–æ–≥–¥–∞ –¥–ª—è –ø—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —à–∞–≥–æ–º \\alpha \\leq \\frac1L –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∞—è –æ—Ü–µ–Ω–∫–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –∏—Ç–µ—Ä–∞—Ü–∏–∏ k &gt; 0: \n\\|x_{k} - x^*\\|_2^2 \\leq \\left(1 - \\alpha \\mu\\right)^k \\|x_{0} - x^*\\|_2^2.\n\n\n\n\n–î–ª—è –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç—Ä–∏ —Ñ–∞–∫—Ç–∞:\n\n–ü—É—Å—Ç—å f: \\mathbb{R}^n \\rightarrow \\mathbb{R} - L-–≥–ª–∞–¥–∫–∞—è –≤—ã–ø—É–∫–ª–∞—è —Ñ—É–Ω–∫—Ü–∏—è. –¢–æ–≥–¥–∞ –¥–ª—è –ª—é–±—ã—Ö x, y \\in \\mathbb{R}^n –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ: \n\\begin{aligned}\nf(x) + \\langle \\nabla f(x), y - x \\rangle + \\frac{1}{2L} & \\|\\nabla f(x) - \\nabla f(y)\\|^2_2 \\leq f(y) \\text{ or, equivalently, }\\\\\n\\|\\nabla f(y)-\\nabla f (x)\\|_2^2 = & \\|\\nabla f(x)-\\nabla f (y)\\|_2^2 \\leq 2L\\left(f(x)-f(y)-\\langle\\nabla f (y),x -y\\rangle \\right)\n\\end{aligned}\n\n–ü—É—Å—Ç—å f: \\mathbb{R}^n \\rightarrow \\mathbb{R} –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞ –Ω–∞ \\mathbb{R}^n. –¢–æ–≥–¥–∞ —Ñ—É–Ω–∫—Ü–∏—è f \\mu-—Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª–∞, –µ—Å–ª–∏ –∏ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–ª—è –ª—é–±—ã—Ö x, y \\in \\mathbb{R}^n –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è: \n\\begin{aligned}\n\\text{Strongly convex case } \\mu &gt;0 & &\\langle \\nabla f(x) - \\nabla f(y), x - y \\rangle &\\geq \\mu \\|x - y\\|^2 \\\\\n\\text{Convex case } \\mu = 0 & &\\langle \\nabla f(x) - \\nabla f(y), x - y \\rangle &\\geq 0\n\\end{aligned}\n\n–ü—É—Å—Ç—å r : \\mathbb{R}^n \\rightarrow \\mathbb{R} \\cup \\{+\\infty\\} - –≤—ã–ø—É–∫–ª–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –¥–ª—è –∫–æ—Ç–æ—Ä–æ–π \\text{prox}_r –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞. –¢–æ–≥–¥–∞ –¥–ª—è –ª—é–±—ã—Ö x, y \\in \\mathbb{R}^n —Å–ª–µ–¥—É—é—â–∏–µ —Ç—Ä–∏ —É—Å–ª–æ–≤–∏—è —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã:\n\n\\text{prox}_r(x) = y,\nx - y \\in \\partial r(y),\n\\langle x - y, z - y \\rangle \\leq r(z) - r(y) –¥–ª—è –ª—é–±–æ–≥–æ z \\in \\mathbb{R}^n.\n\n–î–ª—è –Ω–∞—á–∞–ª–∞ —É—Å—Ç–∞–Ω–æ–≤–∏–º —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å –ø–µ—Ä–≤–æ–≥–æ –∏ –≤—Ç–æ—Ä–æ–≥–æ —É—Å–ª–æ–≤–∏—è. –ü–µ—Ä–≤–æ–µ —É—Å–ª–æ–≤–∏–µ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å –∫–∞–∫: \ny = \\arg \\min_{\\tilde{x} \\in \\mathbb{R}^d} \\left( r(\\tilde{x}) + \\frac{1}{2} \\| x - \\tilde{x} \\|^2 \\right).\n –ò–∑ —É—Å–ª–æ–≤–∏–π –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—ã–ø—É–∫–ª–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ r, —ç—Ç–æ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ: \n0 \\in \\left.\\partial \\left( r(\\tilde{x}) + \\frac{1}{2} \\| x - \\tilde{x} \\|^2 \\right)\\right|_{\\tilde{x} = y} = \\partial r(y) + y - x.\n –ò–∑ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—É–±–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª–∞, –¥–ª—è –ª—é–±–æ–≥–æ —Å—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ g \\in \\partial f(y) –∏ –¥–ª—è –ª—é–±–æ–≥–æ z \\in \\mathbb{R}^d: \n\\langle g, z - y \\rangle \\leq r(z) - r(y).\n –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, —ç—Ç–æ –≤–µ—Ä–Ω–æ –¥–ª—è g = x - y. –û–±—Ä–∞—Ç–Ω–æ, —ç—Ç–æ —Ç–∞–∫–∂–µ –æ—á–µ–≤–∏–¥–Ω–æ –¥–ª—è g = x - y, –≤—ã—à–µ—É–∫–∞–∑–∞–Ω–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è, —á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç g \\in \\partial r(y).\n\n–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ\n\n–°–Ω–∞—á–∞–ª–∞ –¥–æ–∫–∞–∂–µ–º —Å–≤–æ–π—Å—Ç–≤–æ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞: \\text{prox}_S(x^* - \\alpha \\nabla f(x^*)) = x^*.\n–ü—É—Å—Ç—å f: \\mathbb{R}^n \\rightarrow \\mathbb{R} \\cup \\{+\\infty\\} –∏ r: \\mathbb{R}^n \\rightarrow \\mathbb{R} \\cup \\{+\\infty\\} - –≤—ã–ø—É–∫–ª—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –ø—É—Å—Ç—å f –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞ –∏ L-–≥–ª–∞–¥–∫–∞—è, –∏ –¥–ª—è r, \\text{prox}_r –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞. –¢–æ–≥–¥–∞ x^* —è–≤–ª—è–µ—Ç—Å—è —Ä–µ—à–µ–Ω–∏–µ–º –∑–∞–¥–∞—á–∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ç–æ–≥–¥–∞ –∏ —Ç–æ–ª—å–∫–æ —Ç–æ–≥–¥–∞, –∫–æ–≥–¥–∞ –¥–ª—è –ª—é–±–æ–≥–æ \\alpha &gt; 0 –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è: \nx^* = \\text{prox}_{\\alpha r}(x^* - \\alpha \\nabla f(x^*))\n –£—Å–ª–æ–≤–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç–∏: \n\\begin{aligned}\n0 \\in & \\nabla f(x^*) + \\partial r(x^*) \\\\\n- \\alpha \\nabla f(x^*) \\in & \\alpha \\partial r(x^*) \\\\\nx^* - \\alpha \\nabla f(x^*) - x^* \\in & \\alpha \\partial r(x^*)\n\\end{aligned}\n –ò–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –ª–µ–º–º—ã: \n\\text{prox}_r(x) = y \\Leftrightarrow x - y \\in \\partial r(y)\n –ò, –Ω–∞–∫–æ–Ω–µ—Ü, \nx^* = \\text{prox}_{\\alpha r}(x^* - \\alpha \\nabla f(x^*)) = \\text{prox}_{r, \\alpha}(x^* - \\alpha \\nabla f(x^*))\n\n–†–∞—Å—Å–º–æ—Ç—Ä–∏–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Ä–µ—à–µ–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–≤–æ–π—Å—Ç–≤–æ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç–∏: \n\\begin{aligned}\n\\|x_{k+1} - x^*\\|^2_2 &= \\|\\text{prox}_S (x_k - \\alpha \\nabla f (x_k)) - x^*\\|^2_2 \\\\\n\\text{stationary point property} & = \\|\\text{prox}_S (x_k - \\alpha \\nabla f (x_k)) - \\text{prox}_S (x^* - \\alpha \\nabla f (x^*)) \\|^2_2 \\\\\n\\text{nonexpansiveness} & \\leq \\|x_k - \\alpha \\nabla f (x_k) - (x^* - \\alpha \\nabla f (x^*)) \\|^2_2 \\\\\n& =  \\|x_k - x^*\\|^2 - 2\\alpha \\langle \\nabla f(x_k) - \\nabla f(x^*), x_k - x^* \\rangle + \\alpha^2 \\|\\nabla f(x_k) - \\nabla f(x^*)\\|^2_2\n\\end{aligned}\n\n–†–∞—Å—Å–º–æ—Ç—Ä–∏–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Ä–µ—à–µ–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–≤–æ–π—Å—Ç–≤–æ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç–∏: \n\\begin{aligned}\n\\|x_{k+1} - x^*\\|^2_2 &= \\|\\text{prox}_S (x_k - \\alpha \\nabla f (x_k)) - x^*\\|^2_2 \\\\\n{\\scriptsize \\text{stationary point property}}  & = \\|\\text{prox}_S (x_k - \\alpha \\nabla f (x_k)) - \\text{prox}_S (x^* - \\alpha \\nabla f (x^*)) \\|^2_2 \\\\\n{\\scriptsize \\text{nonexpansiveness}}   & \\leq \\|x_k - \\alpha \\nabla f (x_k) - (x^* - \\alpha \\nabla f (x^*)) \\|^2_2 \\\\\n& =  \\|x_k - x^*\\|^2 - 2\\alpha \\langle \\nabla f(x_k) - \\nabla f(x^*), x_k - x^* \\rangle + \\alpha^2 \\|\\nabla f(x_k) - \\nabla f(x^*)\\|^2_2\n\\end{aligned}\n\n–ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–∞–¥–∫–æ—Å—Ç—å –∏ —Å–∏–ª—å–Ω—É—é –≤—ã–ø—É–∫–ª–æ—Å—Ç—å: \n\\begin{aligned}\n\\text{smoothness} \\;\\; &\\|\\nabla f(x_k)-\\nabla f (x^*)\\|_2^2 \\leq 2L\\left(f(x_k)-f(x^*)-\\langle\\nabla f (x^*),x_k -x^*\\rangle \\right) \\\\\n\\text{strong convexity} \\;\\; & - \\langle \\nabla f(x_k) -  \\nabla f(x^*), x_k - x^* \\rangle \\leq \\\\\n& - \\left(f(x_k) - f(x^*) + \\frac{\\mu}{2} \\| x_k - x^* \\|^2_2 \\right) - \\langle \\nabla f(x^*), x_k - x^* \\rangle\n\\end{aligned}\n\n–ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º: \n\\begin{aligned}\n\\|x_{k+1} - x^*\\|^2_2 &\\leq \\|x_k - x^*\\|^2 - 2\\alpha \\left(f(x_k) - f(x^*) + \\frac{\\mu}{2} \\| x_k - x^* \\|^2_2 \\right) - 2\\alpha \\langle \\nabla f(x^*), x_k - x^* \\rangle + \\\\\n& + \\alpha^2 2L\\left(f(x_k)-f(x^*)-\\langle\\nabla f (x^*),x_k -x^*\\rangle \\right)  \\\\\n&\\leq (1 - \\alpha \\mu)\\|x_k - x^*\\|^2 + 2\\alpha (\\alpha L - 1) \\left( f(x_k) - f(x^*) - \\langle \\nabla f(x^*), x_k - x^* \\rangle \\right)\n\\end{aligned}\n\n–ò–∑ –≤—ã–ø—É–∫–ª–æ—Å—Ç–∏ f: f(x_k) - f(x^*) - \\langle \\nabla f(x^*), x_k - x^* \\rangle \\geq 0. –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –µ—Å–ª–∏ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º \\alpha \\leq \\frac1L: \n\\begin{aligned}\n\\|x_{k+1} - x^*\\|^2_2 &\\leq (1 - \\alpha \\mu)\\|x_k - x^*\\|^2 \\\\\n\\|x_k - x^*\\|_2^2 &\\leq (1 - \\alpha \\mu)^k \\|x_0 - x^*\\|_2^2\n\\end{aligned}\n\n\n–¢–µ–æ—Ä–µ–º–∞ –æ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –≤ –≥–ª–∞–¥–∫–æ–º PL-—Å–ª—É—á–∞–µ.\n\n\n\n\n\n\n–ü—É—Å—Ç—å f ‚Äî L-–≥–ª–∞–¥–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è, —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—â–∞—è —É—Å–ª–æ–≤–∏—é –ü–æ–ª—è–∫–∞-–õ–æ—è—Å–∏–µ–≤–∏—á–∞ (PL) —Å –∫–æ–Ω—Å—Ç–∞–Ω—Ç–æ–π \\mu&gt;0, –∞ –¥–∏—Å–ø–µ—Ä—Å–∏—è —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞: \\mathbb{E}[\\|\\nabla f_i(x_k)\\|^2] \\leq \\sigma^2. –¢–æ–≥–¥–∞ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ —Å —É–±—ã–≤–∞—é—â–∏–º —à–∞–≥–æ–º \\alpha_k = \\frac{2k + 1 }{ 2\\mu(k+1)^2} –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç \n\\mathbb{E}[f(x_{k}) - f^*] \\leq \\frac{L \\sigma^2}{ 2 \\mu^2 k}\n\n\n\n\n–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ.\n\n–ò—Å–ø–æ–ª—å–∑—É–µ–º L-–≥–ª–∞–¥–∫–æ—Å—Ç—å: \nf(x_{k+1}) \\leq f(x_k) + \\langle \\nabla f(x_k), x_{k+1} - x_k \\rangle + \\frac{L}{2} \\|x_{k+1}-x_k\\|^2\n\n–ò—Å–ø–æ–ª—å–∑—É—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –º–µ—Ç–æ–¥–∞ (\\text{SGD}): \nf(x_{k+1}) \\leq f(x_k) - \\alpha_k \\langle \\nabla f(x_k),  \\nabla f_{i_k}(x_k)\\rangle + \\alpha_k^2\\frac{L}{2} \\|\\nabla f_{i_k}(x_k)\\|^2\n\n–í–æ–∑—å–º–µ–º –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –ø–æ i_k: \n\\mathbb{E}[f(x_{k+1})] \\leq \\mathbb{E}[f(x_k) - \\alpha_k \\langle \\nabla f(x_k),  \\nabla f_{i_k}(x_k)\\rangle + \\alpha_k^2\\frac{L}{2} \\|\\nabla f_{i_k}(x_k)\\|^2]\n\n–ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–∂–∏–¥–∞–Ω–∏—è: \n\\mathbb{E}[f(x_{k+1})] \\leq f(x_k) - \\alpha_k \\langle \\nabla f(x_k),  \\mathbb{E}[\\nabla f_{i_k}(x_k)]\\rangle + \\alpha_k^2\\frac{L}{2} \\mathbb{E}[\\|\\nabla f_{i_k}(x_k)\\|^2]\n\n–¢–∞–∫ –∫–∞–∫ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ –≤—ã–±–æ—Ä–∫–∞ –æ–∑–Ω–∞—á–∞–µ—Ç –Ω–µ—Å–º–µ—â–µ–Ω–Ω—É—é –æ—Ü–µ–Ω–∫—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞: \\mathbb{E}[\\nabla f_{i_k}(x_k)] = \\nabla f(x_k): \n\\mathbb{E}[f(x_{k+1})] \\leq f(x_k) - \\alpha_k \\|\\nabla f(x_k)\\|^2 + \\alpha_k^2\\frac{L}{2} \\mathbb{E}[\\|\\nabla f_{i_k}(x_k)\\|^2]\n\n–ò—Å–ø–æ–ª—å–∑—É–µ–º PL-—É—Å–ª–æ–≤–∏–µ: \n\\begin{aligned}\n\\mathbb{E}[f(x_{k+1})] &\\leq f(x_k) - \\alpha_k \\|\\nabla f(x_k)\\|^2 + \\alpha_k^2\\frac{L}{2} \\mathbb{E}[\\|\\nabla f_{i_k}(x_k)\\|^2] \\\\\n\\stackrel{\\text{PL: } \\|\\nabla f(x_k)\\|^2 \\geq 2\\mu(f(x_k) - f^*)}{ } \\;\\; &\\leq f(x_k) - 2\\alpha_k \\mu (f(x_k) - f^*) + \\alpha_k^2\\frac{L}{2} \\mathbb{E}[\\|\\nabla f_{i_k}(x_k)\\|^2] \\\\\n{\\stackrel{\\text{–í—ã—á–∏—Ç–∞–µ–º } f^*}{ }} \\;\\; &\\leq (f(x_k) - f^*) - 2\\alpha_k \\mu (f(x_k) - f^*) + \\alpha_k^2\\frac{L}{2} \\mathbb{E}[\\|\\nabla f_{i_k}(x_k)\\|^2] \\\\\n{\\scriptsize \\text{–ü–µ—Ä–µ–≥—Ä—É–ø–ø–∏—Ä—É–µ–º}} \\;\\; &\\leq (1 - 2\\alpha_k \\mu) [f(x_k) - f^*] + \\alpha_k^2\\frac{L}{2} \\mathbb{E}[\\|\\nabla f_{i_k}(x_k)\\|^2] \\\\\n\\stackrel{\\text{–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç—å –¥–∏—Å–ø–µ—Ä—Å–∏–∏: } \\mathbb{E}[\\|\\nabla f_i(x_k)\\|^2] \\leq \\sigma^2}{ } \\;\\; &\\leq (1 - 2\\alpha_k \\mu)[f(x_{k}) - f^*] + \\frac{L \\sigma^2 \\alpha_k^2 }{2}.\n\\end{aligned}\n\n–†–∞—Å—Å–º–æ—Ç—Ä–∏–º —É–±—ã–≤–∞—é—â–∏–π —à–∞–≥ —Å \\alpha_k = \\frac{2k + 1 }{ 2\\mu(k+1)^2} –º—ã –ø–æ–ª—É—á–∞–µ–º: \n\\begin{aligned}\n\\stackrel{1-2\\alpha_k \\mu = \\frac{(k+1)^2}{(k+1)^2} - \\frac{2k + 1 }{(k+1)^2} = \\frac{k^2 }{ (k+1)^2}}{ }\\;\\;\\mathbb{E}[f(x_{k+1}) - f^*] &\\leq \\frac{k^2 }{ (k+1)^2}[f(x_{k}) - f^*]  + \\frac{L \\sigma^2 (2k+1)^2}{ 8 \\mu^2 (k+1)^4} \\\\\n\\stackrel{(2k+1)^2 &lt; (2k + 2)^2 = 4(k+1)^2}{ } \\;\\; &\\leq\\frac{k^2 }{ (k+1)^2}[f(x_{k}) - f^*]  + \\frac{L \\sigma^2}{ 2 \\mu^2 (k+1)^2}\n\\end{aligned}\n\n–£–º–Ω–æ–∂–∞–µ–º –æ–±–µ —Å—Ç–æ—Ä–æ–Ω—ã –Ω–∞ (k+1)^2 –∏ –ø—É—Å—Ç—å \\delta_f(k) \\equiv k^2 \\mathbb{E}[f(x_{k}) - f^*] –º—ã –ø–æ–ª—É—á–∞–µ–º: \n\\begin{aligned}\n(k+1)^2 \\mathbb{E}[f(x_{k+1}) - f^*] &\\leq k^2\\mathbb{E}[f(x_{k}) - f^*]  + \\frac{L\\sigma^2 }{ 2 \\mu^2} \\\\\n\\delta_f(k+1) &\\leq \\delta_f(k)  + \\frac{L\\sigma^2 }{ 2 \\mu^2}.\n\\end{aligned}\n\n–°—É–º–º–∏—Ä—É–µ–º –ø—Ä–µ–¥—ã–¥—É—â–µ–µ –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –æ—Ç i=0 –¥–æ k –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç —Ñ–∞–∫—Ç, —á—Ç–æ \\delta_f(0) = 0 –º—ã –ø–æ–ª—É—á–∞–µ–º: \n\\begin{aligned}\n\\delta_f(i+1) &\\leq \\delta_f(i)  + \\frac{L\\sigma^2 }{ 2 \\mu^2} \\\\\n\\sum_{i=0}^k \\left[ \\delta_f(i+1) - \\delta_f(i) \\right] &\\leq \\sum_{i=0}^k \\frac{L\\sigma^2 }{ 2 \\mu^2} \\\\\n\\delta_f(k+1) - \\delta_f(0)  &\\leq \\frac{L \\sigma^2 (k+1)}{ 2 \\mu^2} \\\\\n(k+1)^2 \\mathbb{E}[f(x_{k+1}) - f^*] &\\leq \\frac{L \\sigma^2 (k+1)}{ 2 \\mu^2} \\\\\n\\mathbb{E}[f(x_{k}) - f^*] &\\leq \\frac{L \\sigma^2}{ 2 \\mu^2 k}\n\\end{aligned}\n\n\n–¢–µ–æ—Ä–µ–º–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –¥–ª—è –≤—ã–ø—É–∫–ª—ã—Ö –∏ PL-—Ñ—É–Ω–∫—Ü–∏–π.\n\n\n\n\n\n\n–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –∑–∞–¥–∞—á—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ f(x(t)) \\rightarrow \\min\\limits_{\\substack{x(t) \\in \\mathbb{R}^d \\\\ t \\in T}}, —Ñ—É–Ω–∫—Ü–∏–∏ f(x) –∏ x(t) –≥–ª–∞–¥–∫–∏–µ, f - –≤—ã–ø—É–∫–ª–∞—è –∏ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è–µ—Ç —É—Å–ª–æ–≤–∏—é PL, —Ç.–µ. \\forall x \\mapsto \\|\\nabla f(x)|^2 \\geqslant 2\\mu (f(x) - f^*), —Ç–æ–≥–¥–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –ø–æ—Ç–æ–∫ —Å—Ö–æ–¥–∏—Ç—Å—è –ª–∏–Ω–µ–π–Ω–æ, –∞ –∏–º–µ–Ω–Ω–æ, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ f(x(t)) - f^* \\leqslant \\exp\\{-2\\mu t\\}\\left(f(x(0)) - f^*\\right).\n\n\n\n–ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —É –Ω–∞—Å –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —É—Å–ª–æ–≤–∏–µ –ü–æ–ª—è–∫–∞-–õ–æ—è—Å–µ–≤–∏—á–∞ \\|\\nabla f(x)\\|^2 \\geqslant 2\\mu (f(x) - f^*), —Ç–æ–≥–¥–∞  \\frac{d}{dt}\\left(f(x(t)) - f(x^*)\\right) = \\nabla f(x(t))^T\\underbrace{\\dot{x}(t)}_{= \\frac{dx}{dt} = -\\nabla f(x)} = -\\|\\nabla f(x(t))|^2_2 \\leqslant -2\\mu \\left(f(x(t)) - f^*\\right)  –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É—è, –ø–æ–ª—É—á–∞–µ–º:  f(x(t)) - f^* \\leqslant \\exp\\{-2\\mu t\\}\\left(f(x(0)) - f^*\\right)"
  },
  {
    "objectID": "Notebooks/polynoms.html",
    "href": "Notebooks/polynoms.html",
    "title": "",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\n\ndef p(k, x, mu=1.0, L=10.0):\n    return np.power(1.0 - 2.0*x/(mu+L), k)\n\nmu, L = 1.0, 10.0\nxs = np.linspace(0, L, 100)\n\n# Create plots for degrees 1 through 6, adding one polynomial at a time\nfor max_degree in range(2, 7):\n    plt.figure(figsize=(4, 4.1))\n    plt.ylim([-0.6, 1.0])\n    plt.title(f'Naive polynomials up to degree {max_degree}')\n    \n    # Plot polynomials from degree 1 up to max_degree\n    for degree in range(2, max_degree + 1):\n        line, = plt.plot(xs, p(degree, xs), label=f'$p_{degree}(a)$')\n        # Use the same color for the upper bound line\n        plt.plot(xs, [p(degree, mu)]*len(xs), '--', color=line.get_color(), alpha=0.3)\n    \n    # Add vertical lines for mu and L\n    plt.axvline(x=mu, color='grey', linestyle='--', alpha=0.5)\n    plt.axvline(x=L, color='grey', linestyle='--', alpha=0.5)\n    \n    # Add text labels for mu and L\n    plt.text(mu-0.3, plt.ylim()[0]+0.1, 'Œº', horizontalalignment='right', verticalalignment='bottom')\n    plt.text(L-0.3, plt.ylim()[0]+0.1, 'L', horizontalalignment='right', verticalalignment='bottom')\n    \n    plt.legend()\n    plt.grid(linestyle=':')\n    plt.tight_layout()\n    plt.savefig(f'gd_polynom_{max_degree}.pdf')\n    plt.close()  # Close the figure to free memory\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef T(k, a):\n    \"\"\"Chebyshev polynomial of degree k\"\"\"\n    if k &lt;= 1:\n        return a**k\n    else:\n        return 2.0*a*T(k-1, a) - T(k-2, a)\n\nxs = np.linspace(-1, 1, 100)\n\n# Create plots for degrees 1 through 6, adding one polynomial at a time\nfor max_degree in range(1, 6):\n    plt.figure(figsize=(4, 4.1))\n    plt.ylim([-1, 1.0])\n    plt.title(f'Chebyshev polynomials up to degree {max_degree}')\n    \n    # Plot polynomials from degree 1 up to max_degree\n    for degree in range(1, max_degree + 1):\n        line, = plt.plot(xs, T(degree, xs), label=f'$p_{degree}(a)$')\n        # Use the same color for the upper bound line\n        # plt.plot(xs, [p(degree, mu)]*len(xs), '--', color=line.get_color(), alpha=0.3)\n    \n    # Add vertical lines for mu and L\n    # plt.axvline(x=mu, color='grey', linestyle='--', alpha=0.5)\n    # plt.axvline(x=L, color='grey', linestyle='--', alpha=0.5)\n    \n    # Add text labels for mu and L\n    # plt.text(mu-0.3, plt.ylim()[0]+0.1, 'Œº', horizontalalignment='right', verticalalignment='bottom')\n    # plt.text(L-0.3, plt.ylim()[0]+0.1, 'L', horizontalalignment='right', verticalalignment='bottom')\n    \n    # plt.legend()\n    plt.grid(linestyle=':')\n    plt.tight_layout()\n    plt.savefig(f'gd_polynom_cheb_{max_degree}.pdf')\n    plt.close()  # Close the figure to free memory\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef p(k, x, mu=1.0, L=10.0):\n    return np.power(1.0 - 2.0*x/(mu+L), k)\n\ndef T(k, a):\n    \"\"\"Chebyshev polynomial of degree k\"\"\"\n    if k &lt;= 1:\n        return a**k\n    else:\n        return 2.0*a*T(k-1, a) - T(k-2, a)\n\ndef P(k, a, alpha=1, beta=10.0):\n    \"\"\"Rescaled Chebyshev polynomial.\"\"\"\n    assert beta &gt; alpha\n    normalization = T(k, (beta+alpha)/(beta-alpha))\n    return T(k, (beta+alpha-2*a)/(beta-alpha))/normalization\n\nmu, L = 1.0, 10.0\nxs = np.linspace(0, L, 100)\n\n# Create plots for degrees 1 through 6, adding one polynomial at a time\nfor degree in range(1, 11):\n    plt.figure(figsize=(5, 2.7))\n    plt.ylim([-0.6, 1.0])\n    plt.title(f'Polynomials of degree {degree}')\n    \n    line, = plt.plot(xs, p(degree, xs), label=f'Naive')\n    line_che, = plt.plot(xs, P(degree, xs), label=f'Chebyshev')\n    # Use the same color for the upper bound line\n    plt.plot(xs, [p(degree, mu)]*len(xs), '--', color=line.get_color(), alpha=0.3)\n    plt.plot(xs, [P(degree, mu)]*len(xs), '--', color=line_che.get_color(), alpha=0.3)\n    \n    # Add vertical lines for mu and L\n    plt.axvline(x=mu, color='grey', linestyle='--', alpha=0.5)\n    plt.axvline(x=L, color='grey', linestyle='--', alpha=0.5)\n    \n    # Add text labels for mu and L\n    plt.text(mu-0.3, plt.ylim()[0]+0.1, 'Œº', horizontalalignment='right', verticalalignment='bottom')\n    plt.text(L-0.3, plt.ylim()[0]+0.1, 'L', horizontalalignment='right', verticalalignment='bottom')\n    \n    plt.legend(loc='upper right')\n    plt.grid(linestyle=':')\n    plt.tight_layout()\n    plt.savefig(f'gd_polynoms_{degree}.pdf')\n    plt.close()  # Close the figure to free memory\n\n\nplt.figure(figsize=(14,7))\nepsilons = np.linspace(0., 0.1, 100)\nplt.xlabel('$\\epsilon$')\ndegree = 9\nplt.plot(epsilons, T(degree, 1+epsilons), \n         label=f'$T_{degree}(1+\\epsilon)$')\nplt.plot(epsilons, (1+epsilons**(1/2))**degree/2, \n         label='$(1+\\sqrt{\\epsilon})^{10}/2$')\nplt.legend();"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "",
    "section": "",
    "text": "Linear algebra basics\n\n[10 points] Effect of Diagonal Scaling on Rank Let A \\in \\mathbb{R}^{n \\times n} be a matrix with rank r. Suppose D \\in \\mathbb{R}^{n \\times n} is a diagonal matrix. Determine the rank of the product DA. Explain your reasoning.\n[20 points] Find SVD of the following matrices:\n\nA = \\begin{bmatrix} 1\\\\2\\\\3 \\end{bmatrix}\nA = \\begin{bmatrix} 1 & 4\\\\4 & 8\\\\3 & 8 \\end{bmatrix}\nA = \\begin{bmatrix} 0 & 0\\\\x & 0\\\\0 & 0 \\end{bmatrix}, where x is the sum of your birthdate numbers (day + month)\n\n[10 points] Assume we have a set of data points x^{(i)}\\in\\mathbb{R}^{n},\\,i=1,\\dots,m, and decide to represent this data as a matrix\n\nX =\n     \\begin{pmatrix}\n     | & & | \\\\\n     x^{(1)} & \\dots & x^{(m)} \\\\\n     | & & | \\\\\n     \\end{pmatrix} \\in \\mathbb{R}^{n \\times m}.\n\nWe suppose that \\text{rank}\\,X = r.\nIn the problem below, we ask you to find the rank of some matrix M related to X. In particular, you need to find relation between \\text{rank}\\,X = r and \\text{rank}\\,M, e.g., that the rank of M is always larger/smaller than the rank of X or that \\text{rank}\\,M = \\text{rank}\\,X \\big / 35. Please support your answer with legitimate arguments and make the answer as accurate as possible.\nNote that depending on the structure of the matrix X, border cases are possible. Make sure to cover them in your answer correctly.\nIn applied statistics and machine learning, data is often normalized. One particularly popular strategy is to subtract the estimated mean \\mu and divide by the square root of the estimated variance \\sigma^2. i.e.\n\nx \\rightarrow (x - \\mu) \\big / \\sigma.\n\nAfter the normalization, we get a new matrix\n\n     \\begin{split}\n     Y &:=\n     \\begin{pmatrix}\n         | & & | \\\\\n         y^{(1)} & \\dots & y^{(m)} \\\\\n         | & & | \\\\\n     \\end{pmatrix},\\\\\n     y^{(i)} &:= \\frac{x^{(i)} - \\frac{1}{m}\\sum_{j=1}^{m} x^{(j)}}{\\sqrt{\\frac{1}{m}\\sum_{j=1}^{m} \\left(x^{(j)}\\right)^2 - \\left(\\frac{1}{m}\\sum_{j=1}^{m} x^{(j)}\\right)^2}}.\n     \\end{split}\n\nWhat is the rank of Y if \\text{rank} \\; X = r?\nImage Compression with Truncated SVD [10 points] Explore image compression using Truncated Singular Value Decomposition (SVD). Understand how varying the number of singular values affects the quality of the compressed image. Implement a Python script to compress a grayscale image using Truncated SVD and visualize the compression quality.\n\nTruncated SVD: Decomposes an image A into U, S, and V matrices. The compressed image is reconstructed using a subset of singular values.\nMathematical Representation: \nA \\approx U_k \\Sigma_k V_k^T\n\n\nU_k and V_k are the first k columns of U and V, respectively.\n\\Sigma_k is a diagonal matrix with the top k singular values.\nRelative Error: Measures the fidelity of the compressed image compared to the original.\n\n\n  \\text{Relative Error} = \\frac{\\| A - A_k \\|}{\\| A \\|}\n  \n\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np\nfrom skimage import io, color\nimport requests\nfrom io import BytesIO\n\ndef download_image(url):\n    response = requests.get(url)\n    img = io.imread(BytesIO(response.content))\n    return color.rgb2gray(img)  # Convert to grayscale\n\ndef update_plot(i, img_plot, error_plot, U, S, V, original_img, errors, ranks, ax1, ax2):\n    # Adjust rank based on the frame index\n    if i &lt; 70:\n        rank = i + 1\n    else:\n        rank = 70 + (i - 69) * 10\n\n\n    reconstructed_img = ... # YOUR CODE HERE \n\n    # Calculate relative error\n    relative_error = ... # YOUR CODE HERE\n    errors.append(relative_error)\n    ranks.append(rank)\n\n    # Update the image plot and title\n    img_plot.set_data(reconstructed_img)\n    ax1.set_title(f\"Image compression with SVD\\n Rank {rank}; Relative error {relative_error:.2f}\")\n\n    # Remove axis ticks and labels from the first subplot (ax1)\n    ax1.set_xticks([])\n    ax1.set_yticks([])\n\n    # Update the error plot\n    error_plot.set_data(ranks, errors)\n    ax2.set_xlim(1, len(S))\n    ax2.grid(linestyle=\":\")\n    ax2.set_ylim(1e-4, 0.5)\n    ax2.set_ylabel('Relative Error')\n    ax2.set_xlabel('Rank')\n    ax2.set_title('Relative Error over Rank')\n    ax2.semilogy()\n\n    # Set xticks to show rank numbers\n    ax2.set_xticks(range(1, len(S)+1, max(len(S)//10, 1)))  # Adjust the step size as needed\n    plt.tight_layout()\n\n    return img_plot, error_plot\n\n\ndef create_animation(image, filename='svd_animation.mp4'):\n    U, S, V = np.linalg.svd(image, full_matrices=False)\n    errors = []\n    ranks = []\n\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 8))\n    img_plot = ax1.imshow(image, cmap='gray', animated=True)\n    error_plot, = ax2.plot([], [], 'r-', animated=True)  # Initial empty plot for errors\n\n    # Add watermark\n    ax1.text(1, 1.02, '@fminxyz', transform=ax1.transAxes, color='gray', va='bottom', ha='right', fontsize=9)\n\n    # Determine frames for the animation\n    initial_frames = list(range(70))  # First 70 ranks\n    subsequent_frames = list(range(70, len(S), 10))  # Every 10th rank after 70\n    frames = initial_frames + subsequent_frames\n\n    ani = animation.FuncAnimation(fig, update_plot, frames=len(frames), fargs=(img_plot, error_plot, U, S, V, image, errors, ranks, ax1, ax2), interval=50, blit=True)\n    ani.save(filename, writer='ffmpeg', fps=8, dpi=300)\n\n# URL of the image\nurl = \"\"\n\n# Download the image and create the animation\nimage = download_image(url)\ncreate_animation(image)\n\n\n\nMatrix calculus\n\n[10 points] Given a matrix A of size m \\times n and a vector x of size n \\times 1, compute the gradient of the function f(x) = \\text{tr}(A^T A x x^T) with respect to x.\n[10 points] Find the gradient \\nabla f(x) and hessian f''(x), if f(x) = \\dfrac{1}{2} \\Vert Ax - b\\Vert^2_2.\n[10 points] Find the gradient \\nabla f(x) and hessian f''(x), if \nf(x) = \\frac1m \\sum\\limits_{i=1}^m \\log \\left( 1 + \\exp(a_i^{T}x) \\right) + \\frac{\\mu}{2}\\Vert x\\Vert _2^2, \\; a_i, x \\in \\mathbb R^n, \\; \\mu&gt;0\n\n[10 points] Compute the gradient \\nabla_A f(A) of the trace of the matrix exponential function f(A) = \\text{tr}(e^A) with respect to A. Hint: hint: Use the definition of the matrix exponential. Use the definition of the differential df = f(A + dA) - f(A) + o(\\Vert dA \\Vert) with the limit \\Vert dA \\Vert \\to 0.\n[10 points] Calculate the first and the second derivative of the following function f : S \\to \\mathbb{R}\n\nf(t) = \\text{det}(A ‚àí tI_n),\n\n\n\n\nAutomatic differentiation\n\n[10 points] You will work with the following function for this exercise, \nf(x,y)=e^{‚àí\\left(sin(x)‚àícos(y)\\right)^2}\n\nDraw the computational graph for the function. Note, that it should contain only primitive operations - you need to do it automatically - jax example, PyTorch example - you can google/find your way to visualize it.\n[10 points] Compare analytic and autograd (with any framework) approach for the calculation of the gradient of:\n\nf(A) = \\text{tr}(e^A)\n\n[10 points] We can use automatic differentiation not only to calculate necessary gradients but also for tuning hyperparameters of the algorithm like learning rate in gradient descent (with gradient descent ü§Ø). Suppose, we have the following function f(x) = \\frac{1}{2}\\Vert x\\Vert^2, select a random point x_0 \\in \\mathbb{B}^{1000} = \\{0 \\leq x_i \\leq 1 \\mid \\forall i\\}. Consider 10 steps of the gradient descent starting from the point x_0: \nx_{k+1} = x_k - \\alpha_k \\nabla f(x_k)\n Your goal in this problem is to write the function, that takes 10 scalar values \\alpha_i and return the result of the gradient descent on function L = f(x_{10}). And optimize this function using gradient descent on \\alpha \\in \\mathbb{R}^{10}. Suppose that each of 10 components of \\alpha is uniformly distributed on [0; 0.1]. \n\\alpha_{k+1} = \\alpha_k - \\beta \\frac{\\partial L}{\\partial \\alpha}\n Choose any constant \\beta and the number of steps you need. Describe the obtained results. How would you understand, that the obtained schedule (\\alpha \\in \\mathbb{R}^{10}) becomes better than it was at the start? How do you check numerically local optimality in this problem?\n[10 points] Compare analytic and autograd (with any framework) approach for the gradient of:\n\nf(X) = - \\log \\det X\n\n\n\n\nConvexity\n\n[5 points] The center of mass of a body is an important concept in physics (mechanics). Obviously, the center of mass of a body does not always lie inside the body. For example, the center of mass of a doughnut is located in its hole. Prove that the center of mass of a system of material points lies in the convex hull of the set of these points.\n[10 points] Show, that \\mathbf{conv}\\{xx^\\top: x \\in \\mathbb{R}^n, \\Vert x\\Vert  = 1\\} = \\{A \\in \\mathbb{S}^n_+: \\text{tr}(A) = 1\\}.\n[5 points] Prove that the set of \\{x \\in \\mathbb{R}^2 \\mid e^{x_1}\\le x_2\\} is convex.\n[5 points] Show that the set of directions of the non-strict local descending of the differentiable function in a point is a convex cone. We assume, that \\nabla f(x_0) \\neq 0 at the target point.\n[10 points] Is the following set convex \nS = \\left\\{ a \\in \\mathbb{R}^k \\mid p(0) = 1, \\vert p(t) \\vert\\leq 1 \\text{ for } \\alpha\\leq t \\leq \\beta\\right\\},\n where \np(t) = a_1 + a_2 t + \\ldots + a_k t^{k-1} \\;?\n\n[10 points] Consider the function f(x) = x^d, where x \\in \\mathbb{R}_{+}. Fill the following table with ‚úÖ or ‚ùé. Explain your answers\n\n\n\n\nd\nConvex\nConcave\nStrictly Convex\n\\mu-strongly convex\n\n\n\n\n-2, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n-1, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0.5\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\\in (1; 2)\n\n\n\n\n\n\n2\n\n\n\n\n\n\n&gt; 2\n\n\n\n\n\n\n\n\n[10 points] Prove that the entropy function, defined as\n\nf(x) = -\\sum_{i=1}^n x_i \\log(x_i),\n\nwith \\text{dom}(f) = \\{x \\in \\R^n_{++} : \\sum_{i=1}^n x_i = 1\\}, is strictly concave.\n[10 points] Show, that the function f: \\mathbb{R}^n_{++} \\to \\mathbb{R} is convex if f(x) = - \\prod\\limits_{i=1}^n x_i^{\\alpha_i} if \\mathbf{1}^T \\alpha = 1, \\alpha \\succeq 0.\n[10 points] Show that the maximum of a convex function f over the polyhedron P = \\text{conv}\\{v_1, \\ldots, v_k\\} is achieved at one of its vertices, i.e.,\n\n\\sup_{x \\in P} f(x) = \\max_{i=1, \\ldots, k} f(v_i).\n\nA stronger statement is: the maximum of a convex function over a closed bounded convex set is achieved at an extreme point, i.e., a point in the set that is not a convex combination of any other points in the set. (you do not have to prove it). Hint: Assume the statement is false, and use Jensen‚Äôs inequality.\n[10 points] Show, that the two definitions of \\mu-strongly convex functions are equivalent:\n\nf(x) is \\mu-strongly convex \\iff for any x_1, x_2 \\in S and 0 \\le \\lambda \\le 1 for some \\mu &gt; 0:\n\nf(\\lambda x_1 + (1 - \\lambda)x_2) \\le \\lambda f(x_1) + (1 - \\lambda)f(x_2) - \\frac{\\mu}{2} \\lambda (1 - \\lambda)\\|x_1 - x_2\\|^2\n\nf(x) is \\mu-strongly convex \\iff if there exists \\mu&gt;0 such that the function f(x) - \\dfrac{\\mu}{2}\\Vert x\\Vert^2 is convex.\n\n\n\n\n\nConjugate sets\n\n[5 points] Let \\mathbb{A}_n be the set of all n dimensional antisymmetric matrices (s.t. X^T = - X). Show that \\left( \\mathbb{A}_n\\right)^* = \\mathbb{S}_n.\n[10 points] Find the sets S^{*}, S^{**}, S^{***}, if\n\nS = \\{ x \\in \\mathbb{R}^2 \\mid 2x_1 + x_2 \\ge -2, \\;\\; x_1 - 2 x_2 \\le 4, \\;\\;  x_2 \\ge 0 \\;\\; x_1 + x_2 \\ge -3\\}\n\n[10 points] Find the conjugate set to the ellipsoid:\n\n  S = \\left\\{ x \\in \\mathbb{R}^n \\mid \\sum\\limits_{i = 1}^n a_i^2 x_i^2 \\le \\varepsilon^2 \\right\\}\n\n\n\n\nConjugate functions\n\n[5 points] Find f^*(y), if f(x) = \\vert \\frac12x \\vert\n[10 points] Find f^*(y), if f(x) = \\log \\left( \\sum\\limits_{i=1}^n e^{x_i} \\right)\n[10 points] Prove, that if f(x) = g(Ax), then f^*(y) = g^*(A^{-\\top}y)\n[15 points] Find f^*(Y), if f(X) = - \\ln \\det X, X \\in \\mathbb{S}^n_{++}\n[15 points] The scalar Huber function is defined as\n\nf_{\\text{hub}}(x) =\n\\begin{cases}\n\\frac{1}{2} x^2 & \\text{if } |x| \\leq 1 \\\\\n|x| - \\frac{1}{2} & \\text{if } |x| &gt; 1\n\\end{cases}\n\n\n\n\nScalar case\n\n\nThis convex function arises in various applications, notably in robust estimation. This problem explores the generalizations of the Huber function to \\mathbb{R}^n. A straightforward extension to \\mathbb{R}^n is expressed as f_{\\text{hub}}(x_1) + \\ldots + f_{\\text{hub}}(x_n), yet this formulation is not circularly symmetric, that is, it‚Äôs not invariant under the transformation of x by an orthogonal matrix. A circularly symmetric extension to \\mathbb{R}^n is given by\n\nf_{\\text{cshub}}(x) = f_{\\text{hub}}(\\Vert x\\Vert )=\n\\begin{cases}\n\\frac{1}{2} \\Vert x\\Vert_2 ^2 & \\text{if } \\Vert x\\Vert_2 \\leq 1 \\\\\n\\Vert x\\Vert_2 - \\frac{1}{2} & \\text{if } \\Vert x\\Vert_2 &gt; 1\n\\end{cases}\n\nwhere the subscript denotes ‚Äúcircularly symmetric Huber function‚Äù. Show, that f_{\\text{cshub}} is convex. Find the conjugate function f^*(y).\n[15 points] Prove that f^{**} is the pointwise maximum of all affine functions that underestimate f, i.e., \nf^{**}(x) = \\max \\{ g(x) : \\text{$g$ is affine, $g \\leq f$}\\}.\n\n[15 points] Derive the conjugate function of f(\\theta)=\\sum_{i=1}^n \\log(1+e^{-y_i\\theta_i}), where y_i \\in \\{-1,1\\}.\n\n\n\nSubgradients\n\n[5 points] Find \\partial f(x), if \nf(x) = \\text{Parametric ReLU}(x) = \\begin{cases}\n     x & \\text{if } x &gt; 0, \\\\\n     ax & \\text{otherwise}.\n\\end{cases}\n\n[10 points] Find \\partial f(x), if f(x) = \\Vert Ax - b\\Vert _1.\n[10 points] Find \\partial f(x), if f(x) = e^{\\Vert x\\Vert}.\n[10 points] Find \\partial f(x), if f(x) = \\frac12 \\Vert Ax - b\\Vert _2^2 + \\lambda \\Vert x\\Vert_1, \\quad \\lambda &gt; 0.\n[5 points] Let S \\subseteq \\mathbb{R}^n be a convex set. We will call a normal cone of the set S at a point x the following set: \nN_S(x) = \\left\\{c \\in \\mathbb{R}^n : \\langle c, y-x\\rangle \\leq 0 \\quad \\forall y \\in S\\right\\}\n\n\nDraw a normal cone for a set at the points A, B, C, D, E, F on the figure below:\n\n\n\nDraw a normal cone for the set S in these points\n\n\nShow, that N_S(x) = \\{0\\} \\quad \\forall x \\in \\mathbf{i }(S).\n\n[15 points] For f(X) = \\|X\\|_{\\text{tr}}, show that subgradients at X=U \\Sigma V^T (this is an SVD of X) satisfy \n\\partial f(X) \\supseteq \\{UV^T + W : \\|W\\|_{\\mathrm{op}} \\leq 1, \\;\nU^T W = 0, \\; WV = 0 \\}.\n Hint: you may use the fact that \\|\\cdot\\|_{\\text{tr}} and \\|\\cdot\\|_{\\mathrm{op}} are dual norms, which implies \\langle A, B \\rangle \\leq \\|A\\|_{\\text{tr}}\\|B\\|_{\\mathrm{op}} for any matrices A,B, where recall \\langle A,B \\rangle = \\text{tr}(A^T B). Bonus (5 pts): prove the other direction.\n\n\n\nOptimality Conditions. KKT\nIn this section, you can consider either the arbitrary norm or the Euclidian norm if nothing else is specified.\n\nToy example [10 points] \n\\begin{split}\n& x^2 + 1 \\to \\min\\limits_{x \\in \\mathbb{R} }\\\\\n\\text{s.t. } & (x-2)(x-4) \\leq 0\n\\end{split}\n\n\nGive the feasible set, the optimal value, and the optimal solution.\nPlot the objective x^2 +1 versus x. On the same plot, show the feasible set, optimal point, and value, and plot the Lagrangian L(x,\\mu) versus x for a few positive values of \\mu. Verify the lower bound property (p^* \\geq \\inf_x L(x, \\mu)for \\mu \\geq 0). Derive and sketch the Lagrange dual function g.\nState the dual problem, and verify that it is a concave maximization problem. Find the dual optimal value and dual optimal solution \\mu^*. Does strong duality hold?\nLet p^*(u) denote the optimal value of the problem\n\n\n\\begin{split}\n& x^2 + 1 \\to \\min\\limits_{x \\in \\mathbb{R} }\\\\\n\\text{s.t. } & (x-2)(x-4) \\leq u\n\\end{split}\n\nas a function of the parameter u. Plot p^*(u). Verify that \\dfrac{dp^*(0)}{du} = -\\mu^*\n[10 points] Give an explicit solution to the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & 1^\\top x = 1, \\\\\n& x \\succeq 0\n\\end{split}\n\nThis problem can be considered the simplest portfolio optimization problem.\n[20 points] Show, that the following problem has a unique solution and find it:\n\n\\begin{split}\n& \\langle C^{-1}, X\\rangle - \\log \\det X \\to \\min\\limits_{x \\in \\mathbb{R}^{n \\times n} }\\\\\n\\text{s.t. } & \\langle Xa, a\\rangle \\leq 1,\n\\end{split}\n\nwhere C \\in \\mathbb{S}^n_{++}, a \\in \\mathbb{R}^n \\neq 0. The answer should not involve inversion of the matrix C.\n[20 points] Derive the KKT conditions for the problem\n\n\\begin{split}\n& \\mathbf{tr \\;}X - \\log\\text{det }X \\to \\min\\limits_{X \\in \\mathbb{S}^n_{++} }\\\\\n\\text{s.t. } & Xs = y,\n\\end{split}\n\nwhere y \\in \\mathbb{R}^n and s \\in \\mathbb{R}^n are given with y^\\top s = 1. Verify that the optimal solution is given by\n\nX^* = I + yy^\\top - \\dfrac{1}{s^\\top s}ss^\\top\n\n\n\n\nDuality\n\n[10 points] Derive the dual problem for the Ridge regression problem with A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m, \\lambda &gt; 0:\n\n\\begin{split}\n\\dfrac{1}{2}\\|y-b\\|^2 + \\dfrac{\\lambda}{2}\\|x\\|^2 &\\to \\min\\limits_{x \\in \\mathbb{R}^n, y \\in \\mathbb{R}^m }\\\\\n\\text{s.t. } & y = Ax\n\\end{split}\n\n[20 points] Derive the dual problem for the support vector machine problem with A \\in \\mathbb{R}^{m \\times n}, \\mathbf{1} \\in \\mathbb{R}^m \\in \\mathbb{R}^m, \\lambda &gt; 0:\n\n\\begin{split}\n\\langle \\mathbf{1}, t\\rangle + \\dfrac{\\lambda}{2}\\|x\\|^2 &\\to \\min\\limits_{x \\in \\mathbb{R}^n, t \\in \\mathbb{R}^m }\\\\\n\\text{s.t. } & Ax \\succeq \\mathbf{1} - t \\\\\n& t \\succeq 0\n\\end{split}\n\n[20 points] Analytic centering. Derive a dual problem for\n\n-\\sum_{i=1}^m \\log (b_i - a_i^\\top x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\n\nwith domain \\{x \\mid a^\\top_i x &lt; b_i , i = [1,m]\\}.\nFirst introduce new variables y_i and equality constraints y_i = b_i ‚àí a^\\top_i x. (The solution to this problem is called the analytic center of the linear inequalities a^\\top_i x \\leq b_i ,i = [1,m]. Analytic centers have geometric applications, and play an important role in barrier methods.)\n\n\n\nLinear Programming\n\n[20 points] üì±üéßüíª Covers manufacturing. Lyzard Corp is producing covers for the following products:\n\nüì± phones\nüéß headphones\nüíª laptops\n\nThe company‚Äôs production facilities are such that if we devote the entire production to headphone covers, we can produce 5000 of them in one day. If we devote the entire production to phone covers or laptop covers, we can produce 4000 or 2000 of them in one day.\nThe production schedule is one week (6 working days), and the week‚Äôs production must be stored before distribution. Storing 1000 headphone covers (packaging included) takes up 30 cubic feet of space. Storing 1000 phone covers (packaging included) takes up 50 cubic feet of space, and storing 1000 laptop covers (packaging included) takes up 220 cubic feet of space. The total storage space available is 1500 cubic feet.\nDue to commercial agreements with Lyzard Corp has to deliver at least 4500 headphone covers and 4000 laptop covers per week to strengthen the product‚Äôs diffusion.\nThe marketing department estimates that the weekly demand for headphones covers, phone, and laptop covers does not exceed 10000 14000, and 7000 units, therefore the company does not want to produce more than these amounts for headphones, phone, and laptop covers.\nFinally, the net profit per headphone cover, phone cover, and laptop cover are $5, $7, and $12, respectively.\nThe aim is to determine a weekly production schedule that maximizes the total net profit.\n\nWrite a Linear Programming formulation for the problem. Use the following variables:\n\ny_1 = number of headphones covers produced over the week,\n\ny_2 = number of phone covers produced over the week,\n\ny_3 = number of laptop covers produced over the week.\n\nFind the solution to the problem using PyOMO\n!pip install pyomo\n! sudo apt-get install glpk-utils --quiet  # GLPK\n! sudo apt-get install coinor-cbc --quiet  # CoinOR\nPerform the sensitivity analysis. Which constraint could be relaxed to increase the profit the most? Prove it numerically.\n\n[10 points] Prove the optimality of the solution\n\nx = \\left(\\frac{7}{3} , 0, \\frac{1}{3}\\right)^T\n\nto the following linear programming problem:\n\n\\begin{split}\n& 9x_1 + 3x_2 + 7x_3 \\to \\max\\limits_{x \\in \\mathbb{R}^3 }\\\\\n\\text{s.t. } & 2x_1 + x_2 + 3x_3 \\leq 6 \\\\\n& 5x_1 + 4x_2 + x_3 \\leq 12 \\\\\n& 3x_3 \\leq 1,\\\\\n& x_1, x_2, x_3 \\geq 0\n\\end{split}\n\nbut you cannot use any numerical algorithm here.\n[5 points] Transform the following linear program into an equivalent linear program in the standard form \\left(c^\\top x \\to \\min\\limits_{x\\in \\mathbb{R}^n} : Ax = b,x ‚â• 0\\right):\n\n\\begin{split}\n& x_1‚àíx_2 \\to \\min\\limits_{x \\in \\mathbb{R}^2 }\\\\\n\\text{s.t. } & 2x_1 + x_2 \\geq 3 \\\\\n& 3x_1 ‚àí x_2 \\leq 7 \\\\\n& x_1 \\geq 0\n\\end{split}\n\n[20 points] Economic interpretation of the dual problem: Suppose a small shop makes wooden toys, where each toy train requires one piece of wood and 2 tins of paint, while each toy boat requires one piece of wood and 1 tin of paint. The profit on each toy train is \\$30, and the profit on each toy boat is \\$20. Given an inventory of 80 pieces of wood and 100 tins of paint, how many of each toy should be made to maximize the profit?\n\nWrite out the optimization problem in standard form, writing all constraints as inequalities.\nSketch the feasible set and determine p^* and x^*\nFind the dual problem, then determine d^* and \\lambda^*. Note that we can interpret the Lagrange multipliers \\lambda_k associated with the constraints on wood and paint as the prices for each piece of wood and tin of paint, so that ‚àíd^* is how much money would be obtained from selling the inventory for those prices. Strong duality says a buyer should not pay more for the inventory than what the toy store would make by producing and selling toys from it, and that the toy store should not sell the inventory for less than that.\nThe other interpretation of the Lagrange multipliers is as sensitivities to changes in the constraints. Suppose the toymaker found some more pieces of wood; the \\lambda_k associated with the wood constraint will equal the partial derivative of ‚àíp^* with respect to how much more wood became available. Suppose the inventory increases by one piece of wood. Use \\lambda^* to estimate how much the profit would increase, without solving the updated optimization problem. How is this consistent with the price interpretation given above for the Lagrange multipliers? source\n\n\n\n\nSequence convergence\n\n[15 points] Determine the convergence or divergence of a given sequences\n\nr_{k} = \\frac{1}{\\sqrt{k}}.\nr_{k} = 0.707^k.\nr_{k} = 0.707^{2^k}.\n\n[10 points] Determine the convergence or divergence of a given sequence r_k =\\begin{cases} \\frac{1}{k}, & \\text{if } k\\text{ is even} \\\\ e^{-k}, & \\text{if } k\\text{ is odd} \\end{cases}.\n[10 points] Determine the following sequence \\{r_k\\} by convergence rate (linear, sublinear, superlinear). In the case of superlinear convergence, additionally, find out whether there is quadratic convergence.\n\nr_k = \\dfrac{1}{k!}\n\n[10 points] Determine the following sequence \\{r_k\\} by convergence rate (linear, sublinear, superlinear). In the case of superlinear convergence, additionally find out whether there is quadratic convergence.\n\nr_k = \\dfrac{1}{k^k}\n\n[15 points] Let \\left\\{ r_k \\right\\}_{k=m}^\\infty be a sequence of non-negative numbers and let s &gt; 0 be some integer. Prove that sequence \\left\\{ r_k \\right\\}_{k=m+s}^\\infty is linearly convergent with constant q if and only if a the sequence \\left\\{ r_k \\right\\}_{k=m}^\\infty converged linearly with constant q.\n\n\n\nLine search\n\n[10 points] Consider a quadratic function f: \\mathbb{R}^n \\rightarrow \\mathbb{R}, and let us start from a point x_k \\in \\mathbb{R}^n moving in the direction of the antigradient -\\nabla f(x_k). Show that the minimum of f along this direction as a function of the step size \\alpha, for a decreasing function at x_k, satisfies Armijo‚Äôs condition for any c_1 in the range 0 \\leq c_1 \\leq \\frac{1}{2}. Specifically, demonstrate that the following inequality holds at the optimal \\alpha^*:\n\n\\varphi(\\alpha) = f(x_{k+1}) = f(x_k - \\alpha \\nabla f(x_k)) \\leq f(x_k) - c_1 \\alpha \\|\\nabla f(x_k)\\|_2^2\n\nImplementing and Testing Line Search Conditions in Gradient Descent [30 points]\n\nx_{k+1} = x_k - \\alpha \\nabla f(x_k)\n\nIn this assignment, you will modify an existing Python code for gradient descent to include various line search conditions. You will test these modifications on two functions: a quadratic function and the Rosenbrock function. The main objectives are to understand how different line search strategies influence the convergence of the gradient descent algorithm and to compare their efficiencies based on the number of function evaluations.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize_scalar\nnp.random.seed(214)\n\n# Define the quadratic function and its gradient\ndef quadratic_function(x, A, b):\n    return 0.5 * np.dot(x.T, np.dot(A, x)) - np.dot(b.T, x)\n\ndef grad_quadratic(x, A, b):\n    return np.dot(A, x) - b\n\n# Generate a 2D quadratic problem with a specified condition number\ndef generate_quadratic_problem(cond_number):\n    # Random symmetric matrix\n    M = np.random.randn(2, 2)\n    M = np.dot(M, M.T)\n\n    # Ensure the matrix has the desired condition number\n    U, s, V = np.linalg.svd(M)\n    s = np.linspace(cond_number, 1, len(s))  # Spread the singular values\n    A = np.dot(U, np.dot(np.diag(s), V))\n\n    # Random b\n    b = np.random.randn(2)\n\n    return A, b\n\n# Gradient descent function\ndef gradient_descent(start_point, A, b, stepsize_func, max_iter=100):\n    x = start_point.copy()\n    trajectory = [x.copy()]\n\n    for i in range(max_iter):\n        grad = grad_quadratic(x, A, b)\n        step_size = stepsize_func(x, grad)\n        x -= step_size * grad\n        trajectory.append(x.copy())\n\n    return np.array(trajectory)\n\n# Backtracking line search strategy using scipy\ndef backtracking_line_search(x, grad, A, b, alpha=0.3, beta=0.8):\n    def objective(t):\n        return quadratic_function(x - t * grad, A, b)\n    res = minimize_scalar(objective, method='golden')\n    return res.x\n\n# Generate ill-posed problem\ncond_number = 30\nA, b = generate_quadratic_problem(cond_number)\n\n# Starting point\nstart_point = np.array([1.0, 1.8])\n\n# Perform gradient descent with both strategies\ntrajectory_fixed = gradient_descent(start_point, A, b, lambda x, g: 5e-2)\ntrajectory_backtracking = gradient_descent(start_point, A, b, lambda x, g: backtracking_line_search(x, g, A, b))\n\n# Plot the trajectories on a contour plot\nx1, x2 = np.meshgrid(np.linspace(-2, 2, 400), np.linspace(-2, 2, 400))\nZ = np.array([quadratic_function(np.array([x, y]), A, b) for x, y in zip(x1.flatten(), x2.flatten())]).reshape(x1.shape)\n\nplt.figure(figsize=(10, 8))\nplt.contour(x1, x2, Z, levels=50, cmap='viridis')\nplt.plot(trajectory_fixed[:, 0], trajectory_fixed[:, 1], 'o-', label='Fixed Step Size')\nplt.plot(trajectory_backtracking[:, 0], trajectory_backtracking[:, 1], 'o-', label='Backtracking Line Search')\n\n# Add markers for start and optimal points\nplt.plot(start_point[0], start_point[1], 'ro', label='Start Point')\noptimal_point = np.linalg.solve(A, b)\nplt.plot(optimal_point[0], optimal_point[1], 'y*', markersize=15, label='Optimal Point')\n\nplt.legend()\nplt.title('Gradient Descent Trajectories on Quadratic Function')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.savefig(\"linesearch.svg\")\nplt.show()\n\n\n\nThe code above plots this\n\n\nStart by reviewing the provided Python code. This code implements gradient descent with a fixed step size and a backtracking line search on a quadratic function. Familiarize yourself with how the gradient descent function and the step size strategies are implemented.\n\nModify the gradient descent function to include the following line search conditions:\n\nSufficient Decrease Condition\nCurvature Condition\nGoldstein Condition\nWolfe Condition\nDichotomy\n\nTest your modified gradient descent algorithm with the implemented line search conditions on the provided quadratic function. Plot the trajectories over iterations for each condition. Choose and specify hyperparameters for inexact line search condition. Choose and specify the termination criterion. Start from the point x_0 = (-1, 2)^T.\nCompare these 7 methods from the budget perspective. Plot the graph of function value from the number of function evaluations for each method on the same graph.\nPlot trajectory for another function with the same set of methods\n\nf(x_1, x_2) =  10(x_2 ‚àí x_1^2)^2 + (x_1 ‚àí 1)^2\n\nwith x_0 = (-1, 2)^T. You might need to adjust hyperparameters.\nPlot the same function value from the number of function calls for this experiment.\n\n\n\n\nGradient Descent\n\nConvergence of Gradient Descent in non-convex smooth case [10 points]\nWe will assume nothing about the convexity of f. We will show that gradient descent reaches an \\varepsilon-substationary point x, such that \\|\\nabla f(x)\\|_2 \\leq \\varepsilon, in O(1/\\varepsilon^2) iterations. Important note: you may use here Lipschitz parabolic upper bound:\n\nf(y) \\leq f(x) + \\nabla f(x)^T (y-x) + \\frac{L}{2} \\|y-x\\|_2^2, \\;\\;\\;\n\\text{for all $x,y$}.  \n  \\tag{1}\n\nPlug in y = x^{k+1} = x^{k} - \\alpha \\nabla f(x^k), x = x^k to (Equation¬†1) to show that\n\n  f(x^{k+1}) \\leq f(x^k) - \\Big (1-\\frac{L\\alpha}{2} \\Big) \\alpha \\|\\nabla f(x^k)\\|_2^2.\n  \nUse \\alpha \\leq 1/L, and rearrange the previous result, to get\n\n  \\|\\nabla f(x^k)\\|_2^2 \\leq \\frac{2}{\\alpha} \\left( f(x^k) - f(x^{k+1}) \\right).\n  \nSum the previous result over all iterations from 1,\\ldots,k+1 to establish\n\n  \\sum_{i=0}^k \\|\\nabla f(x^{i})\\|_2^2 \\leq\n  \\frac{2}{\\alpha} ( f(x^{0}) - f^*).\n  \nLower bound the sum in the previous result to get\n\n  \\min_{i=0,\\ldots,k} \\|\\nabla f(x^{i}) \\|_2\n  \\leq \\sqrt{\\frac{2}{\\alpha(k+1)} (f(x^{0}) - f^*)},\n   which establishes the desired O(1/\\varepsilon^2) rate for achieving \\varepsilon-substationarity.\n\nHow gradient descent convergence depends on the condition number and dimensionality. [20 points] Investigate how the number of iterations required for gradient descent to converge depends on the following two parameters: the condition number \\kappa \\geq 1 of the function being optimized, and the dimensionality n of the space of variables being optimized.\nTo do this, for given parameters n and \\kappa, randomly generate a quadratic problem of size n with condition number \\kappa and run gradient descent on it with some fixed required precision. Measure the number of iterations T(n, \\kappa) that the method required for convergence (successful termination based on the stopping criterion).\nRecommendation: The simplest way to generate a random quadratic problem of size n with a given condition number \\kappa is as follows. It is convenient to take a diagonal matrix A \\in S_{n}^{++} as simply the diagonal matrix A = \\text{Diag}(a), whose diagonal elements are randomly generated within [1, \\kappa], and where \\min(a) = 1, \\max(a) = \\kappa. As the vector b \\in \\mathbb{R}^n, you can take a vector with random elements. Diagonal matrices are convenient to consider since they can be efficiently processed with even for large values of n.\nFix a certain value of the dimensionality n. Iterate over different condition numbers \\kappa on a grid and plot the dependence of T(n,\\kappa) against \\kappa. Since the quadratic problem is generated randomly each time, repeat this experiment several times. As a result, for a fixed value of n, you should obtain a whole family of curves showing the dependence of T(n, \\kappa) on \\kappa. Draw all these curves in the same color for clarity (for example, red).\nNow increase the value of n and repeat the experiment. You should obtain a new family of curves T(n',\\kappa) against \\kappa. Draw all these curves in the same color but different from the previous one (for example, blue).\nRepeat this procedure several times for other values of n. Eventually, you should have several different families of curves - some red (corresponding to one value of n), some blue (corresponding to another value of n), some green, etc.\nNote that it makes sense to iterate over the values of the dimensionality n on a logarithmic grid (for example, n = 10, n = 100, n = 1000, etc.). Use the following stopping criterion: \\|\\nabla f(x_k)\\|_2^2 \\leq \\varepsilon \\|\\nabla f(x_0)\\|_2^2 with \\varepsilon = 10^{-5}. Select the starting point x_0 = (1, \\ldots, 1)^T\nWhat conclusions can be drawn from the resulting picture?\n\n\n\nAccelerated methods\n\nLocal Convergence of Heavy Ball Method. [20 points] We will work with the heavy ball method in this problem\n\n\\tag{HB}\nx_{k+1} = x_k - \\alpha \\nabla f(x_k) + \\beta (x_k - x_{k-1})\n\nIt is known, that for the quadratics the best choice of hyperparameters is \\alpha^* = \\dfrac{4}{(\\sqrt{L} + \\sqrt{\\mu})^2}, \\beta^* = \\dfrac{(\\sqrt{L} - \\sqrt{\\mu})^2}{(\\sqrt{L} + \\sqrt{\\mu})^2}, which ensures accelerated linear convergence for a strongly convex quadratic function.\nConsider the following continuously differentiable, strongly convex with parameter \\mu, and smooth function with parameter L:\n\nf(x) =\n\\begin{cases}\n\\frac{25}{2}x^2, & \\text{if } x &lt; 1 \\\\\n\\frac12x^2 + 24x - 12, & \\text{if } 1 \\leq x &lt; 2 \\\\\n\\frac{25}{2}x^2 - 24x + 36, & \\text{if } x \\geq 2\n\\end{cases}\n\\quad\n\\nabla f(x) =\n\\begin{cases}\n25x, & \\text{if } x &lt; 1 \\\\\nx + 24, & \\text{if } 1 \\leq x &lt; 2 \\\\\n25x - 24, & \\text{if } x \\geq 2\n\\end{cases}\n\n\nHow to prove, that the given function is convex? Strongly convex? Smooth?\nFind the constants \\mu and L for a given function.\nPlot the function value for x \\in [-4, 4].\nRun the Heavy Ball method for the function with optimal hyperparameters \\alpha^* = \\dfrac{4}{(\\sqrt{L} + \\sqrt{\\mu})^2}, \\beta^* = \\dfrac{(\\sqrt{L} - \\sqrt{\\mu})^2}{(\\sqrt{L} + \\sqrt{\\mu})^2} for quadratic function, starting from x_0 = 3.5. If you have done everything above correctly, you should receive something like\n\nYou can use the following code for plotting:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\n# Gradient of the function\ndef grad_f(x):\n    ...\n\n# Heavy Ball method implementation\ndef heavy_ball_method(alpha, beta, x0, num_iterations):\n    x = np.zeros(num_iterations + 1)\n    x_prev = x0\n    x_curr = x0  # Initialize x[1] same as x[0] to start the algorithm\n    for i in range(num_iterations):\n        x[i] = x_curr\n        x_new = x_curr - alpha * grad_f(x_curr) + beta * (x_curr - x_prev)\n        x_prev = x_curr\n        x_curr = x_new\n    x[num_iterations] = x_curr\n    return x\n\n# Parameters\nL = ...\nmu = ...\nalpha_star = ...\nbeta_star = ...\nx0 = ...\nnum_iterations = 30\n\n# Generate the trajectory of the method\ntrajectory = heavy_ball_method(alpha_star, beta_star, x0, num_iterations)\n\n# Setup the figure and axes for the animation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3.5))\nfig.suptitle(\"Heavy ball method with optimal hyperparameters Œ±* Œ≤*\")\n\n# Function for updating the animation\ndef update(i):\n    ax1.clear()\n    ax2.clear()\n\n    # Plot f(x) and trajectory\n    x_vals = np.linspace(-4, 4, 100)\n    f_vals = np.piecewise(x_vals, [x_vals &lt; 1, (x_vals &gt;= 1) & (x_vals &lt; 2), x_vals &gt;= 2],\n                        [lambda x: 12.5 * x**2, lambda x: .5 * x**2 + 24 * x - 12, lambda x: 12.5 * x**2 - 24 * x + 36])\n    ax1.plot(x_vals, f_vals, 'b-')\n    ax1.plot(trajectory[:i], [12.5 * x**2 if x &lt; 1 else .5 * x**2 + 24 * x - 12 if x &lt; 2 else 12.5 * x**2 - 24 * x + 36 for x in trajectory[:i]], 'ro-')\n    # Add vertical dashed lines at x=1 and x=2 on the left subplot\n    ax1.axvline(x=1, color='navy', linestyle='--')\n    ax1.axvline(x=2, color='navy', linestyle='--')\n\n    # Plot function value from iteration\n    f_trajectory = [None for x in trajectory]\n    f_trajectory[:i] = [12.5 * x**2 if x &lt; 1 else .5 * x**2 + 24 * x - 12 if x &lt; 2 else 12.5 * x**2 - 24 * x + 36 for x in trajectory[:i]]\n    ax2.plot(range(len(trajectory)), f_trajectory, 'ro-')\n    ax2.set_xlim(0, len(trajectory))\n    ax2.set_ylim(min(f_vals), max(f_vals))\n    # Add horizontal dashed lines at f(1) and f(2) on the right subplot\n    f_1 = 12.5 * 1.0**2\n    f_2 = .5 * 2.**2 + 24 * 2. - 12\n    ax2.axhline(y=f_1, color='navy', linestyle='--')\n    ax2.axhline(y=f_2, color='navy', linestyle='--')\n\n    # ax1.set_title(\"Function f(x) and Trajectory\")\n    ax1.set_xlabel(\"x\")\n    ax1.set_ylabel(\"f(x)\")\n    ax1.grid(linestyle=\":\")\n\n    # ax2.set_title(\"Function Value from Iteration\")\n    ax2.set_xlabel(\"Iteration\")\n    ax2.set_ylabel(\"f(x)\")\n    ax2.grid(linestyle=\":\")\n\n    plt.tight_layout()\n\n# Create the animation\nani = animation.FuncAnimation(fig, update, frames=num_iterations, repeat=False, interval=100)\nHTML(ani.to_jshtml())\nChange the starting point to x_0 = 3.4. What do you see? How could you name such a behavior of the method?\nChange the hyperparameter \\alpha^{\\text{Global}} = \\frac2L, \\beta^{\\text{Global}} = \\frac{\\mu}{L} and run the method again from x_0 = 3.4. Check whether you have accelerated convergence here.\n\nContext: this counterexample was provided in the paper, while the global convergence of the heavy ball method for general smooth strongly convex function was introduced in another paper. Recently, it was suggested, that the heavy-ball (HB) method provably does not reach an accelerated convergence rate on smooth strongly convex problems.\n[40 points] In this problem we will work with accelerated methods applied to the logistic regression problem. A good visual introduction to the topic is available here.\nLogistic regression is a standard model in classification tasks. For simplicity, consider only the case of binary classification. Informally, the problem is formulated as follows: There is a training sample \\{(a_i, b_i)\\}_{i=1}^m, consisting of m vectors a_i \\in \\mathbb{R}^n (referred to as features) and corresponding numbers b_i \\in \\{-1, 1\\} (referred to as classes or labels). The goal is to construct an algorithm b(\\cdot), which for any new feature vector a automatically determines its class b(a) \\in \\{-1, 1\\}.\nIn the logistic regression model, the class determination is performed based on the sign of the linear combination of the components of the vector a with some fixed coefficients x \\in \\mathbb{R}^n:\n\nb(a) := \\text{sign}(\\langle a, x \\rangle).\n\nThe coefficients x are the parameters of the model and are adjusted by solving the following optimization problem:\n\n\\tag{LogReg}\n\\min_{x \\in \\mathbb{R}^n} \\left( \\frac{1}{m} \\sum_{i=1}^m \\ln(1 + \\exp(-b_i \\langle a_i, x \\rangle)) + \\frac{\\lambda}{2} \\|x\\|^2 \\right),\n\nwhere \\lambda \\geq 0 is the regularization coefficient (a model parameter).\n\nWill the LogReg problem be convex for \\lambda = 0? What is the gradient of the objective function? Will it be strongly convex? What if you will add regularization with \\lambda &gt; 0?\nWe will work with the real-world data for A and b: take the mushroom dataset. Be careful, you will need to predict if the mushroom is poisonous or edible. A poor model can cause death in this exercise.\nimport requests\nfrom sklearn.datasets import load_svmlight_file\n\n# URL of the file to download\nurl = 'https://hse24.fmin.xyz/files/mushrooms.txt'\n\n# Download the file and save it locally\nresponse = requests.get(url)\ndataset = 'mushrooms.txt'\n\n# Ensure the request was successful\nif response.status_code == 200:\n    with open(dataset, 'wb') as f:\n        f.write(response.content)\n\n    # Load the dataset from the downloaded file\n    data = load_svmlight_file(dataset)\n    A, b = data[0].toarray(), data[1]\n    n, d = A.shape\n\n    print(\"Data loaded successfully.\")\n    print(f\"Number of samples: {n}, Number of features: {d}\")\nelse:\n    print(f\"Failed to download the file. Status code: {response.status_code}\")\nDivide the data into two parts: training and test. We will train the model on the A_{train}, b_{train} and measure the accuracy of the model on the A_{test}, b_{test}.\nfrom sklearn.model_selection import train_test_split\n# Split the data into training and test sets\nA_train, A_test, b_train, b_test = train_test_split(A, b, test_size=0.2, random_state=214)\nFor the training part A_{train}, b_{train}, estimate the constants \\mu, L of the training/optimization problem. Use the same small value \\lambda for all experiments\nUsing gradient descent with the step \\frac{1}{L}, train a model. Plot: accuracy versus iteration number.\n\n\\tag{HB}\nx_{k+1} = x_k - \\alpha \\nabla f(x_k) + \\beta (x_k - x_{k-1})\n\nFix a step \\alpha = \\frac{1}{L} and search for different values of the momentum \\beta from -1 to 1. Choose your own convergence criterion and plot convergence for several values of momentum on the same graph. Is the convergence always monotonic?\nFor the best value of momentum \\beta, plot the dependence of the model accuracy on the test sample on the running time of the method. Add to the same graph the convergence of gradient descent with step \\frac{1}{L}. Draw a conclusion. Ensure, that you use the same value of \\lambda for both methods.\nSolve the logistic regression problem using the Nesterov method.\n\n\\tag{NAG}\nx_{k+1} = x_k - \\alpha \\nabla f(x_k + \\beta (x_k - x_{k-1})) + \\beta (x_k - x_{k-1})  \n\nFix a step \\frac{1}{L} and search for different values of momentum \\beta from -1 to 1. Check also the momentum values equal to \\frac{k}{k+3}, \\frac{k}{k+2}, \\frac{k}{k+1} (k is the number of iterations), and if you are solving a strongly convex problem, also \\frac{\\sqrt{L} - \\sqrt{\\mu}}{\\sqrt{L} + \\sqrt{\\mu}}. Plot the convergence of the method as a function of the number of iterations (choose the convergence criterion yourself) for different values of the momentum. Is the convergence always monotonic?\nFor the best value of momentum \\beta, plot the dependence of the model accuracy on the test sample on the running time of the method. Add this graph to the graphs for the heavy ball and gradient descent from the previous steps. Make a conclusion.\nNow we drop the estimated value of L and will try to do it adaptively. Let us make the selection of the constant L adaptive.\n\nf(y) \\leq f(x^k) + \\langle \\nabla f(x^k), y - x^k \\rangle + \\frac{L}{2}\\|x^k - y\\|_2^2\n\nIn particular, the procedure might work:\ndef backtracking_L(f, grad, x, h, L0, rho, maxiter=100):\n    L = L0\n    fx = f(x)\n    gradx = grad(x)\n    iter = 0\n    while iter &lt; maxiter :\n        y = x - 1 / L * h\n        if f(y) &lt;= fx - 1 / L gradx.dot(h) + 1 / (2 * L) h.dot(h):\n            break\n        else:\n            L = L * rho\n\n        iter += 1\n    return L\nWhat should h be taken as? Should \\rho be greater or less than 1? Should L_0 be taken as large or small? Draw a similar figure as it was in the previous step for L computed adaptively (6 lines - GD, HB, NAG, GD adaptive L, HB adaptive L, NAG adaptive L)\n\n\n\n\nConjugate gradients\n\nRandomized Preconditioners for Conjugate Gradient Methods. (20 points)\nLinear least squares\nIn this task, we explore the use of some randomization methods for solving overdetermined least-squares problems, focusing on conjugate gradient methods. Let \\hat{A} \\in \\mathbb{R}^{m \\times n} be a matrix (we assume that m \\gg n) and b \\in \\mathbb{R}^m, we aim to minimize\n\nf(x) = \\frac{1}{2} \\|\\hat{A}x - b\\|^2_2 = \\frac{1}{2} \\sum_{i=1}^m (\\hat{a}_i^T x - b_i)^2,\n\nwhere the \\hat{a}_i \\in \\mathbb{R}^n denote the rows of \\hat{A}.\nPreconditioners\nWe know, that the convergence bound of the CG applied for the problem depends on the condition number of the matrix. Note, that for the problem above we have the matrix \\hat{A}^T \\hat{A} and the condition number is squared after this operation (\\kappa (X^T X) =  \\kappa^2 \\left(X \\right)). That is the reason, why we typically need to use preconditioners (read 12. for more details) with CG.\nThe general idea of using preconditioners implies switchwing from solving Ax = b to MAx = Mb with hope, that \\kappa \\left( MA\\right) \\ll \\kappa \\left( A\\right) or eigenvalues of MA are better clustered than those of A (note, that matrix A here is for the general case, here we have \\hat{A}^T\\hat{A} instead).\nThis idea can also be viewed as coordinate change x = T \\hat{x}, \\; \\hat{x} = T^{-1}x, which leads to the problem T^T A T \\hat{x} = T^Tb. Note, that the spectrum of T^TAT is the same as the spectrum of MA.\nThe best choice of M is A^{-1}, because \\kappa (A^{-1} A) = \\kappa (I) = 1. However, if we know A^{-1}, the original problem is already solved, that is why we need to find some trade-off between enhanced convergence, and extra cost of working with M. The goal is to find M that is cheap to multiply, and approximate inverse of A (or at least has a more clustered spectrum than A).\nNote, that for the linear least squares problem the matrix of quadratic form is A = \\hat{A}^T\\hat{A}. Below you can find Vanilla CG algorithm (on the left) and preconditioned CG algorithm (on the right):\n\n\\begin{aligned}\n& \\mathbf{r}_0 := \\mathbf{b} - \\mathbf{A x}_0 \\\\\n& \\hbox{if } \\mathbf{r}_{0} \\text{ is sufficiently small, then return } \\mathbf{x}_{0} \\text{ as the result}\\\\\n& \\mathbf{d}_0 := \\mathbf{r}_0 \\\\\n& k := 0 \\\\\n& \\text{repeat} \\\\\n& \\qquad \\alpha_k := \\frac{\\mathbf{r}_k^\\mathsf{T} \\mathbf{r}_k}{\\mathbf{d}_k^\\mathsf{T} \\mathbf{A d}_k}  \\\\\n& \\qquad \\mathbf{x}_{k+1} := \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k \\\\\n& \\qquad \\mathbf{r}_{k+1} := \\mathbf{r}_k - \\alpha_k \\mathbf{A d}_k \\\\\n& \\qquad \\hbox{if } \\mathbf{r}_{k+1} \\text{ is sufficiently small, then exit loop} \\\\\n& \\qquad \\beta_k := \\frac{\\mathbf{r}_{k+1}^\\mathsf{T} \\mathbf{r}_{k+1}}{\\mathbf{r}_k^\\mathsf{T} \\mathbf{r}_k} \\\\\n& \\qquad \\mathbf{d}_{k+1} := \\mathbf{r}_{k+1} + \\beta_k \\mathbf{d}_k \\\\\n& \\qquad k := k + 1 \\\\\n& \\text{end repeat} \\\\\n& \\text{return } \\mathbf{x}_{k+1} \\text{ as the result}\n\\end{aligned} \\qquad\n\\begin{aligned}\n& \\mathbf{r}_0 := \\mathbf{b} - \\mathbf{A x}_0 \\\\\n& \\text{if } \\mathbf{r}_0 \\text{ is sufficiently small, then return } \\mathbf{x}_0 \\text{ as the result} \\\\\n& \\mathbf{z}_0 := \\mathbf{M}^{-1} \\mathbf{r}_0 \\\\\n& \\mathbf{d}_0 := \\mathbf{z}_0 \\\\\n& k := 0 \\\\\n& \\text{repeat} \\\\\n& \\qquad \\alpha_k := \\frac{\\mathbf{r}_k^\\mathsf{T} \\mathbf{z}_k}{\\mathbf{d}_k^\\mathsf{T} \\mathbf{A d}_k} \\\\\n& \\qquad \\mathbf{x}_{k+1} := \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k \\\\\n& \\qquad \\mathbf{r}_{k+1} := \\mathbf{r}_k - \\alpha_k \\mathbf{A d}_k \\\\\n& \\qquad \\text{if } \\mathbf{r}_{k+1} \\text{ is sufficiently small, then exit loop} \\\\\n& \\qquad \\mathbf{z}_{k+1} := \\mathbf{M}^{-1} \\mathbf{r}_{k+1} \\\\\n& \\qquad \\beta_k := \\frac{\\mathbf{r}_{k+1}^\\mathsf{T} \\mathbf{z}_{k+1}}{\\mathbf{r}_k^\\mathsf{T} \\mathbf{z}_k} \\\\\n& \\qquad \\mathbf{d}_{k+1} := \\mathbf{z}_{k+1} + \\beta_k \\mathbf{d}_k \\\\\n& \\qquad k := k + 1 \\\\\n& \\text{end repeat} \\\\\n& \\text{return } \\mathbf{x}_{k+1} \\text{ as the result}\n\\end{aligned}\n\nHadamard matrix\nGiven m \\in \\{2^i, i = 1, 2, \\ldots\\}, the (unnormalized) Hadamard matrix of order m is defined recursively as\n\nH_2 = \\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix}, \\quad \\text{and} \\quad H_m = \\begin{bmatrix} H_{m/2} & H_{m/2} \\\\ H_{m/2} & -H_{m/2} \\end{bmatrix}.\n\nThe associated normalized Hadamard matrix is given by H^{(\\text{norm})}_m = \\frac{1}{\\sqrt{m}} H_m, which evidently satisfies H^{(\\text{norm})T}_m H^{(\\text{norm})}_m = I_{m \\times m}. Moreover, via a recursive algorithm, it is possible to compute matvec H_m x in time O(m \\log m), which is much faster than m^2 for a general matrix.\nTo solve the least squares minimization problem using conjugate gradients, we must solve \\hat{A}^T \\hat{A} x = \\hat{A}^T b. Using a preconditioner M such that M \\approx A^{-1} can give substantial speedup in computing solutions to large problems.\nConsider the following scheme to generate a randomized preconditioner, assuming that m = 2^i for some i:\n\nLet S = \\text{diag}(S_{11}, \\ldots, S_{mm}), where S_{jj} are random \\{-1,+1\\} signs\nLet p \\in \\mathbb{Z}^+ be a small positive integer, say 20 for this problem.\nLet R \\in \\{0, 1\\}^{n+p \\times m} be a row selection matrix, meaning that each row of R has only 1 non-zero entry, chosen uniformly at random. (The location of these non-zero columns is distinct.)\nimport jax.numpy as jnp\nfrom jax import random\n\ndef create_row_selection_matrix_jax(m, n, p, key):\n    # m is the number of columns in the original matrix A\n    # n+p is the number of rows in the row selection matrix R\n    # key is a PRNGKey needed for randomness in JAX\n    inds = random.permutation(key, m)[:n+p]  # Generate a random permutation and select the first n+p indices\n    R = jnp.zeros((n+p, m), dtype=jnp.int32)  # Create a zero matrix of shape (n+p, m)\n    R = R.at[np.arange(n+p), inds].set(1)     # Use JAX's indexed update to set the entries corresponding to inds to 1\n    return R\nDefine \\Phi = R H^{(\\text{norm})}_m S \\in \\mathbb{R}^{n+p \\times m}\n\nWe then define the matrix M via its inverse M^{-1} = \\hat{A}^T \\Phi^T \\Phi \\hat{A} \\in \\mathbb{R}^{n \\times n}.\nQuestions\n\n(2 point) How many FLOPs (floating point operations, i.e.¬†multiplication and additions) are required to compute the matrices M^{-1} and M, respectively, assuming that you can compute the matrix-vector product H_mv in time m \\log m for any vector v \\in \\mathbb{R}^m?\n(2 point) How many FLOPs are required to naively compute \\hat{A}^T \\hat{A}, assuming \\hat{A} is dense (using standard matrix algorithms)?\n(2 point) How many FLOPs are required to compute \\hat{A}^T \\hat{A} v for a vector v \\in \\mathbb{R}^n by first computing u = \\hat{A}v and then computing \\hat{A}^T u?\n(4 poins) Suppose that conjugate gradients runs for k iterations. Using the preconditioned conjugate gradient algorithm with M = (\\hat{A}^T \\Phi^T \\Phi \\hat{A})^{-1}, how many total floating point operations have been performed? How many would be required to directly solve \\hat{A}^T \\hat{A} x = \\hat{A}^T b? How large must k be to make the conjugate gradient method slower?\n(10 points) Implement the conjugate gradient algorithm for solving the positive definite linear system \\hat{A}^T \\hat{A} x = \\hat{A}^T b both with and without the preconditioner M. To generate data for your problem, set m = 2^{12} and n = 400, then generate the matrix A and the vector b. For simplicity in implementation, you may directly pass \\hat{A}^T \\hat{A} and \\hat{A}^T b into your conjugate gradient solver, as we only wish to explore how the methods work.\nimport numpy as np\nfrom scipy.sparse import diags\n\nm = 2**12  # 4096\nn = 400\n# Create a linear space of values from 0.001 to 100\nvalues = np.linspace(0.001, 100, n)\n# Generate the matrix A\nA = np.random.randn(m, n) * diags(values).toarray()\nb = np.random.randn(m, 1)\nPlot the norm of the residual r_k = \\hat{A}^T b - \\hat{A}^T \\hat{A} x_k (relative to \\|\\hat{A}^T b\\|_2) as a function of iteration k for each of your conjugate gradient procedures. Additionally, compute and print the condition numbers \\kappa(\\hat{A}^T \\hat{A}) and \\kappa(M^{1/2} \\hat{A}^T \\hat{A} M^{1/2}).\n\n\n\n\nNewton and quasinewton methods\n\nüò± Newton convergence issue (10 points)\nConsider the following function:\n\nf(x,y) = \\dfrac{x^4}{4} - x^2 + 2x + (y-1)^2\n\nAnd the starting point is x_0 = (0,2)^\\top. How does Newton‚Äôs method behave when started from this point? How can this be explained? How does the gradient descent with fixed step \\alpha = 0.01 and the steepest descent method behave under the same conditions? (It is not necessary to show numerical simulations in this problem).\nHessian-Free Newton method (20 points) In this exercise, we‚Äôll explore the optimization of a binary logistic regression problem using various methods. Don‚Äôt worry about the size of the problem description, first 5 bullets out of 7 could be done pretty quickly. In this problem you should start with this colab notebook\nGiven a dataset with n observations, where each observation consists of a feature vector x_i and an associated binary target variable y_i \\in \\{0,1\\}, the logistic regression model predicts the probability that y_i = 1 given x_i using the logistic function. The loss function to be minimized is the negative log-likelihood of the observed outcomes under this model, summed across all observations. It has a high value when the model outputs differ significantly from the data y.\nThe binary cross-entropy loss function for a single observation (x_i, y_i) is given by: \n\\text{Loss}(w; x_i, y_i) = -\\left[ y_i \\log(p(y_i=1 | x_i; w)) + (1-y_i) \\log(1-p(y_i=1 | x_i; w)) \\right]\n\nHere, p(y=1 | x;w) is defined as: \np(y=1 | x;w) = \\frac{1}{1 + e^{-w^T x}}\n\nTo define the total loss over the dataset, we sum up the individual losses: \nf(w) = -\\sum_{i=1}^n \\left[ y_i \\log(p(y_i=1 | x_i; w)) + (1-y_i) \\log(1-p(y_i=1 | x_i; w)) \\right]\n\nTherefore, the optimization problem in logistic regression is: \n\\min_w f(w) = \\min_w -\\sum_{i=1}^n \\left[ y_i \\log\\left(p\\left(y_i=1 | x_i; w\\right)\\right) + \\left(1-y_i\\right) \\log\\left(1-p(y_i=1 | x_i; w)\\right) \\right]\n\nThis is a convex optimization problem and can be solved using gradient-based methods such as gradient descent, Newton‚Äôs method, or more sophisticated optimization algorithms often available in machine learning libraries. However, it is the problem is often together with l_2 regularization:\n\n\\min_w f(w) = \\min_w -\\sum_{i=1}^n \\left[ y_i \\log\\left(p\\left(y_i=1 | x_i; w\\right)\\right) + \\left(1-y_i\\right) \\log\\left(1-p(y_i=1 | x_i; w)\\right) \\right] + \\frac{\\mu}{2} \\|w\\|_2^2\n\n\n(2 points) Firstly, we address the optimization with Gradient Descent (GD) in a strongly convex setting, with \\mu = 1. Use a constant learning rate \\alpha. Run the gradient descent algorithm. Report the highest learning rate that ensures convergence of the algorithm. Plot the convergence graph in terms of both domain (parameter values) and function value (loss). Describe the type of convergence observed.\nparams = {\n    \"mu\": 1,\n    \"m\": 1000,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 3e-2,\n            \"iterations\": 550,\n        },\n    ]\n}\n\nresults, params = run_experiments(params)\n(2 points) Run Newton‚Äôs method under the same conditions, using the second derivatives to guide the optimization. Describe and analyze the convergence properties observed.\nparams = {\n    \"mu\": 1,\n    \"m\": 1000,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 3e-2,\n            \"iterations\": 550,\n        },\n        {\n            \"method\": \"Newton\",\n            \"iterations\": 20,\n        },\n    ]\n}\n\nresults, params = run_experiments(params)\n(2 points) In cases where Newton‚Äôs method may converge too rapidly or overshoot, a damped version can be more stable. Run the damped Newton method. Adjust the damping factor as a learning rate. Report the highest learning rate ensuring stability and convergence. Plot the convergence graph.\nparams = {\n    \"mu\": 1,\n    \"m\": 1000,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 3e-2,\n            \"iterations\": 550,\n        },\n        {\n            \"method\": \"Newton\",\n            \"iterations\": 20,\n        },\n        {\n            \"method\": \"Newton\",\n            \"learning_rate\": 5e-1,\n            \"iterations\": 50,\n        },\n    ]\n}\n\nresults, params = run_experiments(params)\n(2 points) Now turn off the regularization by setting \\mu=0. Try to find the largest learning rate, which ensures convergence of the Gradient Descent. Use a constant learning rate \\alpha. Run the gradient descent algorithm. Report the highest learning rate that ensures convergence of the algorithm. Plot the convergence graph in terms of both domain (parameter values) and function value (loss). Describe the type of convergence observed. How can you describe an idea to run this method for the problem to reach tight primal gap f(x_k) - f^* \\approx 10^{-2} or 10^{-3}, 10^{-4}?\nparams = {\n    \"mu\": 0,\n    \"m\": 1000,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 3e-2,\n            \"iterations\": 200,\n        },\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 7e-2,\n            \"iterations\": 200,\n        },\n    ]\n}\n\nresults, params = run_experiments(params)\n(2 points) What can you say about Newton‚Äôs method convergence in the same setting \\mu=0? Try several learning rates smaller, than 1 for the damped Newton method. Does it work? Write your conclusions about the second-order method convergence for a binary logistic regression problem.\n(5 points) Now switch back to the strongly convex setting \\mu=1. To avoid directly computing the Hessian matrix in Newton‚Äôs method, use the Conjugate Gradient (CG) method to solve the linear system in the Newton step. Develop the newton_method_cg function, which computes the Newton step using CG to solve the system \\nabla^2 f(x_k) d_k = - \\nabla f(x_k), \\; x_{k+1} = x_k + \\alpha d_k defined by the Hessian. You have to use jax.scipy.sparse.linalg.cg function here. So, firstly compute the hessian as it was done in the code, then put it into this linear solver. Compare its performance in terms of computational efficiency and convergence rate to the standard Newton method.\n(5 points) Finally, implement a Hessian-free version of Newton‚Äôs method (HFN) which utilizes Hessian-vector products derived via automatic differentiation. Note, that jax.scipy.sparse.linalg.cg function can take the matvec function, which directly produces the multiplication of any input vector x. Implement the HFN method without explicitly forming or storing the Hessian matrix in function newton_method_hfn. Use autograd to compute Hessian-vector products as it is described here. Compare this method‚Äôs time complexity and memory requirements against previous implementations.\n\n\n\n\nConditional gradient methods\n\nProjection onto the Birkhoff Polytope using Frank-Wolfe [20 points]\nIn a recent book authors presented the following comparison table with complexities of linear minimizations and projections on some convex sets up to an additive error \\epsilon in the Euclidean norm. When \\epsilon is missing, there is no additive error. The \\tilde{\\mathcal{O}} hides polylogarithmic factors in the dimensions and polynomial factors in constants related to thedistancetothe optimum. For the nuclear norm ball, i.e., the spectrahedron, \\nu denotes the number of non-zero entries and \\sigma_1 denotes the top singular value of the projected matrix.\n\n\n\n\n\n\n\n\nSet\nLinear minimization\nProjection\n\n\n\n\nn-dimensional \\ell_p-ball, p \\neq 1,2,\\infty\n\\mathcal{O}(n)\n\\tilde{\\mathcal{O}}\\!\\bigl(\\tfrac{n}{\\epsilon^2}\\bigr)\n\n\nNuclear norm ball of n\\times m matrices\n\\mathcal{O}\\!\\Bigl(\\nu\\,\\ln(m + n)\\,\\tfrac{\\sqrt{\\sigma_1}}{\\sqrt{\\epsilon}}\\Bigr)\n\\mathcal{O}\\!\\bigl(m\\,n\\,\\min\\{m,n\\}\\bigr)\n\n\nFlow polytope on a graph with m vertices and n edges (capacity bound on edges)\n\\mathcal{O}\\!\\Bigl((n \\log m)\\bigl(n + m\\,\\log m\\bigr)\\Bigr)\n\\tilde{\\mathcal{O}}\\!\\bigl(\\tfrac{n}{\\epsilon^2}\\bigr)\\ \\text{or}\\ \\mathcal{O}(n^4\\,\\log n)\n\n\nBirkhoff polytope (n \\times n doubly stochastic matrices)\n\\mathcal{O}(n^3)\n\\tilde{\\mathcal{O}}\\!\\bigl(\\tfrac{n^2}{\\epsilon^2}\\bigr)\n\n\n\nThe Birkhoff polytope, denoted as B_n, is the set of n \\times n doubly stochastic matrices: \nB_n = \\{ X \\in \\mathbb{R}^{n \\times n} \\mid X_{ij} \\ge 0 \\;\\forall i,j, \\quad X \\mathbf{1} = \\mathbf{1}, \\quad X^T \\mathbf{1} = \\mathbf{1} \\}\n where \\mathbf{1} is the vector of all ones. This set is convex and compact. Its extreme points are the permutation matrices.\nGiven an arbitrary matrix Y \\in \\mathbb{R}^{n \\times n}, we want to find its projection onto B_n, which is the solution to the optimization problem: \n\\min_{X \\in B_n} f(X) = \\frac{1}{2} \\| X - Y \\|_F^2\n where \\| \\cdot \\|_F is the Frobenius norm.\nWe will use the Frank-Wolfe (Conditional Gradient) algorithm to solve this problem. Recall the steps of the Frank-Wolfe algorithm:\n\nInitialize X_0 \\in B_n.\nFor k = 0, 1, 2, \\ldots:\n\nCompute the gradient \\nabla f(X_k).\nSolve the Linear Minimization Oracle (LMO): S_k = \\arg\\min_{S \\in B_n} \\langle \\nabla f(X_k), S \\rangle.\nDetermine the step size \\gamma_k \\in [0, 1].\nUpdate X_{k+1} = (1-\\gamma_k) X_k + \\gamma_k S_k.\n\n\nTasks:\n\n[5 points] Explicitly write down the gradient \\nabla f(X_k). Explain how to solve the LMO step \\min_{S \\in B_n} \\langle \\nabla f(X_k), S \\rangle. What kind of matrix is the solution S_k? Hint: Consider the connection to the linear assignment problem (Hungarian algorithm).\n[10 points] Implement the Frank-Wolfe algorithm in Python to solve the projection problem. Use scipy.optimize.linear_sum_assignment to solve the LMO. For the step size, you can use the optimal closed-form solution for projection: \\gamma_k = \\frac{\\langle X_k - Y, X_k - S_k \\rangle}{\\| X_k - S_k \\|_F^2}, clipped to [0, 1].\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\nimport matplotlib.pyplot as plt\n\ndef project_to_birkhoff_frank_wolfe(Y, max_iter=100, tol=1e-6):\n    \"\"\"\n    Projects matrix Y onto the Birkhoff polytope using the Frank-Wolfe algorithm.\n\n    Args:\n        Y (np.ndarray): The matrix to project.\n        max_iter (int): Maximum number of iterations.\n        tol (float): Tolerance for convergence (change in objective value).\n\n    Returns:\n        np.ndarray: The projection of Y onto the Birkhoff polytope.\n        list: History of objective function values.\n    \"\"\"\n    n = Y.shape[0]\n    assert Y.shape[0] == Y.shape[1], \"Input matrix must be square\"\n\n    # Initialize with a feasible point (e.g., uniform matrix)\n    Xk = np.ones((n, n)) / n\n\n    objective_history = []\n\n    for k in range(max_iter):\n        # Objective function value\n        obj_val = 0.5 * np.linalg.norm(Xk - Y, 'fro')**2\n        objective_history.append(obj_val)\n\n        if k &gt; 0 and abs(objective_history[-1] - objective_history[-2]) &lt; tol:\n            print(f\"Converged after {k} iterations.\")\n            break\n\n        # 1. Compute gradient\n        grad_fk = ... # YOUR CODE HERE \n\n        # 2. Solve the LMO: S_k = argmin_{S in Birkhoff} &lt;grad_fk, S&gt;\n        # Use linear_sum_assignment on the cost matrix grad_fk\n        row_ind, col_ind = ... # YOUR CODE HERE using linear_sum_assignment\n        Sk = np.zeros((n, n))\n        # Construct permutation matrix Sk based on row_ind, col_ind\n        ... # YOUR CODE HERE \n\n        # 3. Compute step size gamma_k \n        # Optimal step size for projection, clipped to [0, 1]\n        delta_k = Xk - Sk\n        denom = np.linalg.norm(delta_k, 'fro')**2\n        if denom &lt; 1e-12: # Avoid division by zero if Xk is already the vertex Sk\n            gamma_k = 0.0\n        else:\n            gamma_k = ... # YOUR CODE HERE for optimal step size\n            gamma_k = np.clip(gamma_k, 0.0, 1.0) \n\n        # 4. Update\n        Xk = ... # YOUR CODE HERE \n\n    else: # If loop finishes without breaking\n         print(f\"Reached max iterations ({max_iter}).\")\n\n    return Xk, objective_history\n[5 points] Test your implementation with n=5 and a randomly generated matrix Y = \\text{np.random.rand}(5, 5). Run the algorithm for 200 iterations. Plot the objective function value f(X_k) versus the iteration number k. Verify numerically that the final matrix X_{200} approximately satisfies the conditions for being in B_5 (non-negative entries, row sums equal to 1, column sums equal to 1).\n\n\n\n\nSubgradient method\n\nFinding a point in the intersection of convex sets. [30 points] Let A \\in \\mathbb{R}^{n \\times n} be some non-degenerate matrix and let \\Sigma be an n \\times n diagonal matrix with diagonal entries \\sigma_1,...,\\sigma_n &gt; 0, and y a given vector in \\mathbb{R}^n. Consider the compact convex sets U = \\{x \\in \\mathbb{R}^n \\mid \\|A(x-y)\\|_2 \\leq 1\\} and V = \\{x \\in \\mathbb{R}^n \\mid \\|\\Sigma x\\|_\\infty \\leq 1\\}.\n\n[10 points] Minimize maximum distance from the current point to the convex sets.\n\n  \\min_{x\\in\\mathbb{R}^n} f(x) =  \\min_{x\\in\\mathbb{R}^n} \\max\\{\\mathbf{dist}(x, U), \\mathbf{dist}(x, V)\\}\n  \npropose an algorithm to find a point x \\in U \\cap V. You can assume that U \\cap V is not empty. Your algorithm must be specific and provably converging (although you do not need to prove it and you can simply refer to the lecture slides).\n[15 points] Implement your algorithm with the following data: n = 2, y = (3, 2), \\sigma_1 = 0.5, \\sigma_2 = 1,\n\n  A = \\begin{bmatrix}\n  1 & 0 \\\\\n  -1 & 1\n  \\end{bmatrix},\n  \nPlot the objective value of your optimization problem versus the number of iterations. Choose the following initial points x_0 = [(2, -1), (0, 0), (1, 2)].\n[5 points] Discussion: compare the three curves. Describe the properties of this optimization problem.\n\nIs it convex/strongly convex?\nIs it smooth?\nDo we have a unique solution here?\nWhich start converges fastest / slowest and why? Relate your observations to the initial distance to U \\cap V and to the contact angle between the two sets at the solution.\n\n\n\n\n\nIllustration of the problem\n\n\nSubgradient methods for Lasso. (10 points)\nConsider the optimization problem\n\n\\min_{x \\in \\mathbb{R}^n} f(x) := \\frac12 \\|Ax - b\\|^2 + \\lambda \\|x\\|_1,\n\nwith variables x \\in \\mathbb{R}^n and problem data A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m and \\lambda &gt; 0. This model is known as Lasso, or Least Squares with l_1 regularization, which encourages sparsity in the solution via the non-smooth penalty \\|x\\|_1 := \\sum_{j=1}^n |x_j|. In this problem, we will explore various subgradient methods for fitting this model.\n\nDerive the subdifferential \\partial f(x) of the objective.\nFind the update rule of the subgradient method and state the computational complexity of applying one update using big O notation in terms of the dimensions.\nLet n = 1000, m = 200 and \\lambda = 0.01. Generate a random matrix A \\in \\mathbb{R}^{m \\times n} with independent Gaussian entries with mean 0 and variance 1/m, and a fixed vector x^* = {\\underbrace{[1, \\ldots, 1}_{\\text{k times}}, \\underbrace{0, \\ldots, 0]}_{\\text{n-k times}}}^T \\in \\mathbb{R}^n. Let k = 5 and then set b = Ax^*. Implement the subgradient method to minimize f(x), initialized at the all-zeros vector. Try different step size rules, including:\n\nconstant step size \\alpha_k = \\alpha\nconstant step length \\alpha_k = \\frac{\\gamma}{\\|g_k\\|_2} (so \\|x^{k+1} - x^k\\|_2 = \\gamma)\nInverse square root \\frac{1}{\\sqrt{k}}\nInverse \\frac1k\nPolyak‚Äôs step length with estimated objective value:\n\n  \\alpha_k = \\frac{f(x_k) - f_k^{\\text{best}} + \\gamma_k}{\\|g_k\\|_2^2}, \\quad \\text{ with} \\sum_{k=1}^\\infty \\gamma_k = \\infty, \\quad \\sum_{k=1}^\\infty \\gamma_k^2 &lt; \\infty\n  \nFor example, one can use \\gamma_k = \\frac{10}{10 + k}. Here f_k^{\\text{best}} - \\gamma_k serves as estimate of f^*. It is better to take \\gamma_k in the same scale as the objective value. One can show, that f_k^{\\text{best}} \\to f^*.\n\nPlot objective value versus iteration curves of different step size rules on the same figure.\nRepeat previous part using a heavy ball term, \\beta_k(x^k - x^{k-1}), added to the subgradient. Try different step size rules as in the previous part and tune the heavy ball parameter \\beta_k = \\beta for faster convergence.\n\n\n\n\nProximal gradient method\n\n[20 points] Proximal Method for Sparse Softmax Regression Softmax regression, also known as multinomial logistic regression, is a generalization of logistic regression to multiple classes. It is used to model categorical outcome variables where each category is mutually exclusive. The softmax function transforms any input vector to the probability-like vector as follows:\n\nP(y = j | x; W) = \\frac{e^{W_j^T x}}{\\sum\\limits_{i=1}^{c} e^{W_i^T x}}\n\n\n\n\nScheme of softmax regression\n\n\nwhere x is the input vector, W is the weight matrix, c is the number of classes, and P(y = j | x; W) is the probability that the input x belongs to class j.\nThe optimization problem for softmax regression is to minimize the negative log-likelihood:\n\n\\min_{W \\in \\mathbb{R}^{c \\times d}} -\\sum_{i=1}^{N} \\log P(y_i | x_i; W) + \\lambda \\| W \\|_1\n\nwhere N is the number of training examples, \\lambda is the regularization parameter, and \\| W \\|_1 is the L1 norm of the weight matrix, which promotes sparsity in the solution. I suggest you to vectorize matrix and add 1-vector norm.\nWe will solve the sparse softmax regression problem using the subgradient method and the proximal gradient method, both incorporating L1 regularization. The proximal gradient method is particularly useful for optimization problems involving non-smooth regularizers like the L1 norm. We will use 3 class classification problem of Predicting Students‚Äô Dropout and Academic Success. In this problem you should start with this colab notebook\n\n[4 points] Write down exact formulation of subgradient method and proximal gradient method here (you can not use any optimization problems in this formulation).\n[6 points] Choose \\lambda = 0. Solve the softmax regression problem using subgradient descent and proximal gradient descent. Find the highest learning (individually), acceptable for both methods to converge. Report convergence curves and report final sparsity of both methods. Draw you conclusions.\n[10 points] Solve non-smooth problem and fill the following table. For each value of \\lambda provide convergence curves.\n\nReport the number of iterations needed to reach specified primal gaps for each method. Present the results in the following markdown table:\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nLearning Rate (\\eta)\nTolerance (\\epsilon)\nNumber of Iterations\nComment(if any)\nFinal Sparsity of the solution\n\\lambda\nFinal test accuracy\n\n\n\n\nSubgradient Descent\n\n10^{-1}\n\n\n\n1e-2\n\n\n\nSubgradient Descent\n\n10^{-2}\n\n\n\n1e-2\n\n\n\nSubgradient Descent\n\n10^{-3}\n\n\n\n1e-2\n\n\n\nSubgradient Descent\n\n10^{-4}\n\n\n\n1e-2\n\n\n\nSubgradient Descent\n\n10^{-5}\n\n\n\n1e-2\n\n\n\nProximal Gradient Descent\n\n10^{-1}\n\n\n\n1e-2\n\n\n\nProximal Gradient Descent\n\n10^{-2}\n\n\n\n1e-2\n\n\n\nProximal Gradient Descent\n\n10^{-3}\n\n\n\n1e-2\n\n\n\nProximal Gradient Descent\n\n10^{-4}\n\n\n\n1e-2\n\n\n\nProximal Gradient Descent\n\n10^{-5}\n\n\n\n1e-2\n\n\n\nSubgradient Descent\n\n10^{-2}\n\n\n\n1e-3\n\n\n\nProximal Gradient Descent\n\n10^{-2}\n\n\n\n1e-3\n\n\n\nSubgradient Descent\n\n10^{-2}\n\n\n\n1e-1\n\n\n\nProximal Gradient Descent\n\n10^{-2}\n\n\n\n1e-1\n\n\n\nSubgradient Descent\n\n10^{-2}\n\n\n\n1\n\n\n\nProximal Gradient Descent\n\n10^{-2}\n\n\n\n1\n\n\n\n\n\n\n\nStochastic gradient methods\n\nVariance reduction for stochastic gradient methods. [20 points]\n[5 points]Open colab notebook. Implement SAG and SVRG method. Consider Linear least squares problem with the following setup\nparams = {\n    \"mu\": 0,\n    \"m\": 50,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"SGD\",\n            \"learning_rate\": 1e-2,\n            \"batch_size\": 2,\n            \"iterations\": 1000,\n        },\n        {\n            \"method\": \"SGD\",\n            \"learning_rate\": 1e-2,\n            \"batch_size\": 50,\n            \"iterations\": 1000,\n        },\n        {\n            \"method\": \"SAG\",\n            \"learning_rate\": 1e-2,\n            \"batch_size\": 2,\n            \"iterations\": 1000,\n        },\n        {\n            \"method\": \"SVRG\",\n            \"learning_rate\": 1e-2,\n            \"epoch_length\": 2,\n            \"batch_size\": 2,\n            \"iterations\": 1000,\n        },\n    ]\n}\n\nresults = run_experiments(params)\n[5 points] Then, consider strongly convex case with:\nparams = {\n    \"mu\": 1e-1,\n    \"m\": 50,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"SGD\",\n            \"learning_rate\": 1e-2,\n            \"batch_size\": 2,\n            \"iterations\": 2000,\n        },\n        {\n            \"method\": \"SGD\",\n            \"learning_rate\": 1e-2,\n            \"batch_size\": 50,\n            \"iterations\": 2000,\n        },\n        {\n            \"method\": \"SAG\",\n            \"learning_rate\": 1e-2,\n            \"batch_size\": 2,\n            \"iterations\": 2000,\n        },\n        {\n            \"method\": \"SVRG\",\n            \"learning_rate\": 1e-2,\n            \"epoch_length\": 2,\n            \"batch_size\": 2,\n            \"iterations\": 2000,\n        },\n    ]\n}\n[5 points] And for the convex binary logistic regression:\nparams = {\n    \"mu\": 0,\n    \"m\": 100,\n    \"n\": 200,\n    \"methods\": [\n        {\n            \"method\": \"SGD\",\n            \"learning_rate\": 1e-2,\n            \"batch_size\": 2,\n            \"iterations\": 2000,\n        },\n        {\n            \"method\": \"SAG\",\n            \"learning_rate\": 1e-2,\n            \"batch_size\": 2,\n            \"iterations\": 2000,\n        },\n        {\n            \"method\": \"SVRG\",\n            \"learning_rate\": 1e-2,\n            \"epoch_length\": 3,\n            \"batch_size\": 2,\n            \"iterations\": 2000,\n        },\n        {\n            \"method\": \"SGD\",\n            \"learning_rate\": 1e-2,\n            \"batch_size\": 100,\n            \"iterations\": 2000,\n        },\n    ]\n}\n[5 points] and strongly convex case\nparams = {\n    \"mu\": 1e-1,\n    \"m\": 100,\n    \"n\": 200,\n    \"methods\": [\n        {\n            \"method\": \"SGD\",\n            \"learning_rate\": 2e-2,\n            \"batch_size\": 2,\n            \"iterations\": 3000,\n        },\n        {\n            \"method\": \"SAG\",\n            \"learning_rate\": 2e-2,\n            \"batch_size\": 2,\n            \"iterations\": 3000,\n        },\n        {\n            \"method\": \"SVRG\",\n            \"learning_rate\": 2e-2,\n            \"epoch_length\": 3,\n            \"batch_size\": 2,\n            \"iterations\": 3000,\n        },\n        {\n            \"method\": \"SGD\",\n            \"learning_rate\": 2e-2,\n            \"batch_size\": 100,\n            \"iterations\": 3000,\n        },\n    ]\n}\nDescribe the obtained convergence and compare methods.\n\n\n\n\n\nNeural network training\n\nAnomaly detection with neural network. [30 points]\nIn this problem we will try to detect anomalies in time series with neural network.\n                        \n                                            \nWe will train the model to reconstruct normal data and when the reconstruction error for the actual data on trained model is high, we report an anomaly. Start with this notebook colab notebook. The default solution is adam and after training it can detect 4 out of 5 anomalies. Train and compare several methods on the same problem. For each method try to find hyperparameters, which ensures at least 3 out of 5 anomalies detection. Present learning curves and anomaly predictions for each method.\n\nSGD with momentum [5 points] from optax\nAdadelta [5 points] from optax\nBFGS [10 points] implemented manually\nMuon optimizer [10 points] implemented manually\n\n\n\n\nBig models\n\nFit the largest model you can on a single GPU. [15 points]\nIn this assignment, you will train a language model (LM) using the TinyStories dataset, focusing on optimizing model performance within the constraints of Google Colab‚Äôs hardware. For the sake of speed, we will do it on the part of the dataset.\nOnce upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. \nBeep was a healthy car because he always had good fuel....\nYour objective is to maximize the size of the model without exceeding the available computational resources (~ 16GB VRAM). You could start with the Hugging Face Transformers library and experiment with various memory optimization techniques, such as (but not limited to):\n * Different batch size\n * Different optimizer\n * Gradient accumulation\n * Activation checkpointing\n * CPU offloading\n * 8bit optimizers\nYou have a baseline of training gpt-2 model prepared at the following colab notebook. You can easily switch it to opt-350m, opt-1.3b, gpt2 etc. You can find a great beginner-level guide on the topic here.\nA long time ago in a galaxy far far away... a little girl named Lily was playing in the garden. She was so excited! She wanted to explore the garden and see what was around her.\nSuddenly, she heard a loud noise. Lily looked up and saw a big, hairy creature. Lily was so excited! She ran to the creature and grabbed it by the arm. The creature was so big and hairy that Lily couldn't help but laugh. \n\nYou have to fill this table with your description/observations.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetup\n# of parameters\nGPU peak memory, MB\nFinal eval loss\nBatch Size\nTime to run 5 epochs, s\nGeneration example\nComment\n\n\n\n\nBaseline (OPT-125M)\n125 M\n9044\n1.928\n8\n442.34\nA long time ago in a galaxy far far away... there was a little girl named Lily. She was three years old and loved to explore. One day, she decided to go for a walk in the park. Lily was so excited to go for a walk. She asked her mom, \"What do you want to do?\" Her mom smiled and said, \"I want to explore the galaxy.\" Lily was so excited to explore the galaxy.\n\n\n\nBaseline (GPT2-S)\n124 M\n13016\n2.001\n8\n487.75\nA long time ago in a galaxy far far away... a little girl named Lily was playing in the garden. She was so excited! She wanted to explore the garden and see what was around her. Suddenly, she heard a loud noise. Lily looked up and saw a big, hairy creature. Lily was so excited! She ran to the creature and grabbed it by the arm. The creature was so big and hairy that Lily couldn't help but laugh.\nThe generation seems more interesting, despite the fact, that eval loss is higher.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor each unique trick for memory optimization, you will get 3 points (maximum 15 points). A combination of tricks is not counted as a unique trick, but will, probably, be necessary to train big models. The maximum grade is bounded with the size of the trained model:\n\nIf the model size you train is &lt;= 125M - you can get a maximum of 6 points.\nIf the model size you train is 126M &lt;= 350M - you can get a maximum of 8 points.\nIf the model size you train is 350M &lt;= 1B - you can get a maximum of 12 points.\nIf you fit 1B model or more - you can get a maximum 15 points.\n\n\n\n\nADMM (Dual methods)\n\nLow‚ÄëRank Matrix Completion via ADMM ¬†[25¬†points]\nBackground.¬†In many applications such as recommender systems, computer vision and system identification, the data matrix is approximately low‚Äërank but only a subset of its entries are observed. Recovering the missing entries can be posed as a convex program that combines a data‚Äëfitting term with the nuclear norm, a convex surrogate for rank.\nWe are given a partially observed matrix M \\in \\mathbb{R}^{m\\times n} and the index set of observed entries \\Omega \\subseteq \\{1,\\dots,m\\} \\times \\{1,\\dots,n\\}. Define the sampling operator P_\\Omega : \\mathbb{R}^{m\\times n}\\to\\mathbb{R}^{m\\times n} by (P_\\Omega(X))_{ij}= X_{ij} if (i,j)\\in\\Omega and 0 otherwise.\nWe consider the optimization problem \n\\min_{X\\in\\mathbb{R}^{m\\times n}}\\;\\frac12\\|P_\\Omega(X-M)\\|_F^2\\; + \\;\\lambda\\|X\\|_*,\n where \\|X\\|_* = \\sum_k \\sigma_k(X) is the nuclear norm.\n\n(a)¬†[10¬†points] Derive a two‚Äëblock ADMM algorithm.\nIntroduce an auxiliary variable Z and rewrite the problem in the form\n\n\\min_{X,Z}\\; \\frac12\\|P_\\Omega(Z-M)\\|_F^2 + \\lambda\\|X\\|_* \\quad\\text{s.t. } X-Z = 0.\n Derive explicit closed‚Äëform expressions for each ADMM update:\n\nX‚Äëupdate: singular‚Äëvalue soft‚Äëthresholding (SVT);\nZ‚Äëupdate: projection onto the observed entries (keep M on \\Omega, average with X elsewhere);\ndual‚Äëvariable update.\n\nState a practical stopping rule based on the primal and dual residuals.\n(b)¬†[10¬†points] Implement the algorithm on synthetic data.\nUse the following set‚Äëup (in¬†Python):\nimport numpy as np\nnp.random.seed(0)\nm, n, r = 50, 40, 3\nU = np.random.randn(m, r)\nV = np.random.randn(n, r)\nM_star = U @ V.T                      # ground‚Äëtruth low‚Äërank matrix\nmask = np.random.rand(m, n) &lt; 0.3     # 30¬†% observations\nnoise = 0.01 * np.random.randn(m, n)\nM = mask * (M_star + noise)           # observed matrix (zeros elsewhere)\nlambda_ = 1 / np.sqrt(max(m, n))\n\nImplement the ADMM algorithm derived in¬†part¬†(a).\nRun it from X^0 = 0 for three penalty parameters \\rho \\in \\{0.1, 1, 10\\}.\nFor each \\rho:\n\nplot (i)¬†the objective value and (ii)¬†the relative reconstruction error \\frac{\\|X^k - M_\\star\\|_F}{\\|M_\\star\\|_F} versus iteration number;\nreport the number of iterations required until \\max(\\|r_{\\mathrm p}^k\\|_F,\\|r_{\\mathrm d}^k\\|_F) \\le 10^{-3}.\n\n\n(c)¬†[5¬†points] Discussion.\nCompare the convergence behaviour across the three values of \\rho. How does \\rho influence the rate at which the primal and dual residuals decrease? Comment on\n\nthe rank of the iterates (after SVT);\nthe trade‚Äëoff between data‚Äëfit and nuclear‚Äënorm penalty as \\lambda varies;\nthe quality of the reconstruction once the stopping criterion is met.\n\nRelate your observations to the theory of ADMM and to the sensitivity of singular‚Äëvalue thresholding to the choice of \\rho.\n\n\n\n\nContinuous time methods\n\nSGD as a splitting scheme and the importance of batches order [40 points]\nBackground: (to be honest you can do the task without reading it)\nThe standard Gradient Descent (GD) method for minimizing f(x) = \\frac{1}{n} \\sum_{i=1}^n f_i(x) can be viewed as an Euler discretization of the gradient flow Ordinary Differential Equation (ODE): \n\\frac{d x}{d t} = -\\nabla f(x) = -\\frac{1}{n} \\sum_{i=1}^n \\nabla f_i(x)\n Stochastic Gradient Descent (SGD), particularly with cycling through mini-batches without replacement, can be interpreted as a splitting scheme applied to this ODE. In a first-order splitting scheme for \\frac{dx}{dt} = A x = \\sum_{i=1}^m A_i x, we approximate the solution x(h) by sequentially applying the flows corresponding to each A_i: x(h) \\approx e^{A_{\\sigma(m)} h} \\ldots e^{A_{\\sigma(1)} h} x_0 for some permutation \\sigma.\nIn the paper authors show that for the linear least squares problem f(x) = \\frac{1}{2n}\\|X x - y\\|^2, where X is split into m row blocks X_i, the corresponding ODE involves matrices A_i = -\\frac{1}{n} X_i^T X_i. If X_i^T = Q_i R_i is the QR decomposition (Q_i has orthonormal columns), let \\Pi_i = I - Q_i Q_i^* be the projector onto the null space of X_i. The paper presents the following result for the asymptotic global error of the splitting scheme:\nTheorem: Let A_i = -\\frac{1}{n} X_i^T X_i for i=1,\\dots,m. Assume each A_i is negative semidefinite and does not have full rank, but their sum A = \\sum A_i does have full rank. Then, for any permutation \\sigma of \\{1, \\dots, m\\}: \n\\lim_{t \\to \\infty}\\| e^{A_{\\sigma(m)}t} \\cdots e^{A_{\\sigma(1)}t} - e^{At}\\| = \\left\\|\\prod_{i=1}^m \\Pi_{\\sigma(i)}\\right\\|\n\nThis error bound depends on the product of projectors \\Pi_i and thus on the order specified by the permutation \\sigma. Since one epoch of SGD corresponds to applying the Euler discretization of each local problem \\frac{dx}{dt} = A_i x sequentially, this suggests that the order in which batches are processed in SGD might affect convergence, especially over many epochs.\nTasks:\n\nInvestigating the Bound Distribution [5 points]\n\nConsider a simple linear least squares problem. \n\\frac{1}{2n}\\|X \\theta - y\\|^2 \\to \\min_{\\theta \\in \\mathbb{R}^{d}}, X \\in \\mathbb{R}^{n \\times d}, y \\in \\mathbb{R}^n\n For example, generate a random matrix X \\in \\mathbb{R}^{80 \\times 20} and a random vector y \\in \\mathbb{R}^{80}.\nSplit X into m=8 batches (row blocks) sequentially X_1, \\ldots, X_8, where each X_i \\in \\mathbb{R}^{10 \\times 20}.\nFor each batch X_i \\in \\mathbb{R}^{10 \\times 20}, you have to compute the projector matrix \\Pi_i = I - Q_i Q_i^* \\in \\mathbb{R}^{20 \\times 20}, where X_i^T = Q_i R_i is the (thin) QR decomposition of X_i^T \\in \\mathbb{R}^{20 \\times 10} (Q_i \\in \\mathbb{R}^{20 \\times r_i}, R_i \\in \\mathbb{R}^{r_i \\times 10}, with r_i = \\text{rank}(X_i) \\le 10).\nCalculate the error bound norm E(\\sigma) = \\|\\prod_{j=1}^m \\Pi_{\\sigma(j)}\\|_2 (where the product is a 20 \\times 20 matrix) for all m! = 8! = 40320 possible permutations \\sigma. Note, that this quantity is a scalar and depends on the order of batches in multiplication (permutation \\sigma), i.e.¬†\\|\\Pi_1 \\Pi_2\\| \\neq \\|\\Pi_2 \\Pi_1\\|.\nPlot a histogram of the distribution of these scalar E(\\sigma) values. Does the order seem to matter significantly in this random case?\n\nMaximizing Order Dependence with adversarial dataset construction [30 points]\n\nModify the structure of the matrix X (or the way it is split into X_i, but you cannot change the number of batches and their size) from Task 1 to create a scenario where the distribution of the error bounds E(\\sigma) has a significantly larger variance (to be precise, the ratio of the maximum to minimum values for different permutations should be maximized). Hint: Think about how the projectors \\Pi_i interact. How could you make the product \\Pi_{\\sigma(m)} \\cdots \\Pi_{\\sigma(1)} very different for different orders \\sigma? Consider cases where the null spaces have specific overlaps or orthogonality properties.\nExplain your reasoning for the modification.\nRepeat the calculation and plotting from Task 1 for your modified problem to demonstrate the increased variance in the error bounds. Report the ratio of the maximum to minimum values for different permutations before and after adversarial dataset construction.\n\nTesting SGD Convergence [5 points]\n\nUsing the adversarial dataset from Task 2, identify two specific permutations: \\sigma_{\\text{low}} with a low error bound E(\\sigma_{\\text{low}}) and \\sigma_{\\text{high}} with a high error bound E(\\sigma_{\\text{high}}).\nImplement SGD for the linear least squares problem \\min_x \\frac{1}{2n} \\|Xx - y\\|^2. Use a fixed, small learning rate (e.g., \\alpha = 0.01/L where L is the Lipschitz constant of the full gradient).\nRun SGD for a sufficient number of epochs (e.g., 50-100), applying the batches deterministically according to the order defined by \\sigma_{\\text{low}} in each epoch. Record the squared error \\|X x_k - y\\|^2 at the end of each epoch k.\nRepeat the SGD run using the fixed batch order defined by \\sigma_{\\text{high}}.\nPlot the convergence curves (squared error vs.¬†epoch number) for both \\sigma_{\\text{low}} and \\sigma_{\\text{high}} on the same graph.\nDiscuss your results. Does the observed convergence speed of SGD correlate with the theoretical asymptotic error bound E(\\sigma)? Does the order of batches appear to matter more in your modified problem compared to the random one?"
  },
  {
    "objectID": "index.html#–≤–µ—Å–µ–Ω–Ω–∏–π-—ç–∫–∑–∞–º–µ–Ω",
    "href": "index.html#–≤–µ—Å–µ–Ω–Ω–∏–π-—ç–∫–∑–∞–º–µ–Ω",
    "title": "–ú–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ú–§–¢–ò 2024-2025",
    "section": "–í–µ—Å–µ–Ω–Ω–∏–π —ç–∫–∑–∞–º–µ–Ω",
    "text": "–í–µ—Å–µ–Ω–Ω–∏–π —ç–∫–∑–∞–º–µ–Ω\nüìú –°–ø–∏—Å–æ–∫ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –∏ —Ç–µ–æ—Ä–µ–º\n–û—Ü–µ–Ω–∫–∞ –∑–∞ —ç–∫–∑–∞–º–µ–Ω —Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è –∏–∑ 4 —á–∞—Å—Ç–µ–π:\n\n–í–æ–ø—Ä–æ—Å—ã –ø–æ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º - 2 –±–∞–ª–ª–∞\n–°–Ω–∞—á–∞–ª–∞ –≤—ã–¥–∞—é—Ç—Å—è 5 —Å–ª—É—á–∞–π–Ω—ã—Ö –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π/—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –∏–∑ —Å–ø–∏—Å–∫–∞. –ù–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–µ—Ç—Å—è 10 –º–∏–Ω—É—Ç. –ü—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –æ—Ç–≤–µ—Ç–µ —Ö–æ—Ç—è –±—ã –Ω–∞ 3 –∏–∑ 5 –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π/—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ —ç–∫–∑–∞–º–µ–Ω –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è –¥–∞–ª—å—à–µ, –∏ –≤—ã –ø–æ–ª—É—á–∞–µ—Ç–µ x ‚àí 3 –±–∞–ª–ª–æ–≤, –≥–¥–µ x ‚Äì —á–∏—Å–ª–æ –≤–µ—Ä–Ω–æ –æ—Ç–≤–µ—á–µ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤. –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ –∑–∞ —ç–∫–∑–∞–º–µ–Ω –≤—ã—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è 0 –±–∞–ª–ª–æ–≤.\n–¢–µ–æ—Ä–µ–º–∞ —Å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º - 3 –±–∞–ª–ª–∞\n–†–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á - 3 –±–∞–ª–ª–∞\n–ü—Ä–∏ —É—Å–ø–µ—à–Ω–æ–π —Å–¥–∞—á–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –≤–∞–º –≤—ã–¥–∞–µ—Ç—Å—è –±–∏–ª–µ—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –Ω–∞ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ, –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞—á–∏. –ù–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –∫ –æ—Ç–≤–µ—Ç—É –¥–∞–µ—Ç—Å—è 30 –º–∏–Ω—É—Ç. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –Ω–∞ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –±—É–¥–µ—Ç –ø–æ —Ç–µ–æ—Ä–µ–º–∞–º –∏–∑ —Å–ø–∏—Å–∫–∞. –î–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ –∑–∞–¥–∞—á–∞–º —Å–æ–≤–µ—Ç—É–µ–º –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –¥–æ–º–∞—à–Ω–∏–µ –∑–∞–¥–∞–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞—á–∏ —Å —Å–µ–º–∏–Ω–∞—Ä–æ–≤. –í –ø—Ä–æ—Ü–µ—Å—Å–µ –±–µ—Å–µ–¥—ã –ø–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–º –ø—É–Ω–∫—Ç–∞–º —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä –º–æ–∂–µ—Ç –∑–∞–¥–∞–≤–∞—Ç—å —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã.\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å - 2 –±–∞–ª–ª–∞\n–ü–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —ç—Ç–∞–ø—ã —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä –∑–∞–¥–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–¥–∞—á—É –∏–ª–∏ –≤–æ–ø—Ä–æ—Å, —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å —Ç–µ–æ—Ä–∏–µ–π. –û—Ç–≤–µ—Ç –Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –≤ 2 –±–∞–ª–ª–∞.\n–í–æ –≤—Ä–µ–º—è –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º –Ω–µ–ª—å–∑—è –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –Ω–∏–∫–∞–∫–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏.\n–í–æ –≤—Ä–µ–º—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –º–æ–∂–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ä—É–∫–æ–ø–∏—Å–Ω—ã–º–∏ –∑–∞–ø–∏—Å—è–º–∏ —Ä–∞–∑–º–µ—Ä–∞ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ê4 –∏–ª–∏ –æ–¥–Ω–æ–≥–æ –ª–∏—Å—Ç–∞ —Å –¥–≤—É—Ö —Å—Ç–æ—Ä–æ–Ω –ê5.\n–ï—Å–ª–∏ –≤–æ –≤—Ä–µ–º—è —ç–∫–∑–∞–º–µ–Ω–∞ —á–µ–ª–æ–≤–µ–∫ –Ω–µ –º–æ–∂–µ—Ç –Ω–∞–ø–∏—Å–∞—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ –ô–µ–Ω—Å–µ–Ω–∞ –¥–ª—è –≤—ã–ø—É–∫–ª–æ–π —Ñ—É–Ω–∫—Ü–∏–∏, —É—Å–ª–æ–≤–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —ç–∫—Å—Ç—Ä–µ–º—É–º–∞ –¥–ª—è –∑–∞–¥–∞—á–∏ –±–µ–∑—É—Å–ª–æ–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –º–µ—Ç–æ–¥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞, –º–µ—Ç–æ–¥ –ù—å—é—Ç–æ–Ω–∞, –º–µ—Ç–æ–¥ —Ç—è–∂–µ–ª–æ–≥–æ —à–∞—Ä–∏–∫–∞ –∑–∞ –∫—É—Ä—Å —Å—Ç–∞–≤–∏—Ç—Å—è 0.\n\n\n\n\n\n\n\n–û—Å–µ–Ω–Ω–∏–π —ç–∫–∑–∞–º–µ–Ω\n\n\n\n\n\nüìú –°–ø–∏—Å–æ–∫ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –∏ —Ç–µ–æ—Ä–µ–º\n–û—Ü–µ–Ω–∫–∞ –∑–∞ —ç–∫–∑–∞–º–µ–Ω —Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è –∏–∑ 4 —á–∞—Å—Ç–µ–π:\n\n–í–æ–ø—Ä–æ—Å—ã –ø–æ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º - 3 –±–∞–ª–ª–∞\n–°–Ω–∞—á–∞–ª–∞ –≤—ã–¥–∞—é—Ç—Å—è 6 —Å–ª—É—á–∞–π–Ω—ã—Ö –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π/—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –∏–∑ —Å–ø–∏—Å–∫–∞. –ù–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–µ—Ç—Å—è 10 –º–∏–Ω—É—Ç. –ü—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –æ—Ç–≤–µ—Ç–µ —Ö–æ—Ç—è –±—ã –Ω–∞ 3 –∏–∑ 6 –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π/—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ —ç–∫–∑–∞–º–µ–Ω –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è –¥–∞–ª—å—à–µ, –∏ –≤—ã –ø–æ–ª—É—á–∞–µ—Ç–µ x ‚àí 3 –±–∞–ª–ª–æ–≤, –≥–¥–µ x ‚Äì —á–∏—Å–ª–æ –≤–µ—Ä–Ω–æ –æ—Ç–≤–µ—á–µ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤. –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ –∑–∞ —ç–∫–∑–∞–º–µ–Ω –≤—ã—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è 0 –±–∞–ª–ª–æ–≤.\n–¢–µ–æ—Ä–µ–º–∞ —Å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º - 2 –±–∞–ª–ª–∞\n–†–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ - 3 –±–∞–ª–ª–∞\n–ü—Ä–∏ —É—Å–ø–µ—à–Ω–æ–π —Å–¥–∞—á–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –≤–∞–º –≤—ã–¥–∞–µ—Ç—Å—è –±–∏–ª–µ—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –Ω–∞ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ, –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞—á–∏. –ù–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –∫ –æ—Ç–≤–µ—Ç—É –¥–∞–µ—Ç—Å—è 30 –º–∏–Ω—É—Ç. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –Ω–∞ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –±—É–¥–µ—Ç –ø–æ —Ç–µ–æ—Ä–µ–º–∞–º –∏–∑ —Å–ø–∏—Å–∫–∞. –î–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ –∑–∞–¥–∞—á–∞–º —Å–æ–≤–µ—Ç—É–µ–º –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –¥–æ–º–∞—à–Ω–∏–µ –∑–∞–¥–∞–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞—á–∏ —Å —Å–µ–º–∏–Ω–∞—Ä–æ–≤. –í –ø—Ä–æ—Ü–µ—Å—Å–µ –±–µ—Å–µ–¥—ã –ø–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–º –ø—É–Ω–∫—Ç–∞–º –ø—Ä–∏–Ω–∏–º–∞—é—â–∏–π –º–æ–∂–µ—Ç –∑–∞–¥–∞–≤–∞—Ç—å —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã.\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å - 2 –±–∞–ª–ª–∞\n–ü–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —ç—Ç–∞–ø—ã –ø—Ä–∏–Ω–∏–º–∞—é—â–∏–π –∑–∞–¥–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–¥–∞—á—É –∏–ª–∏ –≤–æ–ø—Ä–æ—Å, —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å —Ç–µ–æ—Ä–∏–µ–π. –û—Ç–≤–µ—Ç –Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –≤ 2 –±–∞–ª–ª–∞.\n–í–æ –≤—Ä–µ–º—è –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º –Ω–µ–ª—å–∑—è –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –Ω–∏–∫–∞–∫–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏.\n–ü—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –¥–æ–ø. –≤–æ–ø—Ä–æ—Å –Ω–µ–ª—å–∑—è –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —à–ø–∞—Ä–≥–∞–ª–∫–æ–π.\n–ü–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –º–æ–∂–Ω–æ –≤–∑—è—Ç—å —Ä—É–∫–æ–ø–∏—Å–Ω—É—é —à–ø–∞—Ä–≥–∞–ª–∫—É —Ä–∞–∑–º–µ—Ä–æ–º –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ê4 –∏–ª–∏ –æ–¥–Ω–æ–≥–æ –ª–∏—Å—Ç–∞ —Å –¥–≤—É—Ö —Å—Ç–æ—Ä–æ–Ω –ê5. –®–ø–∞—Ä–≥–∞–ª–∫–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –Ω–∞–ø–∏—Å–∞–Ω–∞ –æ—Ç —Ä—É–∫–∏ –≤–∞–º–∏. –ù–µ—Ä—É–∫–æ–ø–∏—Å–Ω—ã–µ —à–ø–∞—Ä–≥–∞–ª–∫–∏ –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç. –ß—É–∂–∏–µ —à–ø–∞—Ä–≥–∞–ª–∫–∏ –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç (—ç—Ç–æ —Å–ª–æ–∂–Ω–µ–µ –∑–∞–º–µ—Ç–∏—Ç—å, –Ω–æ —É –Ω–∞—Å –±—ã–ª–∏ –∫–µ–π—Å—ã –≤ –ø—Ä–æ—à–ª–æ–º –≥–æ–¥—É). –®–ø–∞—Ä–≥–∞–ª–∫—É –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ –æ—Ç–≤–µ—Ç—É –Ω–∞ —Ç–µ–æ—Ä–µ–º—É —Å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º –∏ –∑–∞–¥–∞—á—É.\n–ï—Å–ª–∏ –≤–æ –≤—Ä–µ–º—è –∫–æ–ª–ª–æ–∫–≤–∏—É–º–∞ —á–µ–ª–æ–≤–µ–∫ –Ω–µ –º–æ–∂–µ—Ç –Ω–∞–ø–∏—Å–∞—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ –ô–µ–Ω—Å–µ–Ω–∞ –¥–ª—è –≤—ã–ø—É–∫–ª–æ–π —Ñ—É–Ω–∫—Ü–∏–∏, —É—Å–ª–æ–≤–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —ç–∫—Å—Ç—Ä–µ–º—É–º–∞ –¥–ª—è –∑–∞–¥–∞—á–∏ –±–µ–∑—É—Å–ª–æ–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –º–µ—Ç–æ–¥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞, –º–µ—Ç–æ–¥ –ù—å—é—Ç–æ–Ω–∞, –º–µ—Ç–æ–¥ —Ç—è–∂–µ–ª–æ–≥–æ —à–∞—Ä–∏–∫–∞, —á—Ç–æ —Ç–∞–∫–æ–µ –±–∞—Ç—á –∏ —ç–ø–æ—Ö–∞, –∑–∞ –∫—É—Ä—Å —Å—Ç–∞–≤–∏—Ç—Å—è 0."
  },
  {
    "objectID": "files/mds.html",
    "href": "files/mds.html",
    "title": "",
    "section": "",
    "text": "!pip install -q geopy\n\n\n[notice] A new release of pip is available: 23.2.1 -&gt; 23.3.2\n\n[notice] To update, run: python3.9 -m pip install --upgrade pip\n\n\n\n\n\nimport numpy as np\nfrom geopy.distance import geodesic\n\n# Estimated GPS coordinates for the specified Moscow Metro stations\n# Format: \"Station Name\": (Latitude, Longitude)\nstations_coordinates = {\n    \"Fizteh\": (55.5518, 37.3247),           # Estimated coordinates for Fizteh\n    \"Chkalovskaya\": (55.4523, 37.3926),\n    \"Slavyanski Boulevard\": (55.4347, 37.2814),\n    \"Park Kultury\": (55.4409, 37.3529),\n    \"Komsomolskaya\": (55.4629, 37.3918),\n    \"Yugo-Zapadnaya\": (55.3949, 37.2900),\n    \"Kievskaya\": (55.4440, 37.3356),\n    \"Strogino\": (55.4814, 37.2411),\n    \"Konkovo\": (55.3800, 37.3108),\n    \"VDNKh\": (55.4916, 37.3828),\n    \"Tekstilshiki\": (55.4232, 37.4354)\n}\n\n# Calculate the pairwise distance matrix\ndef calculate_distance_matrix(coords_dict):\n    station_names = list(coords_dict.keys())\n    n_stations = len(station_names)\n    distance_matrix = np.zeros((n_stations, n_stations))\n\n    for i in range(n_stations):\n        for j in range(n_stations):\n            if i != j:\n                distance_matrix[i][j] = geodesic(coords_dict[station_names[i]], coords_dict[station_names[j]]).kilometers\n            else:\n                distance_matrix[i][j] = 0\n\n    return station_names, distance_matrix\n\nstation_names, distance_matrix = calculate_distance_matrix(stations_coordinates)\ndistance_matrix, station_names\n\n\n(array([[ 0.        , 11.8795683 , 13.32121955, 12.47476534, 10.76730899,\n         17.60531703, 12.02141556,  9.45093857, 19.14704757,  7.64111262,\n         15.93607716],\n        [11.8795683 ,  0.        ,  7.30513134,  2.81466242,  1.18120326,\n          9.11269391,  3.72338261, 10.11508331,  9.57219188,  4.41903113,\n          4.22314047],\n        [13.32121955,  7.30513134,  0.        ,  4.57796148,  7.6589319 ,\n          4.46432896,  3.58332329,  5.79065187,  6.36819994,  9.01496207,\n          9.83338314],\n        [12.47476534,  2.81466242,  4.57796148,  0.        ,  3.47233987,\n          6.48795948,  1.14799012,  8.3873321 ,  7.28559471,  5.95292621,\n          5.58202776],\n        [10.76730899,  1.18120326,  7.6589319 ,  3.47233987,  0.        ,\n          9.94233619,  4.13173454,  9.75035755, 10.55879981,  3.24552038,\n          5.21047455],\n        [17.60531703,  9.11269391,  4.46432896,  6.48795948,  9.94233619,\n          0.        ,  6.18218626, 10.11534229,  2.11882318, 12.26352411,\n          9.73385562],\n        [12.02141556,  3.72338261,  3.58332329,  1.14799012,  4.13173454,\n          6.18218626,  0.        ,  7.28489644,  7.29627254,  6.08240265,\n          6.72859259],\n        [ 9.45093857, 10.11508331,  5.79065187,  8.3873321 ,  9.75035755,\n         10.11534229,  7.28489644,  0.        , 12.12071314,  9.02961058,\n         13.89681105],\n        [19.14704757,  9.57219188,  6.36819994,  7.28559471, 10.55879981,\n          2.11882318,  7.29627254, 12.12071314,  0.        , 13.23410172,\n          9.24357874],\n        [ 7.64111262,  4.41903113,  9.01496207,  5.95292621,  3.24552038,\n         12.26352411,  6.08240265,  9.02961058, 13.23410172,  0.        ,\n          8.31043471],\n        [15.93607716,  4.22314047,  9.83338314,  5.58202776,  5.21047455,\n          9.73385562,  6.72859259, 13.89681105,  9.24357874,  8.31043471,\n          0.        ]]),\n ['Fizteh',\n  'Chkalovskaya',\n  'Slavyanski Boulevard',\n  'Park Kultury',\n  'Komsomolskaya',\n  'Yugo-Zapadnaya',\n  'Kievskaya',\n  'Strogino',\n  'Konkovo',\n  'VDNKh',\n  'Tekstilshiki'])\n\n\n\nimport matplotlib.pyplot as plt\n\ndef gradient_descent_mds(D, dimensions=2, learning_rate=0.01, iterations=1000):\n    \"\"\"\n    Perform Multidimensional Scaling using Gradient Descent.\n\n    :param D: NxN distance matrix.\n    :param dimensions: Number of dimensions for the output coordinates.\n    :param learning_rate: Learning rate for gradient descent.\n    :param iterations: Number of iterations.\n    :return: Nx2 matrix of coordinates.\n    \"\"\"\n    N = D.shape[0]\n    # Random initialization of coordinates\n    X = np.random.rand(N, dimensions)\n\n    for iteration in range(iterations):\n        # Compute distance matrix for current coordinates\n        D_hat = np.sqrt(np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2, axis=2))\n\n        # Compute gradient\n        delta = D_hat - D\n        for i in range(N):\n            for j in range(N):\n                if i != j:\n                    grad = (delta[i, j] / D_hat[i, j]) * (X[i, :] - X[j, :])\n                    X[i, :] -= learning_rate * grad\n\n    return X\n\n# Perform MDS\ncoordinates_mds = gradient_descent_mds(distance_matrix)\n\n# Plotting the results\nplt.figure(figsize=(10, 8))\nplt.scatter(coordinates_mds[:, 0], coordinates_mds[:, 1])\nfor i, name in enumerate(station_names):\n    plt.annotate(name, (coordinates_mds[i, 0], coordinates_mds[i, 1]))\nplt.title(\"2D Representation of Moscow Metro Stations using MDS\")\nplt.xlabel(\"X Coordinate\")\nplt.ylabel(\"Y Coordinate\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom geopy.distance import geodesic\n\n# Estimated GPS coordinates for the specified Moscow Metro stations\nstations_coordinates = {\n    \"Fizteh\": (55.5518, 37.3247),\n    \"Chkalovskaya\": (55.4523, 37.3926),\n    \"Slavyanski Boulevard\": (55.4347, 37.2814),\n    \"Park Kultury\": (55.4409, 37.3529),\n    \"Komsomolskaya\": (55.4629, 37.3918),\n    \"Yugo-Zapadnaya\": (55.3949, 37.2900),\n    \"Kievskaya\": (55.4440, 37.3356),\n    \"Strogino\": (55.4814, 37.2411),\n    \"Konkovo\": (55.3800, 37.3108),\n    \"VDNKh\": (55.4916, 37.3828),\n    \"Tekstilshiki\": (55.4232, 37.4354)\n}\n\n# Calculate the pairwise distance matrix\ndef calculate_distance_matrix(coords_dict):\n    station_names = list(coords_dict.keys())\n    n_stations = len(station_names)\n    distance_matrix = np.zeros((n_stations, n_stations))\n\n    for i in range(n_stations):\n        for j in range(n_stations):\n            if i != j:\n                distance_matrix[i][j] = geodesic(coords_dict[station_names[i]], coords_dict[station_names[j]]).kilometers\n            else:\n                distance_matrix[i][j] = 0\n\n    return station_names, distance_matrix\n\nstation_names, distance_matrix = calculate_distance_matrix(stations_coordinates)\n\n# Gradient Descent MDS with data collection for animation\ndef gradient_descent_mds(D, dimensions=2, learning_rate=0.01, iterations=1000):\n    N = D.shape[0]\n    X = np.random.rand(N, dimensions)\n\n    # Records for animation\n    positions_record = []\n    loss_record = []\n\n    for iteration in range(iterations):\n        D_hat = np.sqrt(np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2, axis=2))\n        delta = D_hat - D\n        loss = np.sum(delta**2)  # Loss calculation\n        positions_record.append(X.copy())\n        loss_record.append(loss)\n\n        # Gradient descent update\n        for i in range(N):\n            for j in range(N):\n                if i != j:\n                    grad = (delta[i, j] / D_hat[i, j]) * (X[i, :] - X[j, :])\n                    X[i, :] -= learning_rate * grad\n\n    return positions_record, loss_record\n\n# Perform MDS and collect data for animation\npositions_record, loss_record = gradient_descent_mds(distance_matrix, iterations=100)\n\n# Function to update each frame in the animation\ndef update_frame(num, positions_record, loss_record, station_names, scat, line, ax1, ax2):\n    # Clear previous station labels and tails\n    ax1.clear()\n    ax1.set_xlim(-10, 10)\n    ax1.set_ylim(-10, 10)\n    ax1.set_title('Station Positions')\n    ax1.set_xlabel('X Coordinate')\n    ax1.set_ylabel('Y Coordinate')\n\n    # Update station positions and draw tails\n    scat.set_offsets(positions_record[num])\n    for i, name in enumerate(station_names):\n        # Draw the tail for each station\n        if num &gt; 0:\n            for past in range(num):\n                ax1.plot([positions_record[past][i, 0], positions_record[past+1][i, 0]],\n                         [positions_record[past][i, 1], positions_record[past+1][i, 1]],\n                         color='gray', alpha=0.5)\n\n        # Label the final position of the station\n        if num == len(positions_record) - 1:\n            ax1.text(positions_record[num][i, 0], positions_record[num][i, 1], name, fontsize=8)\n\n    # Update loss evolution plot\n    line.set_data(range(num + 1), loss_record[:num + 1])\n    ax2.set_xlim(0, 100)\n    ax2.set_ylim(min(loss_record), max(loss_record))\n    ax2.set_title('Loss Evolution')\n    ax2.set_xlabel('Iteration')\n    ax2.set_ylabel('Loss')\n\n    return scat, line\n\n# Create the figure for animation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n# Scatter plot for station positions\nscat = ax1.scatter([], [], s=30)\nax1.set_xlim(-10, 10)\nax1.set_ylim(-10, 10)\nax1.set_title('Station Positions')\nax1.set_xlabel('X Coordinate')\nax1.set_ylabel('Y Coordinate')\n\n# Line plot for loss evolution\nline, = ax2.semilogy([], [], lw=2)\nax2.set_xlim(0, 100)\nax2.set_ylim(min(loss_record), max(loss_record))\nax2.set_title('Loss Evolution')\nax2.set_xlabel('Iteration')\nax2.set_ylabel('Loss')\n\n# Creating the animation\nani = animation.FuncAnimation(fig, update_frame, frames=100, fargs=(positions_record, loss_record, station_names, scat, line, ax1, ax2), blit=False, repeat=False)\n\n# Saving the animation\nani.save('moscow_metro_mds_animation.mp4', writer='ffmpeg', fps=10)\n\n\n\n\n\n\n\n\n\npositions_record[-1]\n\narray([[ 8.02981262,  8.63001461],\n       [ 2.25518938, -2.67734712],\n       [-3.33834951,  1.74376926],\n       [-0.07801013, -1.44497825],\n       [ 3.09715112, -2.01602395],\n       [-6.46246911, -1.07594008],\n       [-0.48882653, -0.35399986],\n       [-1.32473107,  7.16441546],\n       [-6.86842661, -3.2663034 ],\n       [ 5.7281098 ,  1.15859194],\n       [ 4.50067386, -4.182293  ]])\n\n\n\nnp.max()\n\n100"
  }
]