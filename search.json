[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Методы оптимизации. МФТИ 2023-2024",
    "section": "",
    "text": "Методы оптимизации. МФТИ 2023-2024\nКурс представляет собой систематическое введение в разные области оптимизации. Рассматриваются начала выпуклого анализа, излагаются современные результаты и подходы в решении прикладных задач оптимизации. Курс содержит набор теоретических основ для того, чтобы понимать почему и как определенные методы работают. В начале курса основной упор делается на теоретический аппарат, в конце уделяется большее внимание методам оптимизации. Каждое занятие начинается с тестирования по материалам предыдущего занятия. Первая часть курса больше сфокусирована не теорию, необходимую для последующего использования. Во второй части делается упор на методы оптимизации, начиная c классических результатов, заканчивая самыми актуальными приложениями.\n                        \n                                            \n\n\nКоманда курса\n\n\n    \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Даниил Меркулов\n                    \n                    Преподаватель\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Александр Тришин\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Денис Рубцов\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Илья Забара\n                    \n                    Ассистент\n                  \n                \n              \n        \n    \n\n\nNo matching items\n\n\n\n\nОценка за курс\n\nКурс длится 2 семестра. За каждый семестр отдельная оценка\nИтоговая оценка за курс считается по формуле 50% работа в семестре + 50% экзамен + бонусы\nРабота в семестре складывается из еженедельных тестов и домашней работы по формуле 50/50\nЭкзамен проводится в конце семестра и состоит в устном опросе формулировок, проверке доказательства теоремы, задачи и доп.вопроса\nВ качестве бонусов засчитываются коммиты в основной репозиторий с материалами, проектная работа, а также другая образовательная активность, о которой можно договориться в индивидуальном порядке"
  },
  {
    "objectID": "files/mds.html",
    "href": "files/mds.html",
    "title": "",
    "section": "",
    "text": "!pip install -q geopy\n\n\n[notice] A new release of pip is available: 23.2.1 -&gt; 23.3.2\n[notice] To update, run: python3.9 -m pip install --upgrade pip\n\n\n\nimport numpy as np\nfrom geopy.distance import geodesic\n\n# Estimated GPS coordinates for the specified Moscow Metro stations\n# Format: \"Station Name\": (Latitude, Longitude)\nstations_coordinates = {\n    \"Fizteh\": (55.5518, 37.3247),           # Estimated coordinates for Fizteh\n    \"Chkalovskaya\": (55.4523, 37.3926),\n    \"Slavyanski Boulevard\": (55.4347, 37.2814),\n    \"Park Kultury\": (55.4409, 37.3529),\n    \"Komsomolskaya\": (55.4629, 37.3918),\n    \"Yugo-Zapadnaya\": (55.3949, 37.2900),\n    \"Kievskaya\": (55.4440, 37.3356),\n    \"Strogino\": (55.4814, 37.2411),\n    \"Konkovo\": (55.3800, 37.3108),\n    \"VDNKh\": (55.4916, 37.3828),\n    \"Tekstilshiki\": (55.4232, 37.4354)\n}\n\n# Calculate the pairwise distance matrix\ndef calculate_distance_matrix(coords_dict):\n    station_names = list(coords_dict.keys())\n    n_stations = len(station_names)\n    distance_matrix = np.zeros((n_stations, n_stations))\n\n    for i in range(n_stations):\n        for j in range(n_stations):\n            if i != j:\n                distance_matrix[i][j] = geodesic(coords_dict[station_names[i]], coords_dict[station_names[j]]).kilometers\n            else:\n                distance_matrix[i][j] = 0\n\n    return station_names, distance_matrix\n\nstation_names, distance_matrix = calculate_distance_matrix(stations_coordinates)\ndistance_matrix, station_names\n\n\n(array([[ 0.        , 11.8795683 , 13.32121955, 12.47476534, 10.76730899,\n         17.60531703, 12.02141556,  9.45093857, 19.14704757,  7.64111262,\n         15.93607716],\n        [11.8795683 ,  0.        ,  7.30513134,  2.81466242,  1.18120326,\n          9.11269391,  3.72338261, 10.11508331,  9.57219188,  4.41903113,\n          4.22314047],\n        [13.32121955,  7.30513134,  0.        ,  4.57796148,  7.6589319 ,\n          4.46432896,  3.58332329,  5.79065187,  6.36819994,  9.01496207,\n          9.83338314],\n        [12.47476534,  2.81466242,  4.57796148,  0.        ,  3.47233987,\n          6.48795948,  1.14799012,  8.3873321 ,  7.28559471,  5.95292621,\n          5.58202776],\n        [10.76730899,  1.18120326,  7.6589319 ,  3.47233987,  0.        ,\n          9.94233619,  4.13173454,  9.75035755, 10.55879981,  3.24552038,\n          5.21047455],\n        [17.60531703,  9.11269391,  4.46432896,  6.48795948,  9.94233619,\n          0.        ,  6.18218626, 10.11534229,  2.11882318, 12.26352411,\n          9.73385562],\n        [12.02141556,  3.72338261,  3.58332329,  1.14799012,  4.13173454,\n          6.18218626,  0.        ,  7.28489644,  7.29627254,  6.08240265,\n          6.72859259],\n        [ 9.45093857, 10.11508331,  5.79065187,  8.3873321 ,  9.75035755,\n         10.11534229,  7.28489644,  0.        , 12.12071314,  9.02961058,\n         13.89681105],\n        [19.14704757,  9.57219188,  6.36819994,  7.28559471, 10.55879981,\n          2.11882318,  7.29627254, 12.12071314,  0.        , 13.23410172,\n          9.24357874],\n        [ 7.64111262,  4.41903113,  9.01496207,  5.95292621,  3.24552038,\n         12.26352411,  6.08240265,  9.02961058, 13.23410172,  0.        ,\n          8.31043471],\n        [15.93607716,  4.22314047,  9.83338314,  5.58202776,  5.21047455,\n          9.73385562,  6.72859259, 13.89681105,  9.24357874,  8.31043471,\n          0.        ]]),\n ['Fizteh',\n  'Chkalovskaya',\n  'Slavyanski Boulevard',\n  'Park Kultury',\n  'Komsomolskaya',\n  'Yugo-Zapadnaya',\n  'Kievskaya',\n  'Strogino',\n  'Konkovo',\n  'VDNKh',\n  'Tekstilshiki'])\n\n\n\nimport matplotlib.pyplot as plt\n\ndef gradient_descent_mds(D, dimensions=2, learning_rate=0.01, iterations=1000):\n    \"\"\"\n    Perform Multidimensional Scaling using Gradient Descent.\n\n    :param D: NxN distance matrix.\n    :param dimensions: Number of dimensions for the output coordinates.\n    :param learning_rate: Learning rate for gradient descent.\n    :param iterations: Number of iterations.\n    :return: Nx2 matrix of coordinates.\n    \"\"\"\n    N = D.shape[0]\n    # Random initialization of coordinates\n    X = np.random.rand(N, dimensions)\n\n    for iteration in range(iterations):\n        # Compute distance matrix for current coordinates\n        D_hat = np.sqrt(np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2, axis=2))\n\n        # Compute gradient\n        delta = D_hat - D\n        for i in range(N):\n            for j in range(N):\n                if i != j:\n                    grad = (delta[i, j] / D_hat[i, j]) * (X[i, :] - X[j, :])\n                    X[i, :] -= learning_rate * grad\n\n    return X\n\n# Perform MDS\ncoordinates_mds = gradient_descent_mds(distance_matrix)\n\n# Plotting the results\nplt.figure(figsize=(10, 8))\nplt.scatter(coordinates_mds[:, 0], coordinates_mds[:, 1])\nfor i, name in enumerate(station_names):\n    plt.annotate(name, (coordinates_mds[i, 0], coordinates_mds[i, 1]))\nplt.title(\"2D Representation of Moscow Metro Stations using MDS\")\nplt.xlabel(\"X Coordinate\")\nplt.ylabel(\"Y Coordinate\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom geopy.distance import geodesic\n\n# Estimated GPS coordinates for the specified Moscow Metro stations\nstations_coordinates = {\n    \"Fizteh\": (55.5518, 37.3247),\n    \"Chkalovskaya\": (55.4523, 37.3926),\n    \"Slavyanski Boulevard\": (55.4347, 37.2814),\n    \"Park Kultury\": (55.4409, 37.3529),\n    \"Komsomolskaya\": (55.4629, 37.3918),\n    \"Yugo-Zapadnaya\": (55.3949, 37.2900),\n    \"Kievskaya\": (55.4440, 37.3356),\n    \"Strogino\": (55.4814, 37.2411),\n    \"Konkovo\": (55.3800, 37.3108),\n    \"VDNKh\": (55.4916, 37.3828),\n    \"Tekstilshiki\": (55.4232, 37.4354)\n}\n\n# Calculate the pairwise distance matrix\ndef calculate_distance_matrix(coords_dict):\n    station_names = list(coords_dict.keys())\n    n_stations = len(station_names)\n    distance_matrix = np.zeros((n_stations, n_stations))\n\n    for i in range(n_stations):\n        for j in range(n_stations):\n            if i != j:\n                distance_matrix[i][j] = geodesic(coords_dict[station_names[i]], coords_dict[station_names[j]]).kilometers\n            else:\n                distance_matrix[i][j] = 0\n\n    return station_names, distance_matrix\n\nstation_names, distance_matrix = calculate_distance_matrix(stations_coordinates)\n\n# Gradient Descent MDS with data collection for animation\ndef gradient_descent_mds(D, dimensions=2, learning_rate=0.01, iterations=1000):\n    N = D.shape[0]\n    X = np.random.rand(N, dimensions)\n\n    # Records for animation\n    positions_record = []\n    loss_record = []\n\n    for iteration in range(iterations):\n        D_hat = np.sqrt(np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2, axis=2))\n        delta = D_hat - D\n        loss = np.sum(delta**2)  # Loss calculation\n        positions_record.append(X.copy())\n        loss_record.append(loss)\n\n        # Gradient descent update\n        for i in range(N):\n            for j in range(N):\n                if i != j:\n                    grad = (delta[i, j] / D_hat[i, j]) * (X[i, :] - X[j, :])\n                    X[i, :] -= learning_rate * grad\n\n    return positions_record, loss_record\n\n# Perform MDS and collect data for animation\npositions_record, loss_record = gradient_descent_mds(distance_matrix, iterations=100)\n\n# Function to update each frame in the animation\ndef update_frame(num, positions_record, loss_record, station_names, scat, line, ax1, ax2):\n    # Clear previous station labels and tails\n    ax1.clear()\n    ax1.set_xlim(-10, 10)\n    ax1.set_ylim(-10, 10)\n    ax1.set_title('Station Positions')\n    ax1.set_xlabel('X Coordinate')\n    ax1.set_ylabel('Y Coordinate')\n\n    # Update station positions and draw tails\n    scat.set_offsets(positions_record[num])\n    for i, name in enumerate(station_names):\n        # Draw the tail for each station\n        if num &gt; 0:\n            for past in range(num):\n                ax1.plot([positions_record[past][i, 0], positions_record[past+1][i, 0]],\n                         [positions_record[past][i, 1], positions_record[past+1][i, 1]],\n                         color='gray', alpha=0.5)\n\n        # Label the final position of the station\n        if num == len(positions_record) - 1:\n            ax1.text(positions_record[num][i, 0], positions_record[num][i, 1], name, fontsize=8)\n\n    # Update loss evolution plot\n    line.set_data(range(num + 1), loss_record[:num + 1])\n    ax2.set_xlim(0, 100)\n    ax2.set_ylim(min(loss_record), max(loss_record))\n    ax2.set_title('Loss Evolution')\n    ax2.set_xlabel('Iteration')\n    ax2.set_ylabel('Loss')\n\n    return scat, line\n\n# Create the figure for animation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n# Scatter plot for station positions\nscat = ax1.scatter([], [], s=30)\nax1.set_xlim(-10, 10)\nax1.set_ylim(-10, 10)\nax1.set_title('Station Positions')\nax1.set_xlabel('X Coordinate')\nax1.set_ylabel('Y Coordinate')\n\n# Line plot for loss evolution\nline, = ax2.semilogy([], [], lw=2)\nax2.set_xlim(0, 100)\nax2.set_ylim(min(loss_record), max(loss_record))\nax2.set_title('Loss Evolution')\nax2.set_xlabel('Iteration')\nax2.set_ylabel('Loss')\n\n# Creating the animation\nani = animation.FuncAnimation(fig, update_frame, frames=100, fargs=(positions_record, loss_record, station_names, scat, line, ax1, ax2), blit=False, repeat=False)\n\n# Saving the animation\nani.save('moscow_metro_mds_animation.mp4', writer='ffmpeg', fps=10)\n\n\n\n\n\n\n\n\n\npositions_record[-1]\n\narray([[ 8.02981262,  8.63001461],\n       [ 2.25518938, -2.67734712],\n       [-3.33834951,  1.74376926],\n       [-0.07801013, -1.44497825],\n       [ 3.09715112, -2.01602395],\n       [-6.46246911, -1.07594008],\n       [-0.48882653, -0.35399986],\n       [-1.32473107,  7.16441546],\n       [-6.86842661, -3.2663034 ],\n       [ 5.7281098 ,  1.15859194],\n       [ 4.50067386, -4.182293  ]])\n\n\n\nnp.max()\n\n100"
  },
  {
    "objectID": "files/nanogpt.html",
    "href": "files/nanogpt.html",
    "title": "",
    "section": "",
    "text": "# Re-import necessary libraries and re-define data since the execution state was reset\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom datetime import datetime\nimport pandas as pd\n\n# Data from the table\ndata = {\n    \"Record Time (minutes)\": [45, 31.4, 24.9, 22.3, 15.2, 13.1, 12.0, 10.8, 8.2, 7.8, 7.2, 5.03, 4.66],\n    \"Description\": [\n        \"llm.c baseline\",\n        \"Architectural modernizations & tuned learning rate\",\n        \"Introduced the Muon optimizer\",\n        \"Muon improvements\",\n        \"Pad embeddings & architectural improvements\",\n        \"Distributed the overhead of Muon\",\n        \"Upgraded PyTorch from 2.4.1 to 2.5.0\",\n        \"Untied embed and lm_head\",\n        \"Shortcuts & tweaks\",\n        \"Bfloat16 activations\",\n        \"U-net & 2x lr\",\n        \"FlexAttention\",\n        \"Attention window warmup\"\n    ],\n    \"Date\": [\n        \"05/28/24\", \"06/06/24\", \"10/04/24\", \"10/11/24\", \"10/14/24\", \"10/18/24\", \"10/18/24\", \n        \"11/03/24\", \"11/06/24\", \"11/08/24\", \"11/10/24\", \"11/19/24\", \"11/24/24\"\n    ]\n}\n\n# Convert to a DataFrame\ndf = pd.DataFrame(data)\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%m/%d/%y\")\n\n# Sort the data by date to ensure proper plotting\ndf = df.sort_values(\"Date\")\n\n# Prepare data for plotting\ndates = df[\"Date\"]\ntimes = df[\"Record Time (minutes)\"]\ndescriptions = df[\"Description\"]\n\n# Adjust the plot to spread labels further and use more of the available space\nplt.figure(figsize=(12, 6))\nplt.plot(dates, times, linestyle=\"--\", zorder=0)\nplt.scatter(dates, times, label=\"Training Time\", marker=\"o\", s=70, zorder=100)\n\n# Add labels with arrows for clear separation, spreading them out further\nfor i, (date, time, desc) in enumerate(zip(dates, times, descriptions)):\n    # Larger offset for labels\n    if i &lt;= 1:\n        y_offset = 0\n        x_offset = pd.Timedelta(days=5)\n    elif desc == \"U-net & 2x lr\":\n        y_offset = 16 if i % 2 == 0 else 7\n        y_offset = int(y_offset*(1 - i/20))*0.8\n        x_offset = pd.Timedelta(days=0 if i % 2 == 0 else -100)\n    else:\n        y_offset = 16 if i % 2 == 0 else 7\n        y_offset = int(y_offset*(1 - i/20))\n        x_offset = pd.Timedelta(days=0 if i % 2 == 0 else -100)\n    # Plot the arrow\n    plt.annotate(\n        desc,\n        xy=(date, time),\n        xytext=(date + x_offset, time + y_offset),\n        arrowprops=dict(facecolor='black', arrowstyle=\"-\", lw=0.5),\n        fontsize=14,\n        bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"gray\", facecolor=\"white\", alpha=0.9, zorder=-100)\n    )\n\n# Formatting the plot\nplt.title(\"Ускорение обучения NanoGPT - 125M\")\nplt.xlabel(\"Дата, 2024\")\nplt.ylabel(\"Время обучения на 8xH100, минуты\")\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%d.%m\"))\nplt.gca().xaxis.set_major_locator(mda tes.MonthLocator())\nplt.xticks(rotation=45)\nplt.grid(alpha=0.3, linestyle=\":\")\n# plt.legend()\nplt.tight_layout()\n\n# Show the plot\nplt.savefig(\"nanogpt_speedrun.pdf\")\nplt.show()"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "",
    "section": "",
    "text": "Linear algebra basics\n\n[10 points] Effect of Diagonal Scaling on Rank Let A \\in \\mathbb{R}^{n \\times n} be a matrix with rank r. Suppose D \\in \\mathbb{R}^{n \\times n} is a diagonal matrix. Determine the rank of the product DA. Explain your reasoning.\n[20 points] Find SVD of the following matrices:\n\nA = \\begin{bmatrix} 1\\\\2\\\\3 \\end{bmatrix}\nA = \\begin{bmatrix} 1 & 4\\\\4 & 8\\\\3 & 8 \\end{bmatrix}\nA = \\begin{bmatrix} 0 & 0\\\\x & 0\\\\0 & 0 \\end{bmatrix}, where x is the sum of your birthdate numbers (day + month)\n\n[10 points] Assume we have a set of data points x^{(i)}\\in\\mathbb{R}^{n},\\,i=1,\\dots,m, and decide to represent this data as a matrix\n\nX =\n     \\begin{pmatrix}\n     | & & | \\\\\n     x^{(1)} & \\dots & x^{(m)} \\\\\n     | & & | \\\\\n     \\end{pmatrix} \\in \\mathbb{R}^{n \\times m}.\n\nWe suppose that \\text{rank}\\,X = r.\nIn the problem below, we ask you to find the rank of some matrix M related to X. In particular, you need to find relation between \\text{rank}\\,X = r and \\text{rank}\\,M, e.g., that the rank of M is always larger/smaller than the rank of X or that \\text{rank}\\,M = \\text{rank}\\,X \\big / 35. Please support your answer with legitimate arguments and make the answer as accurate as possible.\nNote that depending on the structure of the matrix X, border cases are possible. Make sure to cover them in your answer correctly.\nIn applied statistics and machine learning, data is often normalized. One particularly popular strategy is to subtract the estimated mean \\mu and divide by the square root of the estimated variance \\sigma^2. i.e.\n\nx \\rightarrow (x - \\mu) \\big / \\sigma.\n\nAfter the normalization, we get a new matrix\n\n     \\begin{split}\n     Y &:=\n     \\begin{pmatrix}\n         | & & | \\\\\n         y^{(1)} & \\dots & y^{(m)} \\\\\n         | & & | \\\\\n     \\end{pmatrix},\\\\\n     y^{(i)} &:= \\frac{x^{(i)} - \\frac{1}{m}\\sum_{j=1}^{m} x^{(j)}}{\\sqrt{\\frac{1}{m}\\sum_{j=1}^{m} \\left(x^{(j)}\\right)^2 - \\left(\\frac{1}{m}\\sum_{j=1}^{m} x^{(j)}\\right)^2}}.\n     \\end{split}\n\nWhat is the rank of Y if \\text{rank} \\; X = r?\nImage Compression with Truncated SVD [10 points] Explore image compression using Truncated Singular Value Decomposition (SVD). Understand how varying the number of singular values affects the quality of the compressed image. Implement a Python script to compress a grayscale image using Truncated SVD and visualize the compression quality.\n\nTruncated SVD: Decomposes an image A into U, S, and V matrices. The compressed image is reconstructed using a subset of singular values.\nMathematical Representation: \nA \\approx U_k \\Sigma_k V_k^T\n\n\nU_k and V_k are the first k columns of U and V, respectively.\n\\Sigma_k is a diagonal matrix with the top k singular values.\nRelative Error: Measures the fidelity of the compressed image compared to the original.\n\n\n  \\text{Relative Error} = \\frac{\\| A - A_k \\|}{\\| A \\|}\n  \n\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np\nfrom skimage import io, color\nimport requests\nfrom io import BytesIO\n\ndef download_image(url):\n    response = requests.get(url)\n    img = io.imread(BytesIO(response.content))\n    return color.rgb2gray(img)  # Convert to grayscale\n\ndef update_plot(i, img_plot, error_plot, U, S, V, original_img, errors, ranks, ax1, ax2):\n    # Adjust rank based on the frame index\n    if i &lt; 70:\n        rank = i + 1\n    else:\n        rank = 70 + (i - 69) * 10\n\n\n    reconstructed_img = ... # YOUR CODE HERE \n\n    # Calculate relative error\n    relative_error = ... # YOUR CODE HERE\n    errors.append(relative_error)\n    ranks.append(rank)\n\n    # Update the image plot and title\n    img_plot.set_data(reconstructed_img)\n    ax1.set_title(f\"Image compression with SVD\\n Rank {rank}; Relative error {relative_error:.2f}\")\n\n    # Remove axis ticks and labels from the first subplot (ax1)\n    ax1.set_xticks([])\n    ax1.set_yticks([])\n\n    # Update the error plot\n    error_plot.set_data(ranks, errors)\n    ax2.set_xlim(1, len(S))\n    ax2.grid(linestyle=\":\")\n    ax2.set_ylim(1e-4, 0.5)\n    ax2.set_ylabel('Relative Error')\n    ax2.set_xlabel('Rank')\n    ax2.set_title('Relative Error over Rank')\n    ax2.semilogy()\n\n    # Set xticks to show rank numbers\n    ax2.set_xticks(range(1, len(S)+1, max(len(S)//10, 1)))  # Adjust the step size as needed\n    plt.tight_layout()\n\n    return img_plot, error_plot\n\n\ndef create_animation(image, filename='svd_animation.mp4'):\n    U, S, V = np.linalg.svd(image, full_matrices=False)\n    errors = []\n    ranks = []\n\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 8))\n    img_plot = ax1.imshow(image, cmap='gray', animated=True)\n    error_plot, = ax2.plot([], [], 'r-', animated=True)  # Initial empty plot for errors\n\n    # Add watermark\n    ax1.text(1, 1.02, '@fminxyz', transform=ax1.transAxes, color='gray', va='bottom', ha='right', fontsize=9)\n\n    # Determine frames for the animation\n    initial_frames = list(range(70))  # First 70 ranks\n    subsequent_frames = list(range(70, len(S), 10))  # Every 10th rank after 70\n    frames = initial_frames + subsequent_frames\n\n    ani = animation.FuncAnimation(fig, update_plot, frames=len(frames), fargs=(img_plot, error_plot, U, S, V, image, errors, ranks, ax1, ax2), interval=50, blit=True)\n    ani.save(filename, writer='ffmpeg', fps=8, dpi=300)\n\n# URL of the image\nurl = \"\"\n\n# Download the image and create the animation\nimage = download_image(url)\ncreate_animation(image)\n\n\n\nMatrix calculus\n\n[10 points] Given a matrix A of size m \\times n and a vector x of size n \\times 1, compute the gradient of the function f(x) = \\text{tr}(A^T A x x^T) with respect to x.\n[10 points] Find the gradient \\nabla f(x) and hessian f''(x), if f(x) = \\dfrac{1}{2} \\Vert Ax - b\\Vert^2_2.\n[10 points] Find the gradient \\nabla f(x) and hessian f''(x), if \nf(x) = \\frac1m \\sum\\limits_{i=1}^m \\log \\left( 1 + \\exp(a_i^{T}x) \\right) + \\frac{\\mu}{2}\\Vert x\\Vert _2^2, \\; a_i, x \\in \\mathbb R^n, \\; \\mu&gt;0\n\n[10 points] Compute the gradient \\nabla_A f(A) of the trace of the matrix exponential function f(A) = \\text{tr}(e^A) with respect to A. Hint: hint: Use the definition of the matrix exponential. Use the definition of the differential df = f(A + dA) - f(A) + o(\\Vert dA \\Vert) with the limit \\Vert dA \\Vert \\to 0.\n[10 points] Calculate the first and the second derivative of the following function f : S \\to \\mathbb{R}\n\nf(t) = \\text{det}(A − tI_n),\n\n\n\n\nAutomatic differentiation\n\n[10 points] You will work with the following function for this exercise, \nf(x,y)=e^{−\\left(sin(x)−cos(y)\\right)^2}\n\nDraw the computational graph for the function. Note, that it should contain only primitive operations - you need to do it automatically - jax example, PyTorch example - you can google/find your way to visualize it.\n[10 points] Compare analytic and autograd (with any framework) approach for the calculation of the gradient of:\n\nf(A) = \\text{tr}(e^A)\n\n[10 points] We can use automatic differentiation not only to calculate necessary gradients but also for tuning hyperparameters of the algorithm like learning rate in gradient descent (with gradient descent 🤯). Suppose, we have the following function f(x) = \\frac{1}{2}\\Vert x\\Vert^2, select a random point x_0 \\in \\mathbb{B}^{1000} = \\{0 \\leq x_i \\leq 1 \\mid \\forall i\\}. Consider 10 steps of the gradient descent starting from the point x_0: \nx_{k+1} = x_k - \\alpha_k \\nabla f(x_k)\n Your goal in this problem is to write the function, that takes 10 scalar values \\alpha_i and return the result of the gradient descent on function L = f(x_{10}). And optimize this function using gradient descent on \\alpha \\in \\mathbb{R}^{10}. Suppose that each of 10 components of \\alpha is uniformly distributed on [0; 0.1]. \n\\alpha_{k+1} = \\alpha_k - \\beta \\frac{\\partial L}{\\partial \\alpha}\n Choose any constant \\beta and the number of steps you need. Describe the obtained results. How would you understand, that the obtained schedule (\\alpha \\in \\mathbb{R}^{10}) becomes better than it was at the start? How do you check numerically local optimality in this problem?\n[10 points] Compare analytic and autograd (with any framework) approach for the gradient of:\n\nf(X) = - \\log \\det X\n\n\n\n\nConvexity\n\n[5 points] The center of mass of a body is an important concept in physics (mechanics). Obviously, the center of mass of a body does not always lie inside the body. For example, the center of mass of a doughnut is located in its hole. Prove that the center of mass of a system of material points lies in the convex hull of the set of these points.\n[10 points] Show, that \\mathbf{conv}\\{xx^\\top: x \\in \\mathbb{R}^n, \\Vert x\\Vert  = 1\\} = \\{A \\in \\mathbb{S}^n_+: \\text{tr}(A) = 1\\}.\n[5 points] Prove that the set of \\{x \\in \\mathbb{R}^2 \\mid e^{x_1}\\le x_2\\} is convex.\n[5 points] Show that the set of directions of the non-strict local descending of the differentiable function in a point is a convex cone. We assume, that \\nabla f(x_0) \\neq 0 at the target point.\n[10 points] Is the following set convex \nS = \\left\\{ a \\in \\mathbb{R}^k \\mid p(0) = 1, \\vert p(t) \\vert\\leq 1 \\text{ for } \\alpha\\leq t \\leq \\beta\\right\\},\n where \np(t) = a_1 + a_2 t + \\ldots + a_k t^{k-1} \\;?\n\n[10 points] Consider the function f(x) = x^d, where x \\in \\mathbb{R}_{+}. Fill the following table with ✅ or ❎. Explain your answers\n\n\n\n\nd\nConvex\nConcave\nStrictly Convex\n\\mu-strongly convex\n\n\n\n\n-2, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n-1, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0.5\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\\in (1; 2)\n\n\n\n\n\n\n2\n\n\n\n\n\n\n&gt; 2\n\n\n\n\n\n\n\n\n[10 points] Prove that the entropy function, defined as\n\nf(x) = -\\sum_{i=1}^n x_i \\log(x_i),\n\nwith \\text{dom}(f) = \\{x \\in \\R^n_{++} : \\sum_{i=1}^n x_i = 1\\}, is strictly concave.\n[10 points] Show, that the function f: \\mathbb{R}^n_{++} \\to \\mathbb{R} is convex if f(x) = - \\prod\\limits_{i=1}^n x_i^{\\alpha_i} if \\mathbf{1}^T \\alpha = 1, \\alpha \\succeq 0.\n[10 points] Show that the maximum of a convex function f over the polyhedron P = \\text{conv}\\{v_1, \\ldots, v_k\\} is achieved at one of its vertices, i.e.,\n\n\\sup_{x \\in P} f(x) = \\max_{i=1, \\ldots, k} f(v_i).\n\nA stronger statement is: the maximum of a convex function over a closed bounded convex set is achieved at an extreme point, i.e., a point in the set that is not a convex combination of any other points in the set. (you do not have to prove it). Hint: Assume the statement is false, and use Jensen’s inequality.\n[10 points] Show, that the two definitions of \\mu-strongly convex functions are equivalent:\n\nf(x) is \\mu-strongly convex \\iff for any x_1, x_2 \\in S and 0 \\le \\lambda \\le 1 for some \\mu &gt; 0:\n\nf(\\lambda x_1 + (1 - \\lambda)x_2) \\le \\lambda f(x_1) + (1 - \\lambda)f(x_2) - \\frac{\\mu}{2} \\lambda (1 - \\lambda)\\|x_1 - x_2\\|^2\n\nf(x) is \\mu-strongly convex \\iff if there exists \\mu&gt;0 such that the function f(x) - \\dfrac{\\mu}{2}\\Vert x\\Vert^2 is convex.\n\n\n\n\n\nConjugate sets\n\n[5 points] Let \\mathbb{A}_n be the set of all n dimensional antisymmetric matrices (s.t. X^T = - X). Show that \\left( \\mathbb{A}_n\\right)^* = \\mathbb{S}_n.\n[10 points] Find the sets S^{*}, S^{**}, S^{***}, if\n\nS = \\{ x \\in \\mathbb{R}^2 \\mid 2x_1 + x_2 \\ge -2, \\;\\; x_1 - 2 x_2 \\le 4, \\;\\;  x_2 \\ge 0 \\;\\; x_1 + x_2 \\ge -3\\}\n\n[10 points] Find the conjugate set to the ellipsoid:\n\n  S = \\left\\{ x \\in \\mathbb{R}^n \\mid \\sum\\limits_{i = 1}^n a_i^2 x_i^2 \\le \\varepsilon^2 \\right\\}\n\n\n\n\nConjugate functions\n\n[5 points] Find f^*(y), if f(x) = \\vert \\frac12x \\vert\n[10 points] Find f^*(y), if f(x) = \\log \\left( \\sum\\limits_{i=1}^n e^{x_i} \\right)\n[10 points] Prove, that if f(x) = g(Ax), then f^*(y) = g^*(A^{-\\top}y)\n[15 points] Find f^*(Y), if f(X) = - \\ln \\det X, X \\in \\mathbb{S}^n_{++}\n[15 points] The scalar Huber function is defined as\n\nf_{\\text{hub}}(x) =\n\\begin{cases}\n\\frac{1}{2} x^2 & \\text{if } |x| \\leq 1 \\\\\n|x| - \\frac{1}{2} & \\text{if } |x| &gt; 1\n\\end{cases}\n\n\n\n\nScalar case\n\n\nThis convex function arises in various applications, notably in robust estimation. This problem explores the generalizations of the Huber function to \\mathbb{R}^n. A straightforward extension to \\mathbb{R}^n is expressed as f_{\\text{hub}}(x_1) + \\ldots + f_{\\text{hub}}(x_n), yet this formulation is not circularly symmetric, that is, it’s not invariant under the transformation of x by an orthogonal matrix. A circularly symmetric extension to \\mathbb{R}^n is given by\n\nf_{\\text{cshub}}(x) = f_{\\text{hub}}(\\Vert x\\Vert )=\n\\begin{cases}\n\\frac{1}{2} \\Vert x\\Vert_2 ^2 & \\text{if } \\Vert x\\Vert_2 \\leq 1 \\\\\n\\Vert x\\Vert_2 - \\frac{1}{2} & \\text{if } \\Vert x\\Vert_2 &gt; 1\n\\end{cases}\n\nwhere the subscript denotes “circularly symmetric Huber function”. Show, that f_{\\text{cshub}} is convex. Find the conjugate function f^*(y).\n[15 points] Prove that f^{**} is the pointwise maximum of all affine functions that underestimate f, i.e., \nf^{**}(x) = \\max \\{ g(x) : \\text{$g$ is affine, $g \\leq f$}\\}.\n\n[15 points] Derive the conjugate function of f(\\theta)=\\sum_{i=1}^n \\log(1+e^{-y_i\\theta_i}), where y_i \\in \\{-1,1\\}.\n\n\n\nSubgradients\n\n[5 points] Find \\partial f(x), if \nf(x) = \\text{Parametric ReLU}(x) = \\begin{cases}\n     x & \\text{if } x &gt; 0, \\\\\n     ax & \\text{otherwise}.\n\\end{cases}\n\n[10 points] Find \\partial f(x), if f(x) = \\Vert Ax - b\\Vert _1.\n[10 points] Find \\partial f(x), if f(x) = e^{\\Vert x\\Vert}.\n[10 points] Find \\partial f(x), if f(x) = \\frac12 \\Vert Ax - b\\Vert _2^2 + \\lambda \\Vert x\\Vert_1, \\quad \\lambda &gt; 0.\n[5 points] Let S \\subseteq \\mathbb{R}^n be a convex set. We will call a normal cone of the set S at a point x the following set: \nN_S(x) = \\left\\{c \\in \\mathbb{R}^n : \\langle c, y-x\\rangle \\leq 0 \\quad \\forall y \\in S\\right\\}\n\n\nDraw a normal cone for a set at the points A, B, C, D, E, F on the figure below:\n\n\n\nDraw a normal cone for the set S in these points\n\n\nShow, that N_S(x) = \\{0\\} \\quad \\forall x \\in \\mathbf{i }(S).\n\n[15 points] For f(X) = \\|X\\|_{\\text{tr}}, show that subgradients at X=U \\Sigma V^T (this is an SVD of X) satisfy \n\\partial f(X) \\supseteq \\{UV^T + W : \\|W\\|_{\\mathrm{op}} \\leq 1, \\;\nU^T W = 0, \\; WV = 0 \\}.\n Hint: you may use the fact that \\|\\cdot\\|_{\\text{tr}} and \\|\\cdot\\|_{\\mathrm{op}} are dual norms, which implies \\langle A, B \\rangle \\leq \\|A\\|_{\\text{tr}}\\|B\\|_{\\mathrm{op}} for any matrices A,B, where recall \\langle A,B \\rangle = \\text{tr}(A^T B). Bonus (5 pts): prove the other direction.\n\n\n\nOptimality Conditions. KKT\nIn this section, you can consider either the arbitrary norm or the Euclidian norm if nothing else is specified.\n\nToy example [10 points] \n\\begin{split}\n& x^2 + 1 \\to \\min\\limits_{x \\in \\mathbb{R} }\\\\\n\\text{s.t. } & (x-2)(x-4) \\leq 0\n\\end{split}\n\n\nGive the feasible set, the optimal value, and the optimal solution.\nPlot the objective x^2 +1 versus x. On the same plot, show the feasible set, optimal point, and value, and plot the Lagrangian L(x,\\mu) versus x for a few positive values of \\mu. Verify the lower bound property (p^* \\geq \\inf_x L(x, \\mu)for \\mu \\geq 0). Derive and sketch the Lagrange dual function g.\nState the dual problem, and verify that it is a concave maximization problem. Find the dual optimal value and dual optimal solution \\mu^*. Does strong duality hold?\nLet p^*(u) denote the optimal value of the problem\n\n\n\\begin{split}\n& x^2 + 1 \\to \\min\\limits_{x \\in \\mathbb{R} }\\\\\n\\text{s.t. } & (x-2)(x-4) \\leq u\n\\end{split}\n\nas a function of the parameter u. Plot p^*(u). Verify that \\dfrac{dp^*(0)}{du} = -\\mu^*\n[10 points] Give an explicit solution to the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & 1^\\top x = 1, \\\\\n& x \\succeq 0\n\\end{split}\n\nThis problem can be considered the simplest portfolio optimization problem.\n[20 points] Show, that the following problem has a unique solution and find it:\n\n\\begin{split}\n& \\langle C^{-1}, X\\rangle - \\log \\det X \\to \\min\\limits_{x \\in \\mathbb{R}^{n \\times n} }\\\\\n\\text{s.t. } & \\langle Xa, a\\rangle \\leq 1,\n\\end{split}\n\nwhere C \\in \\mathbb{S}^n_{++}, a \\in \\mathbb{R}^n \\neq 0. The answer should not involve inversion of the matrix C.\n[20 points] Derive the KKT conditions for the problem\n\n\\begin{split}\n& \\mathbf{tr \\;}X - \\log\\text{det }X \\to \\min\\limits_{X \\in \\mathbb{S}^n_{++} }\\\\\n\\text{s.t. } & Xs = y,\n\\end{split}\n\nwhere y \\in \\mathbb{R}^n and s \\in \\mathbb{R}^n are given with y^\\top s = 1. Verify that the optimal solution is given by\n\nX^* = I + yy^\\top - \\dfrac{1}{s^\\top s}ss^\\top\n\n\n\n\nDuality\n\n[10 points] Derive the dual problem for the Ridge regression problem with A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m, \\lambda &gt; 0:\n\n\\begin{split}\n\\dfrac{1}{2}\\|y-b\\|^2 + \\dfrac{\\lambda}{2}\\|x\\|^2 &\\to \\min\\limits_{x \\in \\mathbb{R}^n, y \\in \\mathbb{R}^m }\\\\\n\\text{s.t. } & y = Ax\n\\end{split}\n\n[20 points] Derive the dual problem for the support vector machine problem with A \\in \\mathbb{R}^{m \\times n}, \\mathbf{1} \\in \\mathbb{R}^m \\in \\mathbb{R}^m, \\lambda &gt; 0:\n\n\\begin{split}\n\\langle \\mathbf{1}, t\\rangle + \\dfrac{\\lambda}{2}\\|x\\|^2 &\\to \\min\\limits_{x \\in \\mathbb{R}^n, t \\in \\mathbb{R}^m }\\\\\n\\text{s.t. } & Ax \\succeq \\mathbf{1} - t \\\\\n& t \\succeq 0\n\\end{split}\n\n[20 points] Analytic centering. Derive a dual problem for\n\n-\\sum_{i=1}^m \\log (b_i - a_i^\\top x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\n\nwith domain \\{x \\mid a^\\top_i x &lt; b_i , i = [1,m]\\}.\nFirst introduce new variables y_i and equality constraints y_i = b_i − a^\\top_i x. (The solution to this problem is called the analytic center of the linear inequalities a^\\top_i x \\leq b_i ,i = [1,m]. Analytic centers have geometric applications, and play an important role in barrier methods.)\n\n\n\nLinear Programming\n\n[20 points] 📱🎧💻 Covers manufacturing. Lyzard Corp is producing covers for the following products:\n\n📱 phones\n🎧 headphones\n💻 laptops\n\nThe company’s production facilities are such that if we devote the entire production to headphone covers, we can produce 5000 of them in one day. If we devote the entire production to phone covers or laptop covers, we can produce 4000 or 2000 of them in one day.\nThe production schedule is one week (6 working days), and the week’s production must be stored before distribution. Storing 1000 headphone covers (packaging included) takes up 30 cubic feet of space. Storing 1000 phone covers (packaging included) takes up 50 cubic feet of space, and storing 1000 laptop covers (packaging included) takes up 220 cubic feet of space. The total storage space available is 1500 cubic feet.\nDue to commercial agreements with Lyzard Corp has to deliver at least 4500 headphone covers and 4000 laptop covers per week to strengthen the product’s diffusion.\nThe marketing department estimates that the weekly demand for headphones covers, phone, and laptop covers does not exceed 10000 14000, and 7000 units, therefore the company does not want to produce more than these amounts for headphones, phone, and laptop covers.\nFinally, the net profit per headphone cover, phone cover, and laptop cover are $5, $7, and $12, respectively.\nThe aim is to determine a weekly production schedule that maximizes the total net profit.\n\nWrite a Linear Programming formulation for the problem. Use the following variables:\n\ny_1 = number of headphones covers produced over the week,\n\ny_2 = number of phone covers produced over the week,\n\ny_3 = number of laptop covers produced over the week.\n\nFind the solution to the problem using PyOMO\n!pip install pyomo\n! sudo apt-get install glpk-utils --quiet  # GLPK\n! sudo apt-get install coinor-cbc --quiet  # CoinOR\nPerform the sensitivity analysis. Which constraint could be relaxed to increase the profit the most? Prove it numerically.\n\n[10 points] Prove the optimality of the solution\n\nx = \\left(\\frac{7}{3} , 0, \\frac{1}{3}\\right)^T\n\nto the following linear programming problem:\n\n\\begin{split}\n& 9x_1 + 3x_2 + 7x_3 \\to \\max\\limits_{x \\in \\mathbb{R}^3 }\\\\\n\\text{s.t. } & 2x_1 + x_2 + 3x_3 \\leq 6 \\\\\n& 5x_1 + 4x_2 + x_3 \\leq 12 \\\\\n& 3x_3 \\leq 1,\\\\\n& x_1, x_2, x_3 \\geq 0\n\\end{split}\n\nbut you cannot use any numerical algorithm here.\n[5 points] Transform the following linear program into an equivalent linear program in the standard form \\left(c^\\top x \\to \\min\\limits_{x\\in \\mathbb{R}^n} : Ax = b,x ≥ 0\\right):\n\n\\begin{split}\n& x_1−x_2 \\to \\min\\limits_{x \\in \\mathbb{R}^2 }\\\\\n\\text{s.t. } & 2x_1 + x_2 \\geq 3 \\\\\n& 3x_1 − x_2 \\leq 7 \\\\\n& x_1 \\geq 0\n\\end{split}\n\n[20 points] Economic interpretation of the dual problem: Suppose a small shop makes wooden toys, where each toy train requires one piece of wood and 2 tins of paint, while each toy boat requires one piece of wood and 1 tin of paint. The profit on each toy train is \\$30, and the profit on each toy boat is \\$20. Given an inventory of 80 pieces of wood and 100 tins of paint, how many of each toy should be made to maximize the profit?\n\nWrite out the optimization problem in standard form, writing all constraints as inequalities.\nSketch the feasible set and determine p^* and x^*\nFind the dual problem, then determine d^* and \\lambda^*. Note that we can interpret the Lagrange multipliers \\lambda_k associated with the constraints on wood and paint as the prices for each piece of wood and tin of paint, so that −d^* is how much money would be obtained from selling the inventory for those prices. Strong duality says a buyer should not pay more for the inventory than what the toy store would make by producing and selling toys from it, and that the toy store should not sell the inventory for less than that.\nThe other interpretation of the Lagrange multipliers is as sensitivities to changes in the constraints. Suppose the toymaker found some more pieces of wood; the \\lambda_k associated with the wood constraint will equal the partial derivative of −p^* with respect to how much more wood became available. Suppose the inventory increases by one piece of wood. Use \\lambda^* to estimate how much the profit would increase, without solving the updated optimization problem. How is this consistent with the price interpretation given above for the Lagrange multipliers? source\n\n\n\n\nSequence convergence\n\n[15 points] Determine the convergence or divergence of a given sequences\n\nr_{k} = \\frac{1}{\\sqrt{k}}.\nr_{k} = 0.707^k.\nr_{k} = 0.707^{2^k}.\n\n[10 points] Determine the convergence or divergence of a given sequence r_k =\\begin{cases} \\frac{1}{k}, & \\text{if } k\\text{ is even} \\\\ e^{-k}, & \\text{if } k\\text{ is odd} \\end{cases}.\n[10 points] Determine the following sequence \\{r_k\\} by convergence rate (linear, sublinear, superlinear). In the case of superlinear convergence, additionally, find out whether there is quadratic convergence.\n\nr_k = \\dfrac{1}{k!}\n\n[10 points] Determine the following sequence \\{r_k\\} by convergence rate (linear, sublinear, superlinear). In the case of superlinear convergence, additionally find out whether there is quadratic convergence.\n\nr_k = \\dfrac{1}{k^k}\n\n[15 points] Let \\left\\{ r_k \\right\\}_{k=m}^\\infty be a sequence of non-negative numbers and let s &gt; 0 be some integer. Prove that sequence \\left\\{ r_k \\right\\}_{k=m+s}^\\infty is linearly convergent with constant q if and only if a the sequence \\left\\{ r_k \\right\\}_{k=m}^\\infty converged linearly with constant q.\n\n\n\nLine search\n\n[10 points] Consider a quadratic function f: \\mathbb{R}^n \\rightarrow \\mathbb{R}, and let us start from a point x_k \\in \\mathbb{R}^n moving in the direction of the antigradient -\\nabla f(x_k). Show that the minimum of f along this direction as a function of the step size \\alpha, for a decreasing function at x_k, satisfies Armijo’s condition for any c_1 in the range 0 \\leq c_1 \\leq \\frac{1}{2}. Specifically, demonstrate that the following inequality holds at the optimal \\alpha^*:\n\n\\varphi(\\alpha) = f(x_{k+1}) = f(x_k - \\alpha \\nabla f(x_k)) \\leq f(x_k) - c_1 \\alpha \\|\\nabla f(x_k)\\|_2^2\n\nImplementing and Testing Line Search Conditions in Gradient Descent [30 points]\n\nx_{k+1} = x_k - \\alpha \\nabla f(x_k)\n\nIn this assignment, you will modify an existing Python code for gradient descent to include various line search conditions. You will test these modifications on two functions: a quadratic function and the Rosenbrock function. The main objectives are to understand how different line search strategies influence the convergence of the gradient descent algorithm and to compare their efficiencies based on the number of function evaluations.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize_scalar\nnp.random.seed(214)\n\n# Define the quadratic function and its gradient\ndef quadratic_function(x, A, b):\n    return 0.5 * np.dot(x.T, np.dot(A, x)) - np.dot(b.T, x)\n\ndef grad_quadratic(x, A, b):\n    return np.dot(A, x) - b\n\n# Generate a 2D quadratic problem with a specified condition number\ndef generate_quadratic_problem(cond_number):\n    # Random symmetric matrix\n    M = np.random.randn(2, 2)\n    M = np.dot(M, M.T)\n\n    # Ensure the matrix has the desired condition number\n    U, s, V = np.linalg.svd(M)\n    s = np.linspace(cond_number, 1, len(s))  # Spread the singular values\n    A = np.dot(U, np.dot(np.diag(s), V))\n\n    # Random b\n    b = np.random.randn(2)\n\n    return A, b\n\n# Gradient descent function\ndef gradient_descent(start_point, A, b, stepsize_func, max_iter=100):\n    x = start_point.copy()\n    trajectory = [x.copy()]\n\n    for i in range(max_iter):\n        grad = grad_quadratic(x, A, b)\n        step_size = stepsize_func(x, grad)\n        x -= step_size * grad\n        trajectory.append(x.copy())\n\n    return np.array(trajectory)\n\n# Backtracking line search strategy using scipy\ndef backtracking_line_search(x, grad, A, b, alpha=0.3, beta=0.8):\n    def objective(t):\n        return quadratic_function(x - t * grad, A, b)\n    res = minimize_scalar(objective, method='golden')\n    return res.x\n\n# Generate ill-posed problem\ncond_number = 30\nA, b = generate_quadratic_problem(cond_number)\n\n# Starting point\nstart_point = np.array([1.0, 1.8])\n\n# Perform gradient descent with both strategies\ntrajectory_fixed = gradient_descent(start_point, A, b, lambda x, g: 5e-2)\ntrajectory_backtracking = gradient_descent(start_point, A, b, lambda x, g: backtracking_line_search(x, g, A, b))\n\n# Plot the trajectories on a contour plot\nx1, x2 = np.meshgrid(np.linspace(-2, 2, 400), np.linspace(-2, 2, 400))\nZ = np.array([quadratic_function(np.array([x, y]), A, b) for x, y in zip(x1.flatten(), x2.flatten())]).reshape(x1.shape)\n\nplt.figure(figsize=(10, 8))\nplt.contour(x1, x2, Z, levels=50, cmap='viridis')\nplt.plot(trajectory_fixed[:, 0], trajectory_fixed[:, 1], 'o-', label='Fixed Step Size')\nplt.plot(trajectory_backtracking[:, 0], trajectory_backtracking[:, 1], 'o-', label='Backtracking Line Search')\n\n# Add markers for start and optimal points\nplt.plot(start_point[0], start_point[1], 'ro', label='Start Point')\noptimal_point = np.linalg.solve(A, b)\nplt.plot(optimal_point[0], optimal_point[1], 'y*', markersize=15, label='Optimal Point')\n\nplt.legend()\nplt.title('Gradient Descent Trajectories on Quadratic Function')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.savefig(\"linesearch.svg\")\nplt.show()\n\n\n\nThe code above plots this\n\n\nStart by reviewing the provided Python code. This code implements gradient descent with a fixed step size and a backtracking line search on a quadratic function. Familiarize yourself with how the gradient descent function and the step size strategies are implemented.\n\nModify the gradient descent function to include the following line search conditions:\n\nSufficient Decrease Condition\nCurvature Condition\nGoldstein Condition\nWolfe Condition\nDichotomy\n\nTest your modified gradient descent algorithm with the implemented line search conditions on the provided quadratic function. Plot the trajectories over iterations for each condition. Choose and specify hyperparameters for inexact line search condition. Choose and specify the termination criterion. Start from the point x_0 = (-1, 2)^T.\nCompare these 7 methods from the budget perspective. Plot the graph of function value from the number of function evaluations for each method on the same graph.\nPlot trajectory for another function with the same set of methods\n\nf(x_1, x_2) =  10(x_2 − x_1^2)^2 + (x_1 − 1)^2\n\nwith x_0 = (-1, 2)^T. You might need to adjust hyperparameters.\nPlot the same function value from the number of function calls for this experiment."
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "",
    "section": "",
    "text": "Занятие 1\n    \n        📄 Презентация • 📝 Заметки • ▶️ Youtube • 💿 Скачать\n    \n    Вспоминаем линейную алгебру. Некоторые матричные разложения. Спектр матрицы. SVD. Skeleton. Градиент. Гессиан. Матрично-векторное дифференцирование\n\n    Занятие 2\n    \n        📄 Презентация • 📝 Заметки • ▶️ Youtube • 💿 Скачать\n    \n    Повторяем матричные производные. Проклятие размерности методов нулевого порядка\n\n    Занятие 3\n    \n        📄 Презентация • 📝 Заметки • ▶️ Youtube • 💿 Скачать\n    \n    Автоматическое дифференцирование. Forward\\Reverse Mode. Вычислительный граф.\n\n    Занятие 4\n    \n        📄 Презентация • 📝 Заметки • ▶️ Youtube • 💿 Скачать\n    \n    Выпуклость. Выпуклые, афинные множества. Сумма Минковского. Выпуклые функции. Неравенство Йенсена\n\n    Занятие 5\n    \n        📄 Презентация • 📝 Заметки • ▶️ Youtube • 💿 Скачать\n    \n    Сильно выпуклые функции. Условие Поляка - Лоясиевича. Линейная регрессия. Регуляризация.\n\n    Занятие 6\n    \n        📄 Презентация • 📝 Заметки • ▶️ Youtube • 💿 Скачать\n    \n    Сопряженные множества. Сопряженные конусы. Многогранники. Сопряженные функции. Преобразование Лежандра. Субградиент. Субдифференциал. Теоремы Моро-Рокафеллара, Дубовицкого-Милютина\n\n    Занятие 7\n    \n        📄 Презентация • 📝 Заметки • ▶️ Youtube • 💿 Скачать\n    \n    Субдифференциал. Условия оптимальности в субдифференциальной форме. Функция Лагранжа. Множители Лагранжа. Задачи с ограничениями типа равенств.\n\n    Занятие 8\n    \n        📄 Презентация • 📝 Заметки • ▶️ Youtube • 💿 Скачать\n    \n    Условия оптимальности. Функция Лагранжа. Множители Лагранжа. Теорема Каруша - Куна - Таккера\n\n    Занятие 9\n    \n        📄 Презентация • 📝 Заметки • ▶️ Youtube • 💿 Скачать\n    \n    Двойственность. Введение в двойственность. Двойственная задача. Two-way partitioning problem. Решение прямой задачи с помощью двойственной\n\n    Занятие 10\n    \n        📄 Презентация • 📝 Заметки • ▶️ Youtube • 💿 Скачать\n    \n    Линейное программирование. Транспортная задача и другие формулировки прикладных задач как ЛП. Симплекс метод для решения ЛП\n\n    Занятие 11\n    \n        📄 Презентация • 📝 Заметки • ▶️ Youtube • 💿 Скачать\n    \n    Двойственность в линейном программировании. Анализ чувствительности.\n\n    Занятие 12\n    \n        📄 Презентация • 📝 Заметки\n    \n    Классификация и обозначения в задачах оптимизации. Скорость сходимости. Линейный поиск. Неточная одномерная оптимизация. Правила Армихо  - Гольдштейна. Условие Вульфа\n\n    Занятие 13\n    \n        📄 Презентация • 📝 Заметки\n    \n    Методы нулевого порядка. Безградиентные методы. Оптимизация гиперпараметров модели машинного обучения. Генетический алгоритм. Эволюционные алгоритмы\n\n    Занятие 14\n    \n        📄 Презентация • 📝 Заметки\n    \n    Градиентный спуск. Теоремы сходимости в гладком случае (выпуклые, сильно выпуклые, PL). Верхние и нижние оценки сходимости\n\n    Занятие 15\n    \n        📄 Презентация • 📝 Заметки\n    \n    Ускоренные градиентные методы. Метод Поляка, Нестерова\n\n    Занятие 16\n    \n        📄 Презентация • 📝 Заметки\n    \n    Метод сопряженных направлений. Ортогонализация Грамма - Шмидта. Понятие $A$-ортогональных векторов. Метод сопряженных градиентов\n\n    Занятие 17\n    \n        📄 Презентация • 📝 Заметки\n    \n    Субградиентный метод. Теоремы сходимости в негладком случае (выпуклый случай). Особенности работы градиентного метода в практических негладких задачах. Задача наименьших квадратов с $l_1$ регуляризацией\n\n    Занятие 18\n    \n        📄 Презентация • 📝 Заметки\n    \n    Градиентные методы в условных задачах оптимизации - метод проекции градиента. Метод Франк - Вульфа. Метод зеркального спуска\n\n    Занятие 19\n    \n        📄 Презентация • 📝 Заметки\n    \n    Концепция методов адаптивной метрики. Метод Ньютона. Квазиньютоновские методы\n\n    Занятие 20\n    \n        📄 Презентация • 📝 Заметки\n    \n    Проксимальный градиентный метод\n\n    Занятие 21\n    \n        📄 Презентация • 📝 Заметки\n    \n    Введение в стохастические градиентные методы. Батч, эпоха. Сходимость SGD\n\n    Занятие 22\n    \n        📄 Презентация • 📝 Заметки\n    \n    Методы редукции дисперсии: SAG, SVRG, SAGA. Адаптивные стохастические градиентные методы\n\n    Занятие 23\n    \n        📄 Презентация • 📝 Заметки\n    \n    Обучение нейронных сетей с точки зрения методов оптимизации. Обобщающая способность моделей машинного обучения. Double Descent. Grokking. Mode connectivity\n\n    Занятие 24\n    \n        📄 Презентация • 📝 Заметки\n    \n    Удивительные сюжеты из мира обучения больших нейросетей с точки зрения методов оптимизации. Проекция функции потерь нейронной сети на прямую, плоскость. Инициализация. Grokking. Double Descent. Large batch training. Чекпоинтинг активаций.\n\n    Занятие 25\n    \n        📄 Презентация • 📝 Заметки\n    \n    Вопросы обучения больших моделей. Lars, Lamb. Learning rate schedulers. Warm-up. MultiGPU training. LoRa адаптеры. Квантизация.\n\n    Занятие 26\n    \n        📄 Презентация • 📝 Заметки\n    \n    Методы оптимизации в непрерывном времени. Gradient Flow. Accelerated Gradient Flow. Stochastic gradient flow.\n\n    Занятие 27\n    \n        📄 Презентация • 📝 Заметки\n    \n    Введение в двойственные методы оптимизации. Метод двойственного градиентного подъёма. Метод модифицированной функции Лагранжа. ADMM.\n\n\nNo matching items"
  }
]