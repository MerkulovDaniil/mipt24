---
format: 
    pdf:
        pdf-engine: xelatex
        include-in-header: ../files/docheader.tex  # Custom LaTeX commands and preamble
---

# Определения и формулировки

1.  Положительно определённая матрица.

    :::{.callout-tip appearance="simple"}
    Матрица $A\in \mathbb{S}$ называется положительно (отрицательно) определённой, если для $\forall x\neq 0:\; x^TAx > (<) 0$. Обозначение: $A\prec 0\; (A\succ 0)$.

    Аналогично определяется полуопределённость, только там неравенства нестрогие.
    :::
1.  Евклидова норма вектора.

    :::{.callout-tip appearance="simple"}
    $$
    ||x||_2 = \sqrt{\sum_{i=1}^{n}|x_i|^2}
    $$

    Данная норма соответствует расстоянию в реальном мире. Иначе называется 2-норма (см. p-норма вектора)
    :::
1.  Неравенство треугольника для нормы.

    :::{.callout-tip appearance="simple"}
    Норма должна удовлетворять следующим свойствам:

    1. $||\alpha x|| = |\alpha| ||x||$, $\alpha \in \mathbb{R}$

    2. $||x||=0 \; \Rightarrow \; x=0$

    3. $||x+y||\leq ||x||+||y||$ -- неравенство треугольника
    :::
1.  $p$-норма вектора.

    :::{.callout-tip appearance="simple"}
    $$
    ||x||_p = \left(\sum_{i=0}^{n}|x_i|^{p}\right)^{\frac{1}{p}}
    $$

    Важные частные случаи:

    - Норма Чебышева:
    $||x||_{\infty} = \max\limits_{i} |x_i|$

    - Манхэттенское расстояние или $L1$ норма:
    $||x||_{1} = \sum_{i=0}^{n}|x_i|$
    :::
1.  Как выглядит единичный шар в $p$ - норме на плоскости для $p=1,2,\infty$?

    :::{.callout-tip appearance="simple"}
    ![Шары в разных нормах](p_balls.pdf)
    :::
1.  Норма Фробениуса для матрицы.

    :::{.callout-tip appearance="simple"}
    $$
    ||A||_{F} = \left(\sum\limits_{i = 1}^{m}\sum\limits_{j = 1}^{n} |a_{ij}|^2\right)^{\frac{1}{2}}
    $$
    :::
1.  Спектральная норма матрицы.

    :::{.callout-tip appearance="simple"}
    $$
    \|A\|_2 = \sup_{x \neq 0} \frac{\|Ax\|_2}{\|x\|_2} = \sigma_1 (A) = \sqrt{\lambda_{max}(A^\top A)}
    $$

    Где $\sigma_1 (A)$ -- старшее сингулярное значение $A$, $\lambda_{max}(A^\top A)$ -- наибольшее собственное значение $A^\top A$.
    :::
1.  Скалярное произведение двух векторов.

    :::{.callout-tip appearance="simple"}
    Пусть $x, y \in \mathbb{R}^{n}$, тогда их скалярное произведение это

    $$
    \langle x, y \rangle = x^Ty = \sum\limits_{i=1}^{n} x_iy_i = y^Tx = \langle y, x \rangle
    $$
    :::
1.  Скалярное произведение двух матриц, согласованное с нормой Фробениуса.

    :::{.callout-tip appearance="simple"}
    Пусть $X, Y \in \mathbb{R}^{m \times n}$, тогда их скалярное произведение это

    $$
    \langle X, Y \rangle = tr(X^TY) = \sum\limits_{i=1}^{m}\sum\limits_{j=1}^{n} X_{ij}Y_{ij}= tr(Y^TX) = \langle Y, X \rangle
    $$

    Связь с нормой Фробениуса: $\langle X, X \rangle = ||X||^2_F$
    :::
1.  Собственные значения матрицы. Спектр матрицы.

    :::{.callout-tip appearance="simple"}
    Скаляр $\lambda$ является собственным значением для матрицы $A$, если существует вектор $q$, такой что $Aq = \lambda q$. В таком случае $q$ называют собственным вектором.

    Спектр матрицы -- совокупность её собственных значений.
    :::
1.  Связь спектра матрицы и её определенности.

    :::{.callout-tip appearance="simple"}
    Матрица положительно (неотрицательно) определена $\iff$ её спектр (все её собственные значения) положителен (неотрицателен).
    :::
1.  Спектральное разложение матрицы.

    :::{.callout-tip appearance="simple"}
    Спектральное разложение матрицы, или разложение матрицы на основе собственных векторов, — это представление квадратной матрицы $A$ в виде произведения трёх матриц $A=V\Lambda V^{-1}$, где $V$ — матрица, столбцы которой являются собственными векторами матрицы $A$, $\Lambda$ — диагональная матрица с соответствующими собственными значениями на главной диагонали. В таком виде могут быть представлены только матрицы, обладающие полным набором собственных векторов.

    Тогда $A^n = V\Lambda^n V^{-1}$.
    :::
1.  Сингулярное разложение матрицы.

    :::{.callout-tip appearance="simple"}
    $A \in \mathbb{R}^{m \times n}$ , $rank~A = r$.

    $$
    A = U\Sigma V^T
    $$

    $U \in \mathbb{R}^{m \times r}$, $U^T U = I$, $V \in \mathbb{R}^{n \times r}$, $V^T V = I$, $\Sigma$ is a diagonal matrix with

    $$
    \Sigma = \text{diag}(\sigma_1, \ldots, \sigma_r)
    $$

    such that

    $$
    \sigma_1 \ge \sigma_2 \ge \ldots \ge \sigma_r > 0
    $$

    Столбцы $U$, $V$ - левые и правые собственные векторы $A$, $\sigma_i$ - сингулярные значения.

    $$
    A = \sum_{i=1}^r \sigma_i u_i v_i^T
    $$

    :::
1.  Связь определителя и собственных чисел для квадратной матрицы.

    :::{.callout-tip appearance="simple"}
    Если у матрицы $A$ собственные значения $\lambda_1, \lambda_2, \ldots, \lambda_n$, то её определитель равен:

    $$
    \det(A) = \lambda_1 \cdot \lambda_2 \cdot \ldots \cdot \lambda_n
    $$
    :::
1.  Связь следа и собственных чисел для квадратной матрицы.

    :::{.callout-tip appearance="simple"}
    Если у матрицы $A$ собственные значения $\lambda_1, \lambda_2, \ldots, \lambda_n$, то её след равен:

    $$
    \text{tr}(A) = \lambda_1 + \lambda_2 + \ldots + \lambda_n
    $$
    :::
1.  Градиент функции $f(x): \mathbb{R}^n \to \mathbb{R}$.

    :::{.callout-tip appearance="simple"}
    $\nabla f(x)$, вектор частных производных функции $f$.
    :::
1.  Гессиан функции $f(x): \mathbb{R}^n \to \mathbb{R}$.

    :::{.callout-tip appearance="simple"}
    $$
    f''(x) = \nabla^2 f(x) = \dfrac{\partial^2 f}{\partial x_i \partial x_j} = \begin{pmatrix}
        \frac{\partial^2 f}{\partial x_1 \partial x_1} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \dots  & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
        \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2 \partial x_2} & \dots  & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \dots  & \frac{\partial^2 f}{\partial x_n \partial x_n}
    \end{pmatrix}
    $$
    :::
1.  Якобиан функции $f(x): \mathbb{R}^n \to \mathbb{R}^m$.

    :::{.callout-tip appearance="simple"}
    $$
    J_f = f'(x) = \dfrac{df}{dx^T} = \begin{pmatrix}
        \frac{\partial f_1}{\partial x_1} & \frac{\partial f_2}{\partial x_1} & \dots  & \frac{\partial f_m}{\partial x_1} \\
        \frac{\partial f_1}{\partial x_2} & \frac{\partial f_2}{\partial x_2} & \dots  & \frac{\partial f_m}{\partial x_2} \\
        \vdots & \vdots & \ddots & \vdots \\
        \frac{\partial f_1}{\partial x_n} & \frac{\partial f_2}{\partial x_n} & \dots  & \frac{\partial f_m}{\partial x_n}
    \end{pmatrix}
    $$
    :::
1.  Формула для аппроксимации Тейлора первого порядка $f^I_{x_0}(x)$ функции $f(x): \mathbb{R}^n \to \mathbb{R}$ в точке $x_0$.

    :::{.callout-tip appearance="simple"}
    Для дифференцируемой f: 
    $$f_{x_0}^I(x) = f(x_0) + \nabla f(x_0)^T (x - x_0)$$
    :::
1.  Формула для аппроксимации Тейлора второго порядка $f^{II}_{x_0}(x)$ функции $f(x): \mathbb{R}^n \to \mathbb{R}$ в точке $x_0$.

    :::{.callout-tip appearance="simple"}
    Для дважды дифференцируемой f: 
    $$
    f_{x_0}^{II}(x) = f(x_0) + \nabla f(x_0)^T (x - x_0) + \frac{1}{2} (x - x_0)^T \nabla^2 f(x_0) (x - x_0)
    $$
    :::
1.  Определение дифференцируемости функции в точке через производную как линейный оператор.

    :::{.callout-tip appearance="simple"}
    Пусть $x \in S$ - внутренняя точка множества $S$, и пусть $D : U \rightarrow V$ - линейный оператор. Функция $f$ называется дифференцируемой в точке $x$ с производной $D$ если для всех достаточно малых $h \in U$ верно следующее: 
    $$ 
    f(x + h) = f(x) + D[h] + o(\|h\|)
    $$
    Если для любого линейного оператора $D : U \rightarrow V$ функция $f$ не является дифференцируемой в точке $x$ с производной $D$, тогда мы говорим, что $f$ не дифференцируема в точке $x$.
    :::
1.  Связь дифференциала функции $df$ и градиента $\nabla f$ для функции $f(x): \mathbb{R}^n \to \mathbb{R}$.

    :::{.callout-tip appearance="simple"}
    $$
    df(x) = \langle \nabla f(x), dx\rangle
    $$
    :::
1.  Связь второго дифференциала функции $d^2f$ и гессиана $\nabla^2 f$ для функции $f(x): \mathbb{R}^n \to \mathbb{R}$.

    :::{.callout-tip appearance="simple"}
    $$ d(df) = d^2f(x) = \langle \nabla^2 f(x) dx_1, dx\rangle = \langle H_f(x) dx_1, dx\rangle
    $$
    :::
1.  Формула для приближенного вычисления производной функции $f(x): \mathbb{R}^n \to \mathbb{R}$ по $k$-ой координате с помощью метода конечных разностей.

    :::{.callout-tip appearance="simple"}
    $$
    \dfrac{\partial f}{\partial x_k} (x) \approx \dfrac{f(x+\varepsilon e_k) - f(x)}{\varepsilon}, \quad e_k = (0, \ldots, \underset{{\tiny k}}{1}, \ldots, 0)
    $$

    Время работы: $2dT$, где вызов $f(x)$ занимает $T$,$x \in \mathbb{R}^d$
    :::
1.  Пусть $f = f(x_1(t), \ldots, x_n(t))$. Формула для вычисления $\frac{\partial f}{\partial t}$ через $\frac{\partial x_i}{\partial t}$ (Forward chain rule).

    :::{.callout-tip appearance="simple"}
    $$
    \dfrac{\partial f}{\partial t} = \sum_{i = 1}^{n}\dfrac{\partial f}{\partial x_i}\dfrac{\partial x_i}{\partial t}
    $$
    :::
1.  Пусть $L$ - функция, возвращающая скаляр, а $v_k$ - функция, возвращающая вектор $x \in \mathbb{R}^t$. Формула для вычисления $\frac{\partial L}{\partial v_k}$ через $\frac{\partial L}{\partial x_i}$ (Backward chain rule).

    :::{.callout-tip appearance="simple"}
    $$
    \dfrac{\partial L}{\partial v_k} = \sum_{i = 1}^{t} \dfrac{\partial L}{\partial x_i} \dfrac{\partial x_i}{\partial v_k}
    $$
    :::
1.  Афинное множество. Афинная комбинация. Афинная оболочка.

    :::{.callout-tip appearance="simple"}
    Множество $A$ называется аффинным если для любых $x_1$, $x_2$ из $A$ прямая, проходящая через $x_1$, $x_2$, тоже лежит в $A$. То есть:
    $$
    \forall \theta \in \R, \forall x_1, x_2 \in A : \theta x_1 + \left(1 - \theta \right)x_2 \in A
    $$
    Пример аффинного множества: $\R^n$

    Пусть $x_1, x_2, \ldots, x_k \in S$. Тогда точка $\theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_k x_k$ называется аффинной комбинацией, если
    $$
    \forall i \in \{1, \ldots, k\} \colon \theta_i \in \R, \quad \sum_{i=1}^k \theta_i = 1
    $$

    Аффинная оболочка -- множество всех возможных аффинных комбинаций элементов множества.
    $$
    \text{aff}(S) = \left\{ \sum_{i=1}^{k}\theta_i x_i \bigg| k > 0, x_i \in S, \theta_i \in \R, \sum_{\i=1}^{k} \theta_i = 1 \right\}
    $$

    :::
1.  Выпуклое множество. Выпуклая комбинация. Выпуклая оболочка.

    :::{.callout-tip appearance="simple"}
    Множество $S$ называется выпуклым если для любых $x_1$, $x_2$ из $S$ отрезок между $x_1$, $x_2$ тоже лежит в $S$. То есть:
    $$
    \forall \theta \in [0, 1], \, \forall x_1, x_2 \in S : \; \theta x_1 + \left(1 - \theta \right)x_2 \in S
    $$

    Пусть $x_1, x_2, \ldots, x_k \in S$. Тогда точка $\theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_k x_k$ называется выпуклой комбинацией, если
    $$
    \forall i \in \{1, \ldots, k\} \colon \; \theta_i \geq 0, \quad \sum_{i=1}^k \theta_i = 1
    $$

    Выпуклая оболочка -- множество всех возможных выпуклых комбинаций элементов множества.
    $$
    \text{conv}(S) = \left\{ \sum_{i=1}^{k}\theta_i x_i \, \bigg| \, k > 0, \, x_i \in S, \, \theta_i \geq 0, \, \sum_{\i=1}^{k} \theta_i = 1 \right\}
    $$
    :::
1.  Конус. Выпуклый конус. Коническая комбинация. Коническая оболочка.

    :::{.callout-tip appearance="simple"}
    Множество $S$ называется конусом если для любого $x$ из $S$ луч, проходящий из 0 через $x$, тоже лежит в $S$. То есть:
    $$
    \forall \theta \geq 0, \, \forall x \in S : \; \theta x \in S
    $$

    Множество $S$ называется выпуклым конусом если для любых $x_1, x_2 \in S$ их коническая комбинация тоже лежит в $S$. То есть:

    $$
    \forall x_1, x_2 \in S, \, \theta_1, \theta_2 \geq 0 : \theta_1 x_1 + \theta_2 x_2 \in S
    $$

    Пусть $x_1, x_2, \ldots, x_k \in S$. Тогда точка $\theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_k x_k$ называется конической комбинацией, если
    $$
    \forall i \in \{1, \ldots, k\} \colon \; \theta_i \geq 0
    $$

    Коническая оболочка - множество всех возможных конических комбинаций элементов множества.
    $$
    \text{cone}(S) = \left\{ \sum_{i=1}^{k}\theta_i x_i \, \bigg| \, k > 0, \, x_i \in S, \, \theta_i \geq 0 \right\}
    $$

    :::
1.  Внутренность множества. 

    :::{.callout-tip appearance="simple"}
    Внутренность множества - совокупность всех точек множества, содержащих вместе с собой в множестве некоторую окрестность вокруг себя.
    :::
1.  Относительная внутренность множества.

    :::{.callout-tip appearance="simple"}
    Относительная внутренность множества - внутренность множества в его аффинной оболочке. Может быть полезной при работе с множествами меньшей размерности чем пространство, в котором они находятся.
    $$
    \text{relint}(S) = \left\{ x \in S \, | \, \exists \varepsilon > 0, \; N_{\varepsilon}(x) \cap \text{aff}(S) \subseteq S \right\}
    $$
    $N_{\varepsilon}(x)$ -- шар радиуса $\varepsilon$ с центром в $x$, $\text{aff}(S)$ -- аффинная оболочка $S$

    Пример: отрезок на плоскости имеет пустую внутренность, но его относительная внутренность -- тот же отрезок без концов.

    ![](relative_interior.pdf)
    :::
1.  Сумма Минковского.

    :::{.callout-tip appearance="simple"}
    Сумма Минковского -- евклидово пространство, формирующееся сложением каждого вектора из $S_1$ с каждым вектором из $S_2$:
    $$
    S_1 + S_2 = \left\{ s_1 + s_2 \, | \, s_1 \in S_1, \space s_2 \in S_2\right\}
    $$

    ![](minkowski.pdf)

    :::
1.  Любые 2 операции с множествами, сохраняющие выпуклость.

    :::{.callout-tip appearance="simple"}
    1. Линейная комбинация:
    $$
    S = \left\{s \, | \, s = c_1 x + c_2 y, \; x \in S_x, \; y \in S_y, \; c_1, c_2 \in \R \right\}
    $$

    2. Пересечение любого числа выпуклых множеств

    3. Образ множества в аффинном преобразовании:
    $$
    S \subseteq \R^n \; \text{convex} \rightarrow f(S) = \left\{ f(x) | x \in S \right\} \; \text{convex} \quad \left( f(x) = Ax + b \right)
    $$
    :::
1.  Выпуклая функция.

    :::{.callout-tip appearance="simple"}
    Функция $f(x)$, определённая на выпуклом множестве $S \subseteq \R^n$ называется выпуклой на $S$ если:
    $$
    \forall x_1, x_2 \in S, \quad \forall \lambda \in [0, 1]
    $$
    $$
    f(\lambda x_1 + (1 - \lambda) x_2) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)
    $$
    ![](convex_function.pdf){fig-align="center"}
    :::
1.  Строго выпуклая функция.

    :::{.callout-tip appearance="simple"}
    Функция $f(x)$, определённая на выпуклом множестве $S \subseteq \R^n$ называется строго выпуклой на $S$ если:
    $$
    \forall x_1, x_2 \in S: x_1 \neq x_2, \quad \forall \lambda \in (0, 1)
    $$
    $$
    f(\lambda x_1 + (1 - \lambda) x_2) < \lambda f(x_1) + (1 - \lambda) f(x_2)
    $$
    :::
1.  Надграфик функции $f(x): \mathbb{R}^n \to \mathbb{R}$.

    :::{.callout-tip appearance="simple"}
    Для функции, определённой на $S \subseteq \R^n$, множество:
    $$
    \text{epi}\; f = \left\{[x, \mu] \in S \times \R : f(x) \leq \mu \right\}
    $$
    называется надграфиком функции $f(x)$
    ![](epigraph.pdf){fig-align="center"}
    :::
1.  Множество подуровней функции $f(x): \mathbb{R}^n \to \mathbb{R}$.

    :::{.callout-tip appearance="simple"}
    Для функции, определённой на $S \subseteq \R^n$, множество:
    $$
    \mathcal{L}_{\beta} = \left\{x \in S : f(x) \leq \beta \right\}
    $$
    называется множеством подуровней или множеством Лебега функции $f(x)$

    Если функция выпукла, то множество её подуровней выпукло. Обратное - не верно ($f(x) = \sqrt{|x|}$).
    ![](sublevel_set.pdf){fig-align="center"}
    :::
1.  Дифференциальный критерий выпуклости первого порядка.

    :::{.callout-tip appearance="simple"}
    Дифференцируемая функция определенная на выпуклом множестве $S \subseteq \mathbb{R}^n$ выпукла тогда и только тогда когда $\forall x, y \in S$:
    $$f(y) \geq f(x) + \nabla f^T(x) (y - x)$$
    :::
1.  Дифференциальный критерий выпуклости второго порядка.

    :::{.callout-tip appearance="simple"}
    Дважды дифференцируемая функция определенная на выпуклом множестве $S \subseteq \mathbb{R}^n$ выпукла тогда и только тогда когда для любой внутренней точки x $\forall x \in \textbf{int}(S) \neq \emptyset$:
    $$\nabla^2f(x) \succeq 0$$
    :::
1.  Связь выпуклости функции и её надграфика.

    :::{.callout-tip appearance="simple"}
    Функция выпукла тогда и только тогда, когда её надграфик - выпуклое множество.
    :::
1.  $\mu$-сильно выпуклая функция.

    :::{.callout-tip appearance="simple"}
    Функция определенная на выпуклом множестве $S \subseteq \mathbb{R}^n$ называется сильно выпуклой если $\forall x_1, x_2 \in S$, $0 \leq \lambda \leq 1$ и $\mu > 0$:

    $$f(\lambda x_1 + (1 - \lambda) x_2) \leq \lambda f(x_1) + (1 - \lambda) f(x_2) - \frac{\mu}{2} \lambda(1 - \lambda) \|x_1 - x_2\|^2$$
    :::
1.  Дифференциальный критерий сильной выпуклости первого порядка.

    :::{.callout-tip appearance="simple"}
    Дифференцируемая функция определенная на выпуклом множестве $S \subseteq \mathbb{R}^n$ является сильно выпуклой тогда и только тогда, когда существует $\mu > 0$: $\forall x, y  \in S$:

    $$f(y) \geq f(x) + \nabla f^T(x) (y - x) + \frac{\mu}{2} \| y - x\|^2$$
    :::
1.  Дифференциальный критерий сильной выпуклости второго порядка.

    :::{.callout-tip appearance="simple"}
    Дважды дифференцируемая функция определенная на выпуклом множестве $S \subseteq \mathbb{R}^n$ является сильно выпуклой тогда и только тогда, когда существует $\mu > 0$

    $$\nabla^2f(x) \succeq \mu I$$
    :::
1.  Любые 2 операции с функциями, сохраняющие выпуклость.

    :::{.callout-tip appearance="simple"}

    1. Сумма выпуклых функций с не отрицательными коэффициентами является выпуклой функцией.
    1. Композиция выпуклой функции с афинной выпукла: $g(x) = f(Ax + b)$
    1. Поточечный максимум любого числа выпуклых функций есть выпуклая функция.
    :::
1.  Любые 2 нетривиальных свойства сопряженного множества.

    :::{.callout-tip appearance="simple"}

    1. Сопряженное множество всегда замкнуто, выпукло и содержит ноль.
    1. Для произвольного множества $S \subseteq \mathbb{R}^n$: $S^{**} = \overline{ \mathbf{conv} (S \cup \{0\}) }$
    1. Если $S_1 \subseteq S_2$, то $S_2^* \subseteq S_1^*$.
    1. $\left( \bigcup\limits_{i=1}^m S_i \right)^* = \bigcap\limits_{i=1}^m S_i^*$.
    1. Если $S$ замкнуто, выпукло и включает $0$, то $S^{**} = S$.
    1. $S^* = \left(\overline{S}\right)^*$.
    :::
1.  Является ли задача линейных наименьших квадратов для переопределенной линейной системы выпуклой/сильно выпуклой?

    :::{.callout-tip appearance="simple"}
    Рассмотрим задачу минимизации функции:
    $$
    \| A x - b \|^2 \to \min_{x \in \mathbb{R}^d},
    $$
    где матрица $A \in \mathbb{R}^{m\times n}, \ b\in \mathbb{R}^{m}$, $m > n$ (стоячая). Легко заметить, что гессиан минимизируемой функции $A^T A$ - невырожденная матрица размера $n \times n$. Она является положительно определенной. То есть задача в классической постановке является сильно выпуклой. Т.е. содержит единственный локальный минимум (единственное решение).
    :::
1.  Является ли задача линейных наименьших квадратов для недоопределенной линейной системы выпуклой/сильно выпуклой?

    :::{.callout-tip appearance="simple"}
    Рассмотрим задачу минимизации функции:
    $$
    \| A x - b \|^2 \to \min_{x \in \mathbb{R}^d},
    $$
    где матрица $A \in \mathbb{R}^{m\times n}, \ b\in \mathbb{R}^{m}$, $m < n$ (лежачая). Легко заметить, что гессиан минимизируемой функции $A^T A$ - вырожденная матрица размера $n \times n$. Однако, она является положительно полуопределенной. То есть задача в классической постановке является выпуклой, но не сильно выпуклой. Т.е. содержит бесконечное количество локальных минимумов, каждый из которых - глобальный. Стоит отметить, что добавление $\ell_2$ регуляризации к минимизируемой функции изменит задачу, однако, будет гарантировать сильную выпуклость.
    :::
1.  Сопряжённое множество.

    :::{.callout-tip appearance="simple"}
    Пусть $S \subseteq \mathbb{R}^n$ - произвольное непустое множество. Тогда его сопряженное множество определяется как:

    $$
    S^* = \{y \in \mathbb{R}^n \mid \langle y, x\rangle \ge -1 \;\; \forall x \in S\}
    $$
    :::
1.  Сопряжённый конус.

    :::{.callout-tip appearance="simple"}
    Сопряженным конусом к конусу $K$ называется множество $K^*$ такое, что: 

    $$
    K^* = \left\{ y \mid \langle x, y\rangle \ge 0 \quad \forall x \in K\right\}
    $$
    :::
1.  Сопряженная функция.

    :::{.callout-tip appearance="simple"}
    
    Для функции $f : \mathbb{R}^n \rightarrow \mathbb{R}$, сопряженной к ней называется функция $f^*$, причем область её определения можно считать теми $y$, для которых $\max$ конечен. 
    $$
    f^*(y) = \max_x \left[ y^T x - f(x)\right]
    $$ 

    ![](conj_function.pdf){width=40% fig-align="center"}
    :::


1.  Связь сильной выпуклости функции и гладкости сопряженной функции.

    :::{.callout-tip appearance="simple"}
    Пусть $f$ - замкнутая и выпуклая. Тогда $f$ - сильно выпуклая с константой выпуклости $\mu \Leftrightarrow \nabla f^*$ - липшицев с параметром $\frac{1}{\mu}$.
    :::
1.  Сопряжённая норма. Сопряжённая норма к векторной $p$-норме.

    :::{.callout-tip appearance="simple"}
    Сопряжённой нормой $\|\cdot\|_*$ к норме $\|\cdot\|$ называется норма, определённая как:
    $$
    \|y\|_* = \sup_{\|x\| \leq 1} \langle y, x \rangle,
    $$
    где $x, y \in \mathbb{R}^n$.

    Для $p$-нормы сопряжённой является $q$-норма, где $p$ и $q$ связаны соотношением:
    $$
    \frac{1}{p} + \frac{1}{q} = 1, \quad p, q \geq 1.
    $$

    Например:
    - Для $p = 1$ сопряжённой является $q = \infty$.
    - Для $p = 2$ сопряжённая норма также является $2$-нормой.
    - Для $p = \infty$ сопряжённой является $q = 1$.
    :::

1.  Субградиент. Субдифференциал.

    :::{.callout-tip appearance="simple"}
    Субградиент функции $f$ в точке $x$ — это вектор $g$, удовлетворяющий условию:
    $$
    f(y) \geqslant f(x) + g^T (y - x), \quad \forall y.
    $$
    Множество всех субградиентов в точке $x$ называется субдифференциалом и обозначается как $\partial f(x)$.

    ![Субдифференциал функции ReLU.](subgrad.pdf)
    :::
1.  Нормальный конус.

    :::{.callout-tip appearance="simple"}
    Для $x \in S$, $\partial I_S(x) = \mathcal{N}_S(x)$, **нормальный конус** к $S$ в точке $x$, напомним.

    $$
    \mathcal{N}_S(x) = \{ g \in \mathbb{R}^n : g^T x \geq g^T y \text{ для любого } y \in S \}
    $$
    ![](normal_cone.jpg)
    :::


1.  Теорема Моро — Рокафеллара.

    :::{.callout-tip appearance="simple"}
    Пусть $f_i(x)$ — выпуклые функции, определённые на выпуклых множествах $S_i, \; i = \overline{1, n}$. Если выполнено условие $\bigcap\limits_{i=1}^n \mathbf{ri } (S_i) \neq \emptyset$, то функция $f(x) = \sum\limits_{i=1}^n a_i f_i(x), \; a_i > 0$ имеет субдифференциал $\partial_S f(x)$ на множестве $S = \bigcap\limits_{i=1}^n S_i$, и его можно выразить следующим образом:
    $$
    \partial_S f(x) = \sum\limits_{i=1}^n a_i \partial_{S_i} f_i(x).
    $$
    Это означает, что субдифференциал линейной комбинации выпуклых функций равен взвешенной сумме их субдифференциалов, взятых на пересечении соответствующих множеств.
    :::

1.  Теорема Дубовицкого — Милютина.

    :::{.callout-tip appearance="simple"}
    Пусть $f_i(x)$ — выпуклые функции, определённые на открытом выпуклом множестве $S \subseteq \mathbb{R}^n$, и $x_0 \in S$. Пусть $f(x)$ определяется как покоординатный максимум этих функций:
    $$
    f(x) = \underset{i}{\operatorname{max}} f_i(x).
    $$
    Тогда субдифференциал $f(x_0)$ выражается следующим образом:
    $$
    \partial_S f(x_0) = \mathbf{conv}\left\{ \bigcup\limits_{i \in I(x_0)} \partial_S f_i(x_0) \right\},
    $$
    где множество $I(x)$ определяется как индексы функций, достигающих максимума:
    $$
    I(x) = \{ i \in [1:m] : f_i(x) = f(x) \}.
    $$
    Это утверждение говорит, что субдифференциал максимума выпуклых функций представляет собой выпуклую оболочку объединения субдифференциалов тех функций, которые достигают максимума в данной точке.
    :::

1.  Теорема Вейерштрасса.

    :::{.callout-tip appearance="simple"}
    Пусть $S \subset \mathbb{R}^n$ - компакт, а $f(x)$ - непрерывная функция на $S$. 
    Значит, точка глобального минимума функции $f (x)$ на $S$ существует.
    :::
1.  Теорема Тейлора.

    :::{.callout-tip appearance="simple"}
    $f: \mathbb{R}^n \to \mathbb{R}$ - непрерывная, дифференцируемая функция и $p \in \mathbb{R}^n$, тогда теорема Тейлора гласит:
    $$f(x + p) = f(x) + \nabla f(x + tp)^T p$$
    Для некоторого $t\in (0, 1)$\\

    Более того, если f - дважды дифференцируема, то:

    $$f(x + p) = f(x) + \nabla f(x)^T p + \frac{1}{2} p^T \nabla^2f(x + tp) p$$
    Для некоторого $t\in (0, 1)$
    :::

1.  Необходимые условия локального экстремума.

    :::{.callout-tip appearance="simple"}
    Если $x^*$ - локальный экстремум и f непрерывная дифференцируема в открытой окрестности $x^*$, то:
    $$ \nabla f(x^*) = 0 $$
    :::
1.  Достаточные условия локального экстремума.

    :::{.callout-tip appearance="simple"}
    Если $\nabla^2 f$ непрерывна в открытой окрестности $x^*$ и 
    $$\nabla f(x^*) = 0 $$
    $$ \nabla^2 f(x^*) \succ 0$$

    То $x^*$ - локальный минимум $f(x)$.\\
    Для локального максимума аналогично, только
    $$0\succ  \nabla^2 f(x^*) $$
    :::
1.  Принцип Ферма для минимума функции.

    :::{.callout-tip appearance="simple"}
    Пусть $f : \mathbb{R}^n \to\mathbb{R} \cup \{\infty\}$, тогда $x^*$ является глобальным минимумом $f$ тогда и только тогда, когда
    $$
    0 \in \partial f(x^*)
    $$
    :::

1.  Общая задача математического программирования. Функция Лагранжа.

    :::{.callout-tip appearance="simple"}
    $$
    \left\{
            \begin{aligned}
                & f_0(x) \to \min_{x \in \mathbb{R}^d} \\
                & f_i(x) \leq 0, \quad i = 1, \dots, m, \\
                & h_i(x) = 0, \quad i = 1, \dots, p.
            \end{aligned}
        \right.
    $$
    Функция Лагранжа:
    $$
    L(x, \lambda, \nu) = f_0(x) + \sum_{i = 1}^{m} \lambda_i f_i(x) + \sum_{i = 1}^{p} \nu_i h_i(x),
    $$
    :::
1.  Теорема Каруша - Куна - Таккера в форме необходимых условий решения задачи математического программирования.

    :::{.callout-tip appearance="simple"}
    Пусть $x_*$ - решение задачи с нулевым зазором двойственности 
    $$
    \left\{
            \begin{aligned}
                & f_0(x) \to \min_{x \in \mathbb{R}^d} \\
                & f_i(x) \leq 0, \quad i = 1, \dots, m, \\
                & h_i(x) = 0, \quad i = 1, \dots, p.
            \end{aligned}
        \right.
    $$
    Функция Лагранжа:
    $$
    L(x, \lambda, \nu) = f_0(x) + \sum_{i = 1}^{m} \lambda_i f_i(x) + \sum_{i = 1}^{p} \nu_i h_i(x),
    $$
    Тогда найдутся такие векторы~$\lambda^*$ и~$\nu^*$, что выполнены условия
    $$
    \left\{
            \begin{aligned}
                & \nabla f_0(x_*)
                    +
                    \sum_{i = 1}^{m} \lambda_i^* \nabla f_i(x_*)
                    +
                    \sum_{i = 1}^{p} \nu_i^* \nabla h_i(x_*) = 0 \\
                & f_i(x_*) \leq 0, \quad i = 1, \dots m \\
                & h_i(x_*) = 0, \quad i = 1, \dots p \\
                & \lambda_i^* \geq 0, \quad i = 1, \dots m \\
                & \lambda_i^* f_i(x_*) = 0, \quad i = 1, \dots m\\
                %& (?) \, \forall y \in C(x^{*}) : \nabla^2_{xx}L(x^{*}, \lambda^{*} )y\rangle > 0
            \end{aligned}
        \right.
    $$
    :::
1.  Условие Слейтера.

    :::{.callout-tip appearance="simple"}
    1. Если задача выпуклая (т.е., говоря о задаче минимизации, оптимизируемая функция $f_0$ и ограничения вида неравенство $f_i$ -- выпуклые, ограничения вида равенства $h_i$ -- аффинные)
    1. И существует точка $x$ такая, что $h(x) = 0$ и $f_i(x) < 0$ (ограничения вида равенства активные, а ограничения вида неравенства выполняются строго)

    То тогда задача имеет нулевой зазор двойственности и условия ККТ становятся необходимыми и достаточными.
    :::
1.  Задача выпуклого программирования.

    :::{.callout-tip appearance="simple"}
    Задача выпуклого программирования — это задача оптимизации, в которой целевая функция является выпуклой функцией и область допустимых решений выпукла. В форме ниже функции $f_0, \ldots, f_m$ - выпуклые, а функции $h_i$ - афинные.
    $$
    \left\{
            \begin{aligned}
                & f_0(x) \to \min_{x \in \mathbb{R}^d} \\
                & f_i(x) \leq 0, \quad i = 1, \dots, m, \\
                & h_i(x) = 0, \quad i = 1, \dots, p.
            \end{aligned}
        \right.
    $$

    :::
1.  Двойственная функция в задаче математического программирования.

    :::{.callout-tip appearance="simple"}
    Предположим, что $D = \bigcap\limits_{i=0}^m \textbf{dom}\,f_i \cap \bigcap\limits_{i=0}^p \textbf{dom}\,h_i$ непустое. Определим двойственную функцию $g : \mathbb{R}^{m}\times 
    \mathbb{R}^{p}\to \mathbb{R}$ как минимум лагранжиана по $x$
    : для $\lambda \in \mathbb{R}^m, \nu \in \mathbb{R}^p$ 
    $$
    g(\lambda, \nu) = \inf_{x\in D}
    L(x, \lambda, \nu) = \inf_{x\in D}
    f_0(x) +\sum^m_
    {i=1}
    \lambda_if_i(x) +\sum^p_
    {i=1}
    \nu_ih_i(x)
    $$
    Так как двойственная функция это поточечный инфинум семейства аффинных функций от $(\lambda, \nu)$, она вогнутая, даже если изначальная задача не выпуклая.
    :::
1.  Двойственная задача для задачи математического программирования.

    :::{.callout-tip appearance="simple"}
    Пусть $p^*$ - оптимальное решение изначальной задачи. Пусть $\hat{x}$ достижимая точка для изначальной задачи, т.е. $f_i(\hat{x}) \leq 0$ and $h_i(\hat{x}) = 0, \lambda \geq 0$. Тогда имеем:
    $$L(\hat{x}, \lambda, \nu) = f_0(\hat{x}) + \underbrace{\lambda^Tf(\hat{x})}_{\leq{0}}
    + \underbrace{\nu^T h(\hat{x})}_{=0}
    \leq f_0(\hat{x})$$
    Тогда
    $$g(\lambda, \nu) = \inf_{x\in D} L(x, \lambda, \nu) \leq L(\hat{x}, \lambda, \nu)\leq f_0(\hat{x})
    $$ $$g(\lambda, \nu) \leq p^{*}$$
    Двойственной задачей называется 
    $$g(\lambda, \nu) \to \max_{\lambda \in \mathbb{R}^m, \nu \in \mathbb{R}^p}$$
    $$s.t. \, \lambda \geq 0$$
    :::
1.  Сильная двойственность. Зазор двойственности.

    :::{.callout-tip appearance="simple"}
    Пусть $p^{*}$ -решение прямой задачи, $d^{*}$ - решение двойственной задачи. Зазором двойственности называется $$p^{*} - d^{*}\geq 0$$
    Сильная двойственность возникает, если зазор равен нулю
    $$p^{*} = d^{*}$$
    :::
1.  Локальный анализ чувствительности с помощью множителей Лагранжа.

    :::{.callout-tip appearance="simple"}
    Перейдем к возмущенной версии задачи:
    $$f_0(x) \to \min_x$$
    $$f_{i}(x)\leq u_i,\quad i=1,\dots ,m$$ 
    $$h_{i}(x)=v_i,\quad i=1,\dots ,p,$$
    Обозначим $p^*(u, v)$ - оптимальное решение этой задачи. Если имеет место сильная двойственность, то выполнено:
    $$p^*(u, v) \geq p^*(0, 0)-\lambda^{*^T}u-\nu^{*^T}v$$
    Если множители Лагранжа $\lambda_i^*, \nu_i^*$ большие, то небольшое изменение ограничений приведет к существенному изменению оптимального решения. То есть соответствующие ограничения очень сильно влияют на задачу.\\
    Если множители Лагранжа маленькие, то соответствующие ограничения мало влияют на задачу.
    $$
    \lambda_i^* = -\dfrac{\partial p^*(0,0)}{\partial u_i} \quad \nu_i^* = -\dfrac{\partial p^*(0,0)}{\partial v_i}
    $$
    :::
1.  Задача линейного программирования. Задача линейного программирования в стандартной форме.

    :::{.callout-tip appearance="simple"}
    Все задачи с линейным функционалом и линейными ограничениями считаются задачами линейного программирования. Стандартная форма:
    $$\min_{x \in \mathbb{R}^n} c^Tx$$
    $$s.t. Ax=b$$
    $$x_i \geq 0, i=1, \dots , n$$
    :::
1.  Возможные случаи двойственности в задаче линейного программирования.

    :::{.callout-tip appearance="simple"}
    Двойственная задача:
    $$\max_{\nu \in \mathbb{R}^m}-b^T\nu$$
    $$s.t. -A^T\nu \leq c$$
    1. Если либо у прямой, либо у двойственной задачи есть конечное решение, то и у другой тоже, и целевые переменные равны.
    1. Если либо прямая, либо двойственная задача неограничена, то вторая из них невыполнима.
    :::
1.  Симплекс метод.

    :::{.callout-tip appearance="simple"}
    Симплекс метод решает следующую задачу:
    $$\min_{x \in \mathbb{R}^n } c^\top x$$
    $$s.t. Ax \leq b$$

    Шаги выполнения симплекс метода:

    1. **Поиск начальной базисной допустимой точки:**
        Выберем начальную базисную (она является решением системы $A_B x = b_B$, где $B$ - базис размера $n$ пространства, а матрица $A$ обычно имеет больше $n$ ограничений) допустимую ($Ax_0 \leq b$) точку $x_0$ (искать ее будем через двухфазный симплексметод). Если такая точка не найдена, задача не имеет допустимого решения.

    1. **Проверка оптимальности:**
        - Разложение вектора $c$ в данном базисе $B$ с коэффициентами $\lambda_B$:
        $$
        \lambda_B^\top A_B = c^\top \quad \text{или} \quad \lambda_B^\top = c^\top A_B^{-1}
        $$
        - Если все компоненты $\lambda_B$ неположительны, текущий базис является оптимальным. Иначе далее меняем вершину симплекса.

    1. **Определение переменной для удаления из базиса:**
    1. **Вычисление шага вдоль выбранного направления $d$:**
        - Для всех $j \notin B$ считаем шаг:
        $$
        \mu_j = \frac{b_j - a_j^\top x_B}{a_j^\top d}
        $$
        - Новая вершина, которую добавим в базис:
        $$
        t = \arg\min_j \{\mu_j \mid \mu_j > 0\}
        $$

    1. **Обновление базиса:**
    1. **Повторение:**
        - Далее повторяем шаги 2-5 до достижения оптимального решения или установления, что задача не имеет допустимого решения.
    :::
1.  Нахождение первоначальной угловой точки с помощью двухфазного симплекс метода.

    :::{.callout-tip appearance="simple"}
    1. Рассмотрим задачу (Phase 1):
        $$\min_{\xi \in \mathbb{R}^m, y \in \mathbb{R}^n, z \in \mathbb{R}^n } \sum^m_{i=1} \xi_i$$
        $$s.t. Ay - Az \leq b + \xi$$
        $$y \geq 0, z \geq 0, \xi \geq 0$$
        Для нее есть допустимая угловая точка $z = 0, y = 0, \xi_i = \max (0, -b_i)$. Начиная с нее, решим задачу симплекс методом и получим точку оптимума, в которой $\xi = 0$ и выполнены указанные ограничения.
    1. Решение задачи Phase 1 является допустимым базисом задачи Phase 2:
        $$\min_{y \in \mathbb{R}^n, z \in \mathbb{R}^n } c^\top (y - z)$$
        $$s.t. Ay - Az \leq b$$
        $$y \geq 0, z \geq 0$$
    1. Заметим, что оно так же будет являться допустимым базисом и угловой точкой для исходной задачи:
        $$\min_{x \in \mathbb{R}^n } c^\top x$$
        $$s.t. Ax \leq b$$
    1. Так и нашли первоначальную угловую точку для исходной задачи.
    :::
1.  Сходимость симплекс метода.

    :::{.callout-tip appearance="simple"}
    В худшем случае симплекс метод сходится экспоненциально от размерности задачи, но на практике в среднем алгоритм работает сильно лучше (полиномиально). Задача, на которой симплекс метод работает экспоненциальное время, называется примером Klee Minty.
    :::
1.  Теорема о связи задач max-flow и min-cut (надо суметь описать обе задачи).

    :::{.callout-tip appearance="simple"}
    **Задача Max-Flow (максимальный поток):**
    Дано ориентированное взвешенное графовое представление сети, где узлы — это вершины графа, а ребра имеют пропускную способность (capacity). Требуется найти максимальный поток из источника (source) в сток (sink), при условии:
    1. Поток на каждом ребре не превышает его пропускную способность.
    2. Сохраняется закон сохранения потока в промежуточных узлах (входящий поток равен исходящему, за исключением источника и стока).

    ![](maxflow.pdf){fig-align="center" width=40%}

    **Задача Min-Cut (минимальный разрез):**
    Для той же сети требуется найти разрез — разделение вершин графа на два множества (одно включает источник, другое — сток), при котором суммарная пропускная способность ребер, пересекающих разрез, минимальна.

    ![](mincut1.pdf){width=40%} ![](mincut2.pdf){width=40%}

    **Теорема Max-Flow Min-Cut:**
    Максимальный поток из источника в сток равен минимальной пропускной способности разреза между источником и стоком.

    Формально:
    $$
    \text{MAXFLOW} = \text{MINCUT}.
    $$

    Эта теорема утверждает, что задачи поиска максимального потока и минимального разреза являются двойственными: решение одной задачи предоставляет решение другой.
    :::

1.  Линейная сходимость последовательности. 

    :::{.callout-tip appearance="simple"}
    Пусть есть последовательность $\{ \| x_k - x^* \|_2 \}$ в $\mathbb{R}$, сходящаяся к $0$.

    Линейная сходимость при $q \in (0, 1)$ (скорость сходимости) и $C \in (0, \infty)$ (константа сходимости) определяется одним из двух способов:

    $$
    \| x_{k+1} - x^* \| \leq C q^k \,\,\, \text{или} \,\,\, \| x_{k+1} - x^* \| \leq q \| x_{k} - x^* \|
    $$

    Чем меньше $q$, тем быстрее сходится последовательность.

    По-другому, говорят, что последовательность ${x_k}$ сходится к числу $L$. Мы говорим, что эта последовательность линейно сходится к $L$, если $\exists$ число $\mu \in (0,1)$, такое, что

    $$
    \lim_{k \to \infty} \frac{|x_{k+1}-L|}{|x_k-L|} = \mu
    $$

    и $\mu$ называется скоростью сходимости.
    :::
1.  Сублинейная сходимость последовательности. 

    :::{.callout-tip appearance="simple"}
    Если последовательность $r_k$ сходится к нулю, но не обладает линейной сходимостью, то говорят, что она сходится сублинейно. Иногда мы можем рассматривать следующий класс сублинейной сходимости:

    $$
    |x_{k+1} - x^*|_2 \leq Ck^q,
    $$

    где $q < 0$ и $0 < C < \infty$.
    :::
1.  Сверхлинейная сходимость последовательности. 

    :::{.callout-tip appearance="simple"}
    Мы определяем сверхлинейную сходимость как сходимость последовательности, которая быстрее любой линейной сходимости. Иногда рассматривают более специальный класс. Тогда говорят, что сверхлинейная сходимость при $q>1$, $C>0$ определяется следующим образом:

    $$
    | x_{k+1} - x^* | \leq C | x_k - x^* |^q
    $$
    :::
1.  Квадратичная сходимость последовательности.

    :::{.callout-tip appearance="simple"}
    Квадратичная сходимость является частным случаем сверхлинейной сходимости, когда $q=2$. Она определяется следующим образом:

    $$
    | x_{k+1} - x^* | \leq C | x_k - x^* |^2
    $$

    Или по-другому:

    $$
    \lim_{k \to \infty} \frac{|x_{k+1}-L|}{|x_k-L|^2} = \mu
    $$

    где $\mu > 0$.
    :::
1.  Тест корней для определения скорости сходимости последовательности.

    :::{.callout-tip appearance="simple"}
    Пусть $(r_k)_{k=m}^\infty$ - последовательность неотрицательных чисел, сходящаяся к нулю, и пусть $\alpha := \limsup_{k \to \infty} r_k^{1/k}$. (Заметим, что $\alpha \ge 0$.)

    1. Если $0 \le \alpha < 1$, то $(r_k)_{k=m}^\infty$ сходится линейно с константой $\alpha$.
    1. В частности, если $\alpha = 0$, то $(r_k)_{k=m}^\infty$ сходится сверхлинейно.
    1. Если $\alpha = 1$, то $(r_k)_{k=m}^\infty$ сходится сублинейно.
    1. Случай $\alpha > 1$ невозможен.
    :::
1.  Тест отношений для определения скорости сходимости последовательности.

    :::{.callout-tip appearance="simple"}
    Пусть ${r_k}_{k=m}^\infty$ - последовательность строго положительных чисел, сходящаяся к нулю. Пусть
    $$
    q = \lim_{k \to \infty} \frac{r_{k+1}}{r_k}
    $$

    1. Если существует $q$ и $0 \le q < 1$, то ${r_k}_{k=m}^\infty$ имеет линейную сходимость с константой $q$.

    2. В частности, если $q = 0$, то ${r_k}_{k=m}^\infty$ имеет сверхлинейную сходимость.

    3. Если $q$ не существует, но $q = \lim_{k \to \infty} \sup_k \frac{r_{k+1}}{r_k} < 1$, то ${r_k}_{k=m}^\infty$ имеет линейную сходимость с константой, не превышающей $q$.

    4. Если $\lim_{k \to \infty} \inf_k \frac{r_{k+1}}{r_k} = 1$, то ${r_k}_{k=m}^\infty$ имеет сублинейную сходимость.

    5. Случай $\lim_{k \to \infty} \inf_k \frac{r_{k+1}}{r_k} > 1$ невозможен.
    :::
1.  Унимодальная функция.

    :::{.callout-tip appearance="simple"}
    Функция $f(x)$ называется унимодальной на $[a, b]$, если существует $x^* \in [a, b]$, такое, что

    1. $f(x_1) > f(x_2)$ для всех $a \le x_1 < x_2 < x^*$

    2. $f(x_1) < f(x_2)$ для всех $x^* < x_1 < x_2 \le b$
    :::
1.  Метод дихотомии.

    :::{.callout-tip appearance="simple"}
    Наша цель - решить следующую задачу: $\min_{x \in [a, b]} f(x)$ Мы делим отрезок на две равные части и выбираем ту, которая содержит решение задачи, используя значения функции, опираясь на ключевое свойство, описанное выше. Наша цель после одной итерации метода - уменьшить область поиска решения в два раза (в среднем). Метод описан на рисунках ниже.

    ![Диаграмма метода дихотомии](Dichotomy4.pdf)

    Длина отрезка на $(k+1)$-ой итерации:
    $$
    \Delta_{k+1} = b_{k+1} - a_{k+1} = \frac{1}{2^k} (b - a)
    $$
    Для унимодальных функций:
    $$
    |x_{k+1} - x^*| \le \frac{\Delta_{k+1}}{2} \le \frac{1}{2^{k+1}} (b - a) \le (0.5)^{k+1} \cdot (b - a)
    $$
    Заметим, что на каждой итерации мы обращаемся к оракулу не более чем два раза, поэтому число вычислений функции равно $N = 2 \cdot k$, что подразумевает:
    $$
    |x_{k+1} - x^*| \le (0.5)^{\frac{N}{2} + 1} \cdot (b - a) \le (0.707)^N \frac{b - a}{2}
    $$
    :::
1.  Метод золотого сечения.

    :::{.callout-tip appearance="simple"}
    Общая идея: хотим поделить отрезок на 3 части так, чтобы потом когда одна из частей отпадет на следующей итерации одно из нужных значений функций будет уже известно.

    ![Иллюстрация метода золотого сечения](golden_search.pdf)

    ```python
    def golden_search(f, a, b, epsilon):
        tau = (sqrt(5) + 1) / 2
        y = a + (b - a) / tau**2
        z = a + (b - a) / tau
        while b - a > epsilon:
            if f(y) <= f(z):
                b = z
                z = y
                y = a + (b - a) / tau**2
            else:
                a = y
                y = z
                z = a + (b - a) / tau
        return (a + b) / 2
    ```
    :::
1.  Метод параболической интерполяции (без точных формул).

    :::{.callout-tip appearance="simple"}
    Идея метода: берем 3 точки, по этим 3 точкам однозначно строим параболу, находим ее минимум, и из этих 4 точек оставляем 3 так, чтобы между первой и третьей находился минимум.

    ```python
    def parabola_search(f, x1, x2, x3, epsilon):
        f1, f2, f3 = f(x1), f(x2), f(x3)
        while x3 - x1 > epsilon:
            u = x2 - ((x2 - x1)**2*(f2 - f3) - (x2 - x3)**2*(f2 - f1))/(2*((x2 - x1)*(f2 - f3) - (x2 - x3)*(f2 - f1)))
            fu = f(u)

            if x2 <= u:
                if f2 <= fu:
                    x1, x2, x3 = x1, x2, u
                    f1, f2, f3 = f1, f2, fu
                else:
                    x1, x2, x3 = x2, u, x3
                    f1, f2, f3 = f2, fu, f3
            else:
                if fu <= f2:
                    x1, x2, x3 = x1, u, x2
                    f1, f2, f3 = f1, fu, f2
                else:
                    x1, x2, x3 = u, x2, x3
                    f1, f2, f3 = fu, f2, f3
        return (x1 + x3) / 2
    ```

    Сходится сверхлинейно, но метод довольно неустойчивый. Если $f(x)$ не похожа на параболу, нам конец. Если она обратна параболе, то мы и вовсе уйдём искать максимум.
    :::
1.  Условие достаточного убывания для неточного линейного поиска.

    :::{.callout-tip appearance="simple"}
    Неточный линейный поиск:

    $$
    x_{k+1} = x_k - \alpha \nabla f(x_k) \\
    \alpha = \text{argmin } f(x_{k+1}) 
    $$

    Хотим приближенно найти $\alpha$. Сведем задачу к поиску минимума следующей функции:

    $$
    \phi(\alpha) = f(x_k - \alpha\nabla f(x_k)), \alpha \geq 0
    $$

    Приблизим ее через первые 2 члена ряда Тейлора:

    $$
    \phi(\alpha) \approx f(x_k) - \alpha\nabla f(x_k)^\top \nabla f(x_k)
    $$

    Тогда условием достаточного убывания (Armijo condition) является:

    $$
    f(x_k - \alpha \nabla f (x_k)) \leq f(x_k) - c_1 \cdot \alpha\nabla f(x_k)^\top \nabla f(x_k), c_1 \in (0, 1)
    $$

    Иллюстрация для понимания:

    ![Иллюстрация условия достаточного убывания](sufficient%20decrease.pdf)
    :::
1.  Условия Гольдштейна для неточного линейного поиска.

    :::{.callout-tip appearance="simple"}
    Определим $\phi_1$ и $\phi_2$ следующим образом ($c_1 > c_2$)
    $$
    \phi_1(\alpha) = f(x_k) - c_1 \alpha \|\nabla f(x_k)\|^2
    $$

    $$
    \phi_2(\alpha) = f(x_k) - c_2 \alpha \|\nabla f(x_k)\|^2
    $$

    Тогда условие Гольдштейна заключается в том, что $\phi_1(\alpha) \leq \phi(\alpha) \leq \phi_2(\alpha)$.

    Иллюстрация для понимания:

    ![Иллюстрация условий Гольдштейна](Goldstein.pdf)
    :::
1.  Условие ограничения на кривизну для неточного линейного поиска.

    :::{.callout-tip appearance="simple"}
    $$
    -\nabla f (x_k - \alpha \nabla f(x_k))^\top \nabla f(x_k) \geq c_2 \nabla f(x_k)^\top(- \nabla f(x_k)),
    $$
    где $c_2\in (c_1, 1)$, и $c_1$ взято из условия достаточного убывания.

    Иллюстрация для понимания:

    ![Иллюстрация условия ограничения на кривизну](Curvature.pdf)
    :::

# Теоремы с доказательствами

1.  Критерий положительной определенности матрицы через знаки собственных значений матрицы.

    :::{.callout-note appearance="simple"}
    $A \succeq (\succ) 0 \Longleftrightarrow$ все собственные значения матрицы $A \geq (>) 0$ 
    :::

    $\rightarrow$  Пусть некоторые собственные значения $\lambda$ отрицательны, и $x$ - соответствующий ему собственный вектор.  Тогда:

    $Ax = \lambda x, x^{T} A x \geq 0 \rightarrow x^{T} A x = \lambda x^T  x$, $x^T x \geq 0 \rightarrow \lambda \geq 0$ - противоречие

    $\leftarrow$ Помним, что положительная определённость задаётся для симметричных матриц. Для симметричной матрицы можем выбрать собственные векторы $v_i$, образующие ортогональный базис ($i\neq j: v_i^T v_j = 0$ - выкиываем часть слогаемых из суммы в доказательстве). Тогда для $x \in \mathbb{R}^{n}$

    $$x^T A x = (\alpha_1 v_1 + \dots + \alpha_{n} v_{n})^T A (\alpha_1 v_1 + \dots + \alpha_{n} v_{n}) = \sum \alpha_{i}^2 v_{i}^T A v_{i} = \sum \alpha_{i}^2 v_{i}^T \lambda v_{i}$$ 

    Так как $\lambda_{i} \geq 0$, то и вся сумма неотрицательна.
    
1.  Связь $\dfrac{\partial L}{\partial W}$ и $\dfrac{\partial L}{\partial \Sigma}$, если $W = U \Sigma V^T \in \mathbb{R}^{m \times n}$, при этом
    $$
    U^TU = I, \quad V^TV = I, \quad \Sigma = \text{diag}(\sigma_1, \ldots, \sigma_{\min(m,n)})
    $$

    :::{.callout-note appearance="simple"}
    $$
    \dfrac{\partial L}{\partial W} = U\dfrac{\partial L}{\partial \Sigma}V^T
    $$ 
    :::

    ![](svd_singular_regularizer_comp_graph.pdf){width=40% fig-align="center"}

    Пусть у нас есть прямоугольная матрица $W \in \mathbb{R}^{m \times n}$, имеющая сингулярное разложение:
    $$
    W = U \Sigma V^T, \quad U^TU = I, \quad V^TV = I, \quad \Sigma = \text{diag}(\sigma_1, \ldots, \sigma_{\min(m,n)})
    $$
    1. Отметим:
        $$
        \begin{split}
        W &= U \Sigma V^T \\\
        dW &= dU \Sigma V^T + U d\Sigma V^T + U \Sigma dV^T \\
        U^T dW V &= U^TdU \Sigma V^TV + U^TU d\Sigma V^TV + U^TU \Sigma dV^TV \\\
        U^T dW V &= U^TdU \Sigma + d\Sigma + \Sigma dV^TV
        \end{split}
        $$
    2. Заметим, что $U^T U = I \to dU^TU + U^T dU = 0$. Но также $dU^TU = (U^T dU)^T$, из чего фактически следует, что матрица $U^TdU$ антисимметрична:
        $$
        (U^T dU)^T + U^T dU = 0 \quad \to \quad \text{diag}( U^T dU) = (0, \ldots, 0)
        $$
        Та же логика может быть применена к матрице $V$ и
        $$
        \text{diag}(dV^T V) = (0, \ldots, 0)
        $$

    3. В то же время матрица $d \Sigma$ диагональная, а это значит (посмотрите на 1.), что
        $$
        \text{diag}(U^T dW V) = d \Sigma 
        $$
        Здесь с обеих сторон мы имеем диагональные матрицы.

    4. Теперь мы можем разложить дифференциал функции потерь как функцию от $\Sigma$ - такие задачи возникают в ML, где нужно ограничить ранг матрицы:
        $$
        \begin{split}
        dL &= \left\langle\dfrac{\partial L}{\partial \Sigma}, d\Sigma \right\rangle \\
        &= \left\langle\dfrac{\partial L}{\partial \Sigma}, \text{diag}(U^T dW V)\right\rangle \\
        &= \text{tr}\left(\dfrac{\partial L}{\partial \Sigma}^T \text{diag}(U^T dW V)\right)
        \end{split}
        $$

    5. Поскольку внутри произведения находятся диагональные матрицы, след диагональной части матрицы будет равен следу всей матрицы:
        $$
        \begin{split}
        dL &= \text{tr}\left(\dfrac{\partial L}{\partial \Sigma}^T \text{diag}(U^T dW V)\right)\\
        &= \text{tr}\left(\dfrac{\partial L}{\partial \Sigma}^T U^T dW V \right)\\
        &= \left\langle\dfrac{\partial L}{\partial \Sigma}, U^T dW V \right\rangle \\
        &= \left\langle U \dfrac{\partial L}{\partial \Sigma} V^T, dW \right\rangle 
        \end{split}
        $$

1.  Базовые операции, сохраняющие выпуклость множеств: пересечение бесконечного числа множеств, линейная комбинация множеств, образ афинного отображения.

    :::{.callout-note appearance="simple"}
    * Пересечение любого (!) количества выпуклых множеств — выпуклое множество.
    * Линейная комбинация выпуклых множеств выпукла.
    * Образ выпуклого множества после применения афинного отображения — выпуклое множество.
    :::

    **Пересечение бесконечного числа множеств**

    Пересечение любого (!) количества выпуклых множеств — выпуклое множество.

    Если итоговое пересечение пустое или содержит одну точку, то свойство выпуклости выполняется по определению. Иначе возьмем 2 точки и отрезок между ними. Эти точки должны лежать во всех пересекаемых множествах. Так как все пересекаемые множества выпуклы, отрезок между этими двумя точками лежит во всех множествах. А значит, отрезок лежит и в их пересечении.

    **Линейная комбинация множеств**

    Линейная комбинация выпуклых множеств выпукла.

    Пусть есть 2 выпуклых множества $S_x, S_y$, рассмотрим их линейную комбинацию
        $$
        S = \left\{s \mid s = c_1 x + c_2 y, \; x \in S_x, \; y \in S_y, \; c_1, c_2 \in \mathbb{R}\right\}
        $$
    Возьмем две точки из $S$: $s_1 = c_1 x_1 + c_2 y_1, s_2 = c_1 x_2 + c_2 y_2$ и докажем, что отрезок между ними $\theta s_1 + (1 - \theta)s_2, \theta \in [0,1]$ также принадлежит $S$
        $$
        \theta s_1 + (1 - \theta)s_2
        $$
        $$
        \theta (c_1 x_1 + c_2 y_1) + (1 - \theta)(c_1 x_2 + c_2 y_2)
        $$
        $$
        c_1 (\theta x_1 + (1 - \theta)x_2) + c_2 (\theta y_1 + (1 - \theta)y_2)
        $$
        $$
        c_1 x + c_2 y \in S
        $$

    **Образ афинного отображения**

    Образ выпуклого множества после применения афинного отображения — выпуклое множество.
        $$
        S \subseteq \mathbb{R}^n \text{ выпукло}\;\; \rightarrow \;\; f(S) = \left\{ f(x) \mid x \in S \right\} \text{ выпукло} \;\;\;\; \left(f(x) = \mathbf{A}x + \mathbf{b}\right)
        $$

    **Доказательство**

    При $\theta \in [0, 1]; x, y \in S, S$ — выпуклое. Тогда и $\theta x + (1 - \theta)y \in S$. В то же время $f(\theta x + (1 - \theta)y) = \theta A x + \theta b + (1 - \theta) A y + (1 - \theta) b = \theta A x + (1 - \theta) A y + b = \theta f(x) + (1 - \theta)f(y)$. В итоге мы доказали, что образ $f(S)$ — тоже выпуклый, так как $\forall \theta \in [0, 1], x, y \in S$ выполняется $\theta f(x) + (1 - \theta)f(y) \in f(S)$.

    Примеры афинных функций: растяжение, сжатие, проекция, транспонирование, множество решений линейного матричного неравенства $\left\{ x \mid x_1 A_1 + \ldots + x_m A_m \preceq B\right\}$. Здесь $A_i, B \in \mathbf{S}^p$ — симметричные матрицы $p \times p$. 

    Заметим также, что прообраз выпуклого множества при аффинном отображении также является выпуклым.
        $$
        S \subseteq \mathbb{R}^m \text{ convex}\; \rightarrow \; f^{-1}(S) = \left\{ x \in \mathbb{R}^n \mid f(x) \in S \right\} \text{ convex} \;\; \left(f(x) = \mathbf{A}x + \mathbf{b}\right)
        $$

1.  Неравенство Йенсена для выпуклой функции и выпуклой комбинации точек.

    :::{.callout-note appearance="simple"}
    Пусть $f(x)$ -- выпуклая функция, определённая на выпуклом множестве $S \subseteq \mathbb{R}^n$. Тогда для точек $x_1, \dots, x_m \in S$ выполнено неравенство:

    $$f \left( \sum \limits_{i=1}^m \lambda_i x_i \right) \leq \sum \limits_{i=1}^m \lambda_i f(x_i)$$

    $\lambda=[\lambda_1, \dots, \lambda_m] \in \Delta_m$.
    :::

    1. Заметим, что $\sum \limits_{i=1}^m \lambda_i x_i$ является выпуклой комбинацией элементов $S$ и лежит в $S$.

    2. Доказательство по индукции. Для $m=1$ очевидно, для $m=2$ следует из определения выпуклой функции.

    3. Пусть неравенство верно для $m=1, \dots, k$, докажем для $m=k+1$. Пусть $\lambda \in \Delta_{k+1}$, $x=\sum \limits_{i=1}^{k+1} \lambda_i x_i = \lambda_{k+1} x_{k+1} + \sum \limits_{i=1}^{k} \lambda_i x_i$. При $\lambda_i = 0$ либо $1$ выражение сводится к уже рассмотренным случаям, далее полагаем $0 < \lambda_i < 1$:
        $$
        x=\lambda_{k+1} x_{k+1} + (1-\lambda_{k+1}) \sum \limits_{i=1}^{k} \frac{\lambda_i}{1-\lambda_{k+1}} x_i =\lambda_{k+1} x_{k+1} + (1-\lambda_{k+1}) \hat{x}
        $$
    где $\hat{x} = \sum \limits_{i=1}^{k} \gamma_i x_i$ и $\gamma_i=\frac{\lambda_i}{1-\lambda_{k+1}} \geq 0,\,\, 1\geq i \geq k$.

    4. Так как $\lambda \in \Delta_{k+1}$, то $\gamma=[\gamma_1, \dots, \gamma_k] \in \Delta_k$. Значит, $\hat{x} \in S$, из выпуклости $f(x)$ и предположения индукции следует:
        $$
        f \left( \sum \limits_{i=1}^{k+1} \lambda_i x_i \right) = f(\lambda_{k+1} x_{k+1} + (1-\lambda_{k+1})\hat{x}) \leq \lambda_{k+1} f(x_{k+1})+(1-\lambda_{k+1})f(\hat{x}) \leq \sum \limits_{i=1}^{k+1} \lambda_i f(x_i)
        $$

1.  Выпуклость надграфика как критерий выпуклости функции.

    :::{.callout-note appearance="simple"}
    Чтобы функция $f(x)$, определенная на выпуклом множестве $X$, была выпуклой на $X$, необходимо и достаточно чтобы надграфик $f$ был выпуклым множеством.
    :::

    Для функции $f(x)$, определенной на $S \subseteq \mathbb{R}^n$, множество:
        $$
        \text{epi}\ f = \left\{[x, \mu] \in S \times \mathbb{R}: f(x) \le \mu \right\}
        $$
    называется **надграфиком** функции $f(x)$ (здесь $\mu \in \mathbb{Р}, x \in S$).


    **Необходимость**

    Предположим, что $f(x)$ выпукла на $X$. Возьмем две произвольные точки $[x_1, \mu_1] \in \text{epi}f$ и $[x_2, \mu_2] \in \text{epi}f$. Также возьмем $0 \leq \lambda \leq 1$ и обозначим $x_{\lambda} = \lambda x_1 + (1 - \lambda) x_2, \mu_{\lambda} = \lambda \mu_1 + (1 - \lambda) \mu_2$. Тогда,
        $$
        \lambda\begin{bmatrix} x_1 \\ \mu_1 \end{bmatrix} + (1 - \lambda)\begin{bmatrix} x_2 \\ \mu_2 \end{bmatrix} = \begin{bmatrix} x_{\lambda} \\ \mu_{\lambda} \end{bmatrix}.
        $$
    Из выпуклости $X$ следует, что $x_{\lambda} \in X$. Более того, так как $f(x)$ -- выпуклая функция, то
        $$
        f(x_{\lambda}) \leq \lambda f(x_1) + (1 - \lambda) f(x_2) \leq \lambda \mu_1 + (1 - \lambda) \mu_2 = \mu_{\lambda}
        $$
    Из неравенства выше по определению надграфика следует, что $\begin{bmatrix} x_{\lambda} \\ \mu_{\lambda} \end{bmatrix} \in \text{epi}f$. Следовательно, надграфик $f$ -- выпуклое множество.

    **Достаточность**

    Предположим, что надграфик $f$, $\text{epi}f$, выпуклое множество. Тогда, исходя из того что $[x_1, \mu_1] \in \text{epi}f$ и $[x_2, \mu_2] \in \text{epi}f$, получаем
        $$
        \begin{bmatrix} x_{\lambda} \\ \mu_{\lambda} \end{bmatrix} = \lambda\begin{bmatrix} x_1 \\ \mu_1 \end{bmatrix} + (1 - \lambda)\begin{bmatrix} x_2 \\ \mu_2 \end{bmatrix} \in \text{epi}f
        $$
    для любого $0 \leq \lambda \leq 1$.

    Следовательно, из определения надграфика, подставив значение $\mu_{\lambda}$, получаем, что $f(x_{\lambda}) \leq \mu_{\lambda} = \lambda \mu_1 + (1 - \lambda) \mu_2$. 
        $$
        f(x_{\lambda}) = f(\lambda x_1 + (1 - \lambda) x_2) \leq \mu_\lambda = \lambda \mu_1 + (1 - \lambda) \mu_2
        $$

    Но это верно для всех $\mu_1 \geq f(x_1)$ и $\mu_2 \geq f(x_2)$, в том числе и при $\mu_1 = f(x_1)$ и $\mu_2 = f(x_2)$. Тогда мы получаем неравенство:
        $$
        f(x_{\lambda}) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)
        $$

    Так как $x_1 \in X$ и $x_2 \in X$ выбирались произвольно, $f(x)$ - выпуклая функция на $X$.

1.  Дифференциальный критерий сильной выпуклости первого порядка.

    :::{.callout-note appearance="simple"}
    Пусть $f(x)$ — дифференцируемая функция на выпуклом множестве $X \subseteq \mathbb{R}^n$. Тогда $f(x)$ сильно выпукла на $X$ с константой $\mu > 0$ тогда и только тогда, когда

    $$
    f(x) - f(x_0) \geq \langle \nabla f(x_0), x - x_0 \rangle + \frac{\mu}{2} \|x - x_0\|^2
    $$

    для всех $x, x_0 \in X$.
    :::

    **Необходимость**

    Пусть $0 < \lambda \leq 1$. Согласно определению сильно выпуклой функции,

    $$
    f(\lambda x + (1 - \lambda)x_0) \leq \lambda f(x) + (1 - \lambda) f(x_0) - \frac{\mu}{2} \lambda (1 - \lambda) \|x - x_0\|^2
    $$

    или эквивалентно,

    $$
    f(x) - f(x_0) - \frac{\mu}{2} (1 - \lambda) \|x - x_0\|^2 \geq \frac{1}{\lambda} \left[ f(\lambda x + (1 - \lambda)x_0) - f(x_0) \right] =
    $$

    $$
    = \frac{1}{\lambda} \left[ f(x_0 + \lambda(x - x_0)) - f(x_0) \right] = \frac{1}{\lambda} \left[ \lambda \langle \nabla f(x_0), x - x_0 \rangle + o(\lambda) \right] =
    $$

    $$
    = \langle \nabla f(x_0), x - x_0 \rangle + \frac{o(\lambda)}{\lambda}.
    $$

    Таким образом, переходя к пределу при $\lambda \to 0$, мы приходим к первоначальному утверждению.

    **Достаточность**

    Предположим, что неравенство в теореме выполнено для всех $x, x_0 \in X$. Возьмем $x_0 = \lambda x_1 + (1 - \lambda)x_2$, где $x_1, x_2 \in X$, $0 \leq \lambda \leq 1$. Согласно неравенству из условия теоремы, выполняются следующие неравенства:

    $$
    f(x_1) - f(x_0) \geq \langle \nabla f(x_0), x_1 - x_0 \rangle + \frac{\mu}{2} \|x_1 - x_0\|^2,
    $$

    $$
    f(x_2) - f(x_0) \geq \langle \nabla f(x_0), x_2 - x_0 \rangle + \frac{\mu}{2} \|x_2 - x_0\|^2.
    $$

    Умножая первое неравенство на $\lambda$ и второе на $1 - \lambda$ и складывая их, учитывая, что

    $$
    x_1 - x_0 = (1 - \lambda)(x_1 - x_2), \quad x_2 - x_0 = \lambda(x_2 - x_1),
    $$

    и что $\lambda(1 - \lambda)^2 + \lambda^2(1 - \lambda) = \lambda(1 - \lambda)$, получаем:

    $$
    \lambda f(x_1) + (1 - \lambda)f(x_2) - f(x_0) - \frac{\mu}{2} \lambda(1 - \lambda) \|x_1 - x_2\|^2 \geq
    $$

    $$
    \geq \langle \nabla f(x_0), \lambda x_1 + (1 - \lambda)x_2 - x_0 \rangle = 0.
    $$

    Таким образом, неравенство из определения сильно выпуклой функции выполнено. Важно отметить, что при $\mu = 0$ получаем случай выпуклости и соответствующий дифференциальный критерий.

1.  Дифференциальный критерий сильной выпуклости второго порядка.

    :::{.callout-note appearance="simple"}

    Пусть $X \subseteq \mathbb{R}^n$ — выпуклое множество с непустой внутренностью. Пусть также $f(x)$ — дважды непрерывно дифференцируемая функция на $X$. Тогда $f(x)$ сильно выпукла на $X$ с константой $\mu > 0$ тогда и только тогда, когда

    $$
    \langle y, \nabla^2 f(x) y \rangle \geq \mu \|y\|^2
    $$

    для всех $x \in X$ и $y \in \mathbb{R}^n$.

    Другая форма записи:
    $$ \nabla^2 f(x) \succcurlyeq \mu I $$
    :::

    Целевое неравенство тривиально, когда $y = 0_n$, поэтому предположим, что $y \neq 0_n$.

    **Необходимость**

    Пусть $x$ является внутренней точкой $X$. Тогда $x + \alpha y \in X$ для всех $y \in \mathbb{R}^n$ и достаточно малых $\alpha$. Поскольку $f(x)$ дважды дифференцируема,

    $$
    f(x + \alpha y) = f(x) + \alpha \langle \nabla f(x), y \rangle + \frac{\alpha^2}{2} \langle y, \nabla^2 f(x) y \rangle + o(\alpha^2).
    $$

    Основываясь на критерии первого порядка сильной выпуклости, имеем

    $$
    \frac{\alpha^2}{2} \langle y, \nabla^2 f(x) y \rangle + o(\alpha^2) = f(x + \alpha y) - f(x) - \alpha \langle \nabla f(x), y \rangle \geq \frac{\mu}{2} \alpha^2 \|y\|^2.
    $$

    Это неравенство сводится к целевому неравенству после деления обеих частей на $\alpha^2$ и перехода к пределу при $\alpha \to 0$.

    Если $x \in X$, но $x \notin \text{int}X$, рассмотрим последовательность $\{x_k\}$ такую, что $x_k \in \text{int}X$ и $x_k \to x$ при $k \to \infty$. Тогда мы приходим к целевому неравенству после перехода к пределу.

    **Достаточность**

    Формула Тейлора с остаточным членом Лагранжа второго порядка $\forall x, y: x, x + y \in X$ найдется $\alpha$ такая, что:

    $$
    f(x + y) = f(x) + \langle \nabla f(x), y \rangle + \frac{1}{2} \langle y, \nabla^2 f(x + \alpha y) y \rangle
    $$

    где $0 < \alpha < 1$.

    Используя формулу Тейлора с остаточным членом Лагранжа и неравенство из условия, получаем для $x + y \in X$:

    $$
    f(x + y) - f(x) - \langle \nabla f(x), y \rangle = \frac{1}{2} \langle y, \nabla^2 f(x + \alpha y) y \rangle \geq \frac{\mu}{2} \|y\|^2,
    $$

    где $0 \leq \alpha \leq 1$. Следовательно,

    $$
    f(x + y) - f(x) \geq \langle \nabla f(x), y \rangle + \frac{\mu}{2} \|y\|^2.
    $$

    Таким образом, по критерию первого порядка сильной выпуклости, функция $f(x)$ является сильно выпуклой с константой $\mu$. Важно отметить, что $\mu = 0$ соответствует случаю выпуклости и соответствующему дифференциальному критерию.
1.  Теорема о построении сопряженного множества к многогранному множеству.

    :::{.callout-note appearance="simple"}
    Пусть $x_1, \ldots, x_m \in \mathbb{R}^n$. Сопряжённое к многогранному множеству:

    $$
    S = \mathbf{conv}(x_1, \ldots, x_k) + \mathbf{cone}(x_{k+1}, \ldots, x_m) 
    $$

    будет многогранным множеством:

    $$
    S^* = \left\{ p \in \mathbb{R}^n \mid \langle p, x_i\rangle \ge -1, i = \overline{1,k} ; \langle p, x_i\rangle \ge 0, i = \overline{k+1,m} \right\}
    $$

    :::

    * Пусть $S = X, S^* = Y$. Возьмём произвольный $p \in X^*$, тогда $\langle p, x_i\rangle \ge -1, i = \overline{1,k}$. В то же время, для любого $\theta > 0, i = \overline{k+1,m}$: 
    
        $$
        \langle p, x_i\rangle \ge -1 \to \langle p, \theta x_i\rangle \ge -1
        $$

        $$
        \langle p, x_i\rangle \ge -\frac{1}{\theta} \to \langle p, x_i\rangle \geq 0. 
        $$

        Таким образом, $p \in Y \to X^* \subset Y$.

    * В обратную сторону: пусть $p \in Y$. Для любого $x \in X$:

        $$
        x = \sum\limits_{i=1}^m\theta_i x_i \;\;\;\;\;\;\; \sum\limits_{i=1}^k\theta_i = 1, \theta_i \ge 0
        $$
    
        Тогда:

        $$
        \langle p, x\rangle = \sum\limits_{i=1}^m\theta_i \langle p, x_i\rangle = \sum\limits_{i=1}^k\theta_i \langle p, x_i\rangle + \sum\limits_{i=k+1}^m\theta_i \langle p, x_i\rangle \ge \sum\limits_{i=1}^k\theta_i (-1) + \sum\limits_{i=1}^k\theta_i \cdot 0 = -1.
        $$

        Значит, $p \in X^* \to Y \subset X^*$.

1.  Вывод сопряженной функции к норме.
1.  Вывод субдифференциала нормы.
1.  Связь субградиента сопряженной функции и субградиента функции.
1.  Субдифференциальное условие оптимальности для условных выпуклых задач.
1.  Необходимые условия безусловного экстремума.

    :::{.callout-note appearance="simple"}
    Если в $x^*$ достигается локальный минимум и $f$ непрерывно дифференцируема в открытой окрестности, то
    $$
    \nabla f(x^*) = 0
    $$
    :::

    Предположим обратное. Пусть $\nabla f(x^*) \neq 0$. Рассмотрим вектор $p = -\nabla f(x^*)$ и заметим, что
    $$
    p^T \nabla f(x^*) = -\| \nabla f(x^*) \|^2 < 0
    $$
    Так как $\nabla f$ непрерывна в окрестности $x^*$, то существует скаляр $T > 0$ такой, что
    $$
    p^T \nabla f(x^* + tp) < 0, \text{ для любого } t \in [0,T]
    $$
    Для любого $\bar{t} \in (0, T]$, мы можем воспользоваться теоремой Тейлора:
    $$
    f(x^* + \bar{t}p) = f(x^*) + \bar{t} p^T \nabla f(x^* + tp), \text{ для некоторого } t \in (0,\bar{t})
    $$
    Следовательно, $f(x^* + \bar{t}p) < f(x^*)$ для любого $\bar{t} \in (0, T]$. Мы нашли направление, идя вдоль которого из $x^*$ функция $f$ убывает. Тогда $x^*$ -- не точка локального минимума. Получили противоречие.

1.  Достаточные условия безусловного экстремума.

    :::{.callout-note appearance="simple"}
    Пусть $\nabla^2 f$ непрерывна в открытой окрестности $x^*$ и
    $$
    \nabla f(x^*) = 0 \quad \nabla^2 f(x^*) \succ 0.
    $$
    Тогда $x^*$ -- точка локального минимума $f$.
    :::

    Так как гессиан непрерывен и положительно определен в $x^*$, то мы можем выбрать радиус $r > 0$ такой, что $\nabla^2 f(x)$ остается положительно определенной для всех $x$ в открытом шаре $B = \{ z \mid \|z - x^*\| < r \}$. Взяв любой ненулевой вектор $p$, для которого выполняется $\|p\| < r$, мы получаем $x^* + p \in B$, а также по формуле Тейлора:
    $$ 
    f(x^* + p) = f(x^*) + p^T \nabla f(x^*) + \frac{1}{2} p^T \nabla^2 f(z) p
    $$
    $$ 
    = f(x^*) + \frac{1}{2} p^T \nabla^2 f(z) p
    $$
    где $z = x^* + tp$ для некоторого $t \in (0,1)$. Так как $z \in B$, мы получаем $p^T \nabla^2 f(z) p > 0$, и следовательно $f(x^* + p) > f(x^*)$. Таким образом $x^*$ -- точка локального минимума.
1.  Субдифференциальная форма теоремы Каруша Куна Таккера (доказательство). Необходимые условия ККТ для произвольной задачи математического программирования (только формулировка).
1.  Формулировка симплекс метода для задачи линейного программирования в стандартной форме. Теорема о проверке оптимальности решения.

    :::{.callout-note appearance="simple"}
    Если все элементы $\lambda_B$ неположительны и базис $B$ допустимый, тогда базис $B$ оптимален.

    Здесь $\lambda_B$ это коэффициенты при разложении $c$ по базису $B$: $\lambda_B^{T}A_{B} = c^T \Rightarrow \lambda_B^T = c^TA_{B}^{-1}$.
    :::

    **Формулировка симплекс метода для задачи линейного программирования в стандартной форме. Теорема о проверке оптимальности решения**

    Задача линейного программирования:

    Пусть $c \in \mathbb{R}^n$, $b \in \mathbb{R}^m$, $A \in \mathbb{R}^{m \times n}$, тогда задача формулируется так:

    $$
    \begin{aligned}
    & \min_{x \in \mathbb{R}^n} && c^T x \\
    & \text{s.t.} && Ax \leq b
    \end{aligned}
    $$

    **Идейное описание симплекс метода:**

    1. Убедится, что точка, в которой мы находимся, является угловой
    2. Проверить оптимальность точки
    3. Если необходимо, сменить угол (то есть сменить базис)
    4. Повторять до схождения

    Шаги выполнения симплекс метода:

    1. **Поиск начальной базисной допустимой точки:**
        - Выберем начальную базисную (она является решением системы 
        $A_B x = b_B$, где $B$ - базис размера $n$ пространства, а матрица $A$ обычно имеет больше $n$ ограничений) допустимую (
            $Ax_0 \leq b$) точку 
        $x_0$ (искать ее будем через двухфазный симплексметод). Если такая точка не найдена, задача не имеет допустимого решения.

    2. **Проверка оптимальности:**
        - Разложение вектора 
        $c$ в данном базисе $B$ с коэффициентами 
        $\lambda_B$:
        $$
        \lambda_B^\top A_B = c^\top \quad \text{или} \quad \lambda_B^\top = c^\top A_B^{-1}
        $$
        - Если все компоненты 
        $\lambda_B$ неположительны, текущий базис является оптимальным. Иначе далее меняем вершину симплекса.

    3. **Определение переменной для удаления из базиса:**
        - Если в разложении 
        $\lambda_B$ есть положительные координаты, продолжаем оптимизацию. Пусть 
        $\lambda_B^k > 0$. Необходимо исключить 
        $k$ из базиса. Рассчитаем направляющий вектор 
        $d$, идя вдоль которого изменим вершину следующим образом: во-первых, для вектором всех ограничений из базиса, которые мы оставляем, направление должно быть им ортогонально, и, во-вторых, вдоль него значение, связанное с нашим ограничением, должно убывать:
        $$
        \begin{cases}
        A_{B \backslash \{k\} } d = 0 \\
        a_k^\top d < 0
        \end{cases}
        $$

    4. **Вычисление шага вдоль выбранного направления $d$:**
        - Для всех 
        $j \notin B$ считаем шаг:
        $$
        \mu_j = \frac{b_j - a_j^\top x_B}{a_j^\top d}
        $$
        - Новая вершина, которую добавим в базис:
        $$
        t = \arg\min_j \{\mu_j \mid \mu_j > 0\}
        $$

    5. **Обновление базиса:**
        - Обновляем базис и текущее решение:
        $$
        \begin{aligned}
            B' = B \backslash \{k\} \cup \{t\}, \\
            x_{B'} = x_B + \mu_t d = A_{B'}^{-1} b_{B'}
        \end{aligned}
        $$
        - Изменение базиса приводит к уменьшению значения целевой функции:
        $$
        c^\top x_{B'} = c^\top (x_B + \mu_t d) = c^\top x_B + \mu_t c^\top d
        $$

    6. **Повторение:**
        - Далее повторяем шаги 2-5 до достижения оптимального решения или установления, что задача не имеет допустимого решения.

    **Теорема о проверке оптимальности решения:**

    Если все элементы $\lambda_B$ неположительны и базис $B$ достижим, тогда базис $B$ оптимален.

    Здесь $\lambda_B$ это коэффициенты при разложении $c$ по базису $B$: $\lambda_B^{T}A_{B} = c^T \Rightarrow \lambda_B^T = c^TA_{B}^{-1}$.

    **Доказательство:**

    Предположим противное (что этот базис не оптимален), пусть $\exists x^*: \; Ax^{*}\leq b$ и при этом $c^Tx^* < c^T x_{B}$. Так как для всей матрицы $A$ и вектора $b$ неравенство верно, то и для подматрицы оно верно:

    $$
    A_{B}x^* \leq b_{B}
    $$

    Так как все элементы $\lambda_{B}$ неположительны, то домножим строки на соответствующие элементы и сложим:

    $$
    \lambda_{B}^TA_{B}x^* \geq \lambda_{B}^T b_{B}
    $$

    $$
    c^Tx^* \geq \lambda_{B}^T b_{B} = \lambda_{B}^T A_{B}x_{B} =c^Tx_B 
    $$

    Противоречие.
1.  Доказательство работы теста корней
1.  Метод дихотомии и золотого сечения для унимодальных функций. Скорость сходимости.

    :::{.callout-note appearance="simple"}
    Методы локализации решения для скалярной минимизации. Сходятся линейно.
    :::

    **Метод дихотомии**

    Решаем следующую задачу: 
    $$
    f(x) \rightarrow \min_{x \in [a, b]}
    $$
    где $f(x)$ — унимодальная функция.

    Мы хотим на каждом шаге вдвое сокращать область, в которой ищем минимум. Для этого будем пользоваться основным свойством унимодальных функций:
    $$
    \forall a \leq x_1 < x_2 \leq b:
    $$
    $$
    f(x_1) \leq f(x_2) \Rightarrow x_* \in [a, x_2]
    $$
    $$
    f(x_1) \geq f(x_2) \Rightarrow x_* \in [x_1, b]
    $$
    где $x_*$ — точка, в которой достигается минимум

    Алгоритм:

    ![Алгоритм дихотомии](Dichotomy4.pdf)

    Можно заметить, что на каждой итерации требуется не более 2-х вычислений значения функции.

    **Сходимость метода дихотомии**

    Длина отрезка на $k+1$ итерации:
    $$
    \Delta_{k+1} = b_{k+1} - a_{k+1} = \frac{1}{2^k}(b - a)
    $$

    Если будем выбирать середину отрезка как выход $k+1$ итерации:
    $$
    |x_{k+1} - x_*| \leq \frac{\Delta_{k+1}}{2}
    $$

    Подставим полученное ранее выражение для длины отрезка:
    $$
    |x_{k+1} - x_*| \leq \frac{1}{2^{k+1}}(b - a)
    $$
    $$
    |x_{k+1} - x_*| \leq (0.5)^{k+1}(b - a)
    $$

    Получили выражение для сходимости по итерациям. Отсюда также можно выразить необходимое количество итераций для достижения точности $\varepsilon$:

    $$
    K = \left\lceil \log_2 \frac{b-a}{\varepsilon} - 1 \right\rceil
    $$

    Теперь получим выражение для сходимости по количеству вычислений значения функции. Знаем, что на каждой итерации вычисляем значение не более 2-х раз, значит количество вычислений значения функции возьмём $N = 2k$:

    $$
    |x_{k+1} - x_*| \leq (0.5)^{\frac{N}{2}+1}(b - a)
    $$
    $$
    |x_{k+1} - x_*| \leq (0.707)^{N}\frac{b - a}{2}
    $$

    **Метод золотого сечения**

    Идея такая же, как и в методе дихотомии, но хотим уменьшить количество вычислений значения функции. Для этого будем вычислять значения в точках золотого сечения. Так на каждой итерации нам нужно будет вычислять значение только в одной точке, так как для нового отрезка в одной из точек золотого сечения значение будет уже посчитано:

    ![Золотое сечение](golden_search.pdf)

    Алгоритм:

    ```python
    def golden_search(f, a, b, epsilon):
        tau = (sqrt(5) + 1) / 2
        y = a + (b - a) / tau**2
        z = a + (b - a) / tau
        while b - a > epsilon:
            if f(y) <= f(z):
                b = z
                z = y
                y = a + (b - a) / tau**2
            else:
                a = y
                y = z
                z = a + (b - a) / tau
        return (a + b) / 2
    ```

    **Сходимость метода золотого сечения**

    На каждой итерации длина отрезка будет уменьшаться в $\tau = \frac{\sqrt{5} + 1}{2}$ раз. Тогда оценка сходимости (и по итерациям, и по вычислениям значений функции):
    $$
    |x_{k+1} - x_*| \leq \frac{b_{k+1} - a_{k+1}}{2} = \left( \frac{1}{\tau} \right)^{N-1} \frac{b - a}{2} \approx 0.618^k\frac{b - a}{2}
    $$

    Получили сходимость по итерациям хуже, чем у дихотомии, так как отрезки уменьшаются слабее на каждой итерации. Но по количеству вычислений значения функции сходимость у метода золотого сечения быстрее.
